{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "2 - Desafio I - Redes (text).ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alcoleasnoelia/Proyecto1-IA-DigitalHouse-Grupo4/blob/master/Notebook%20lll%20-%20Redes_(text).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "J7aABcEU3fjj"
      },
      "source": [
        "(Continuación de la notebook anterior)\n",
        "Notebook 3\n",
        "    \n",
        "6.  Modelado Avanzado Neural Networks (LSTMs)\n",
        "    *   Red Secuencial Inicial\n",
        "    *   Red Secuencial con regularizaciómn \n",
        "    *   Modelado de Red con Cross Validation\n",
        "    *   Análisis final de performance\n",
        "7.  Conclusiones\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n9bw471EXMLz",
        "colab_type": "code",
        "outputId": "644c53ab-b3a0-4a2f-fded-694aa6a58abf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "VzsMDcdC3fjr",
        "outputId": "616db159-5609-424d-8e39-9d65382f9df0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import pickle\n",
        "import re\n",
        "\n",
        "#Librerias para generación de features\n",
        "import string\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "#Librerias para construcción de red\n",
        "from keras import losses\n",
        "from keras import metrics\n",
        "from keras import optimizers\n",
        "from keras import models, layers\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.callbacks import EarlyStopping\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import Embedding\n",
        "from keras import regularizers"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n",
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "-i3Tilcfb3Ub"
      },
      "source": [
        "Cargamos el dataset a partir del pickle generado por la notebook de limpieza:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "oEL723aFb3Uc",
        "colab": {}
      },
      "source": [
        "news = pd.read_pickle(\"/content/drive/My Drive/Colab Notebooks/Desafio1Local/news_.p\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "LS9zw5BM5PI_",
        "outputId": "00d32322-292c-4ac8-a7b6-499dca47222b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 598
        }
      },
      "source": [
        "news.head(5)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>title</th>\n",
              "      <th>text</th>\n",
              "      <th>subject</th>\n",
              "      <th>date</th>\n",
              "      <th>text_st</th>\n",
              "      <th>fakenews_</th>\n",
              "      <th>title_clean</th>\n",
              "      <th>text_clean</th>\n",
              "      <th>text_polarity</th>\n",
              "      <th>text_subjectivity</th>\n",
              "      <th>title_polarity</th>\n",
              "      <th>title_subjectivity</th>\n",
              "      <th>text_clean_polarity</th>\n",
              "      <th>text_clean_subjectivity</th>\n",
              "      <th>title_clean_polarity</th>\n",
              "      <th>title_clean_subjectivity</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>As U.S. budget fight looms, Republicans flip t...</td>\n",
              "      <td>WASHINGTON (Reuters) - The head of a conservat...</td>\n",
              "      <td>politicsNews</td>\n",
              "      <td>December 31, 2017</td>\n",
              "      <td>washington (reuters) - the head of a conserv r...</td>\n",
              "      <td>0</td>\n",
              "      <td>us budget fight looms republicans flip fiscal ...</td>\n",
              "      <td>head conservative republican faction us congre...</td>\n",
              "      <td>0.037083</td>\n",
              "      <td>0.410250</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.036979</td>\n",
              "      <td>0.403438</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>U.S. military to accept transgender recruits o...</td>\n",
              "      <td>WASHINGTON (Reuters) - Transgender people will...</td>\n",
              "      <td>politicsNews</td>\n",
              "      <td>December 29, 2017</td>\n",
              "      <td>washington (reuters) - transgend peopl will be...</td>\n",
              "      <td>0</td>\n",
              "      <td>us military accept transgender recruits monday...</td>\n",
              "      <td>transgender people allowed first time enlist u...</td>\n",
              "      <td>0.055880</td>\n",
              "      <td>0.298557</td>\n",
              "      <td>-0.10</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.113095</td>\n",
              "      <td>0.296168</td>\n",
              "      <td>-0.1</td>\n",
              "      <td>0.10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Senior U.S. Republican senator: 'Let Mr. Muell...</td>\n",
              "      <td>WASHINGTON (Reuters) - The special counsel inv...</td>\n",
              "      <td>politicsNews</td>\n",
              "      <td>December 31, 2017</td>\n",
              "      <td>washington (reuters) - the special counsel inv...</td>\n",
              "      <td>0</td>\n",
              "      <td>senior us republican senator let mr mueller job</td>\n",
              "      <td>special counsel investigation links russia pre...</td>\n",
              "      <td>0.115930</td>\n",
              "      <td>0.316798</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.129766</td>\n",
              "      <td>0.311252</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>FBI Russia probe helped by Australian diplomat...</td>\n",
              "      <td>WASHINGTON (Reuters) - Trump campaign adviser ...</td>\n",
              "      <td>politicsNews</td>\n",
              "      <td>December 30, 2017</td>\n",
              "      <td>washington (reuters) - trump campaign advis ge...</td>\n",
              "      <td>0</td>\n",
              "      <td>fbi russia probe helped australian diplomat ti...</td>\n",
              "      <td>trump campaign adviser george papadopoulos tol...</td>\n",
              "      <td>0.035968</td>\n",
              "      <td>0.306569</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.030305</td>\n",
              "      <td>0.276323</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Trump wants Postal Service to charge 'much mor...</td>\n",
              "      <td>SEATTLE/WASHINGTON (Reuters) - President Donal...</td>\n",
              "      <td>politicsNews</td>\n",
              "      <td>December 29, 2017</td>\n",
              "      <td>seattle/washington (reuters) - presid donald t...</td>\n",
              "      <td>0</td>\n",
              "      <td>trump wants postal service charge much amazon ...</td>\n",
              "      <td>president donald trump called us postal servic...</td>\n",
              "      <td>0.030093</td>\n",
              "      <td>0.399891</td>\n",
              "      <td>0.35</td>\n",
              "      <td>0.3</td>\n",
              "      <td>0.042130</td>\n",
              "      <td>0.379259</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.15</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                               title  ... title_clean_subjectivity\n",
              "0  As U.S. budget fight looms, Republicans flip t...  ...                     0.00\n",
              "1  U.S. military to accept transgender recruits o...  ...                     0.10\n",
              "2  Senior U.S. Republican senator: 'Let Mr. Muell...  ...                     0.00\n",
              "3  FBI Russia probe helped by Australian diplomat...  ...                     0.00\n",
              "4  Trump wants Postal Service to charge 'much mor...  ...                     0.15\n",
              "\n",
              "[5 rows x 16 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ectm8dRY3fk1"
      },
      "source": [
        "Dividimos el dataset en train y test con shuffle y estratificación:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "F03PUTvf3fk3",
        "outputId": "9cb7cbad-1afa-4ffe-ab81-04b881cdf2a9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "data_train, data_test = train_test_split(news,test_size=0.30,shuffle=True,random_state=10, stratify=news[\"fakenews_\"])\n",
        "\n",
        "data_train = data_train.copy()\n",
        "data_test = data_test.copy() \n",
        "print(\"data train shape: {}\".format(data_train.shape))\n",
        "print(\"data test shape: {}\".format(data_test.shape))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "data train shape: (31428, 16)\n",
            "data test shape: (13470, 16)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hlmQxJMUASEq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def correr_stemming(text):\n",
        "    # Stemming\n",
        "    text = text.split()\n",
        "    stemmer = SnowballStemmer('english')\n",
        "    stemmed_words = [stemmer.stem(word) for word in text]\n",
        "    text = \" \".join(stemmed_words)\n",
        "    return(text)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "0n9AJ0-q3flA",
        "colab": {}
      },
      "source": [
        "X_train=data_train[\"text_clean\"].apply(correr_stemming)\n",
        "X_test=data_test[\"text_clean\"].apply(correr_stemming)\n",
        "y_train=data_train[\"fakenews_\"].copy()\n",
        "y_test=data_test[\"fakenews_\"].copy()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oTHVvL7OSmzr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train_partial=X_train[8000:]\n",
        "X_val=X_train[:8000]\n",
        "y_val=y_train[:8000]\n",
        "y_train_partial=y_train[8000:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UTYcLPvZSmzz",
        "colab_type": "code",
        "outputId": "13c6cd1b-766f-41fd-fa20-9920a0b0ed32",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "X_train_partial.shape, X_val.shape, X_test.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((23428,), (8000,), (13470,))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M-Kaja0sSmz5",
        "colab_type": "text"
      },
      "source": [
        "A partir de una vectorizacion por TFIDF obtenemos features delimitadas por las frecuencias\n",
        "Frecuencia Máxima 80%\n",
        "Frecuencia Mínima 2%\n",
        "\n",
        "quedarán en nuestro modelo 1422 Features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5J2pXl8BSmz5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tfidf_vec= TfidfVectorizer(stop_words='english', max_df=0.8, min_df=0.02)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4oVDAJWhSmz-",
        "colab_type": "code",
        "outputId": "ab3cd6de-3572-4a4e-b301-2150aa8c7ea3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "X_train_partial_vec=tfidf_vec.fit_transform(X_train_partial).copy()\n",
        "X_val_vec=tfidf_vec.transform(X_val).copy()\n",
        "X_test_vec=tfidf_vec.transform(X_test).copy()\n",
        "X_train_partial_vec.shape, X_val_vec.shape, X_test_vec.shape, y_test.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((23428, 1422), (8000, 1422), (13470, 1422), (13470,))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xDRVlZDySm0E",
        "colab_type": "code",
        "outputId": "3b3ad9c2-6591-4850-a0c8-f43029299ce7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "(X_train_partial_vec.shape[1],)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1422,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "t42KtQGf3flv"
      },
      "source": [
        "# **6.   Modelado Avanzado Neural Networks (LSTMs)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H-Q9YBRxSm0N",
        "colab_type": "text"
      },
      "source": [
        "Definimos una función para la construcción del modelo de capas densas:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "p5hS8v623fm_",
        "colab": {}
      },
      "source": [
        "def build_model(input_shape, layers=[15,10,1], optimizer='rmsprop'):\n",
        "    # Instanciamos la clase del modelo secuencial\n",
        "    model = Sequential(name='Modelo de base')\n",
        "    \n",
        "    #Configuramos la primera capa para que tome las features, con regularizacion l1\n",
        "    model.add(Dense(layers[0], activation='relu', input_shape=(X_train_partial_vec.shape[1],), kernel_regularizer=regularizers.l1(0.001)))\n",
        "    model.add(Dropout(0.5))\n",
        "\n",
        "    for l in layers[1:-1]:\n",
        "        model.add(Dense(units=l, activation='relu'))\n",
        "        model.add(Dropout(0.5))\n",
        "\n",
        "    # Agregamos la última capa \n",
        "    model.add(Dense(units=layers[-1], activation='sigmoid'))\n",
        "\n",
        "    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['binary_accuracy'])\n",
        "    # Retornamos el modelo compilado\n",
        "    return model\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XbEoMtvdSm0S",
        "colab_type": "text"
      },
      "source": [
        "Realizamos una busqueda por iteraciones para encontrar los mejores hiperparametros "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ucFB_eEr3fnB",
        "colab": {}
      },
      "source": [
        "from keras import optimizers"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZZzHip4ySm0X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Definimos la \"grilla\" de parámetros que vamos a explorar\n",
        "layers = [[512, 10, 1], [64, 32,10, 1]]\n",
        "optimizers = [ optimizers.SGD(momentum=0.9, nesterov=True), optimizers.Adam(), optimizers.RMSprop()]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I5p9766LSm0a",
        "colab_type": "code",
        "outputId": "24e15b4f-6cd0-4c39-ac89-806c389ba4fd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "import itertools\n",
        "combinaciones = list(itertools.product(layers, optimizers))\n",
        "combinaciones"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[([512, 10, 1], <keras.optimizers.SGD at 0x7f535fc21320>),\n",
              " ([512, 10, 1], <keras.optimizers.Adam at 0x7f535fc21be0>),\n",
              " ([512, 10, 1], <keras.optimizers.RMSprop at 0x7f535fc21390>),\n",
              " ([64, 32, 10, 1], <keras.optimizers.SGD at 0x7f535fc21320>),\n",
              " ([64, 32, 10, 1], <keras.optimizers.Adam at 0x7f535fc21be0>),\n",
              " ([64, 32, 10, 1], <keras.optimizers.RMSprop at 0x7f535fc21390>)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xhGz9OVOSm0h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Definamos algunas variables\n",
        "n_splits = 3\n",
        "batch_size = 3000\n",
        "epochs = 60\n",
        "verbose = 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P9Vgc6NRSm0n",
        "colab_type": "text"
      },
      "source": [
        "Creamos una lista vacía para ir guardando los entrenamientos y luego ejecutamos la función de cross validation para la selección del modelo con mejor arquitectura y optimizadores según su accuracy:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tx4Tq1TASm0o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "global_history = []"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZyFsP3PfSm0s",
        "colab_type": "code",
        "outputId": "bb4db192-3261-4f3f-e9b5-394138dd4efd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Importamos KFold para hacer cross-validation\n",
        "from sklearn.model_selection import KFold\n",
        "\n",
        "# Instanciamos el objeto KFold\n",
        "kfold = KFold(n_splits=n_splits, shuffle=False)\n",
        "\n",
        "# Recorremos las combinaciones y generamos distintos modelos a ensayar\n",
        "for (layers, optimizer) in combinaciones:\n",
        "    print('\\n\\nEnsayando modelo con estructura {} y optimizador {}'.format(layers, optimizer))\n",
        "    \n",
        "    # Construimos el modelo\n",
        "    \n",
        "    model = build_model(input_shape=X_train_partial_vec.shape, layers=layers, optimizer=optimizer)\n",
        "    \n",
        "    # Guardamos los pesos iniciales para usarlos en cada fold\n",
        "    model.save_weights('initial_weights.h5')\n",
        "    \n",
        "    # Generamos los sets de train y val para ensayar el modelo\n",
        "    for fold, (train_idx, val_idx) in enumerate(kfold.split(X_train_partial_vec)):\n",
        "        \n",
        "        # Reiniciamos los pesos del modelo\n",
        "        model.load_weights('initial_weights.h5')\n",
        "        \n",
        "        print(X_train_partial_vec.shape, X_val_vec.shape, X_test_vec.shape, y_train_partial.shape, y_val.shape, y_test.shape)       \n",
        "        \n",
        "        \n",
        "        # Lo entrenamos con el split de x_train e y_train correspondiente\n",
        "        history = model.fit(x=X_train_partial_vec,\n",
        "                            y=y_train_partial,\n",
        "                            batch_size=batch_size,\n",
        "                            epochs=epochs,\n",
        "                            validation_data=(X_val_vec, y_val),\n",
        "                            verbose=verbose\n",
        "                           )\n",
        "        \n",
        "        \n",
        "        \n",
        "        # Evaluamos en train y en val (estos mismos valores los podemos sacar de history)\n",
        "        train_loss, train_acc = model.evaluate(X_train_partial_vec, y_train_partial)\n",
        "        val_loss, val_acc = model.evaluate(X_val_vec, y_val)\n",
        "        \n",
        "        # Agregamos esta corrida a la historia global\n",
        "        global_history.append({'fold':fold, \n",
        "                               'layers':layers,\n",
        "                               'optimizer':optimizer,\n",
        "                               'train_loss':train_loss,\n",
        "                               'train_acc':train_acc,\n",
        "                               'val_loss':val_loss,\n",
        "                               'val_acc':val_acc,\n",
        "                               'history':history\n",
        "                              })"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "Ensayando modelo con estructura [512, 10, 1] y optimizador <keras.optimizers.SGD object at 0x7f535fc21320>\n",
            "(23428, 1422) (8000, 1422) (13470, 1422) (23428,) (8000,) (13470,)\n",
            "Train on 23428 samples, validate on 8000 samples\n",
            "Epoch 1/60\n",
            "23428/23428 [==============================] - 2s 79us/step - loss: 20.8881 - binary_accuracy: 0.5092 - val_loss: 20.7231 - val_binary_accuracy: 0.5238\n",
            "Epoch 2/60\n",
            "23428/23428 [==============================] - 1s 60us/step - loss: 20.5478 - binary_accuracy: 0.5236 - val_loss: 20.2895 - val_binary_accuracy: 0.5309\n",
            "Epoch 3/60\n",
            "23428/23428 [==============================] - 1s 59us/step - loss: 20.0765 - binary_accuracy: 0.5414 - val_loss: 19.7814 - val_binary_accuracy: 0.5220\n",
            "Epoch 4/60\n",
            "23428/23428 [==============================] - 1s 60us/step - loss: 19.5540 - binary_accuracy: 0.5460 - val_loss: 19.2461 - val_binary_accuracy: 0.5194\n",
            "Epoch 5/60\n",
            "23428/23428 [==============================] - 1s 59us/step - loss: 19.0148 - binary_accuracy: 0.5451 - val_loss: 18.7038 - val_binary_accuracy: 0.5191\n",
            "Epoch 6/60\n",
            "23428/23428 [==============================] - 1s 59us/step - loss: 18.4726 - binary_accuracy: 0.5505 - val_loss: 18.1633 - val_binary_accuracy: 0.5191\n",
            "Epoch 7/60\n",
            "23428/23428 [==============================] - 1s 59us/step - loss: 17.9343 - binary_accuracy: 0.5522 - val_loss: 17.6282 - val_binary_accuracy: 0.5192\n",
            "Epoch 8/60\n",
            "23428/23428 [==============================] - 1s 59us/step - loss: 17.4021 - binary_accuracy: 0.5639 - val_loss: 17.0998 - val_binary_accuracy: 0.5207\n",
            "Epoch 9/60\n",
            "23428/23428 [==============================] - 1s 59us/step - loss: 16.8773 - binary_accuracy: 0.5754 - val_loss: 16.5789 - val_binary_accuracy: 0.5289\n",
            "Epoch 10/60\n",
            "23428/23428 [==============================] - 1s 61us/step - loss: 16.3597 - binary_accuracy: 0.5935 - val_loss: 16.0659 - val_binary_accuracy: 0.5504\n",
            "Epoch 11/60\n",
            "23428/23428 [==============================] - 1s 61us/step - loss: 15.8509 - binary_accuracy: 0.6040 - val_loss: 15.5610 - val_binary_accuracy: 0.6006\n",
            "Epoch 12/60\n",
            "23428/23428 [==============================] - 1s 60us/step - loss: 15.3502 - binary_accuracy: 0.6118 - val_loss: 15.0644 - val_binary_accuracy: 0.6258\n",
            "Epoch 13/60\n",
            "23428/23428 [==============================] - 1s 60us/step - loss: 14.8563 - binary_accuracy: 0.6307 - val_loss: 14.5758 - val_binary_accuracy: 0.6752\n",
            "Epoch 14/60\n",
            "23428/23428 [==============================] - 1s 61us/step - loss: 14.3722 - binary_accuracy: 0.6364 - val_loss: 14.0955 - val_binary_accuracy: 0.7107\n",
            "Epoch 15/60\n",
            "23428/23428 [==============================] - 1s 60us/step - loss: 13.8959 - binary_accuracy: 0.6448 - val_loss: 13.6237 - val_binary_accuracy: 0.7268\n",
            "Epoch 16/60\n",
            "23428/23428 [==============================] - 1s 60us/step - loss: 13.4279 - binary_accuracy: 0.6519 - val_loss: 13.1601 - val_binary_accuracy: 0.7287\n",
            "Epoch 17/60\n",
            "23428/23428 [==============================] - 1s 60us/step - loss: 12.9676 - binary_accuracy: 0.6681 - val_loss: 12.7047 - val_binary_accuracy: 0.7734\n",
            "Epoch 18/60\n",
            "23428/23428 [==============================] - 1s 60us/step - loss: 12.5166 - binary_accuracy: 0.6733 - val_loss: 12.2575 - val_binary_accuracy: 0.7820\n",
            "Epoch 19/60\n",
            "23428/23428 [==============================] - 1s 60us/step - loss: 12.0724 - binary_accuracy: 0.6852 - val_loss: 11.8185 - val_binary_accuracy: 0.8115\n",
            "Epoch 20/60\n",
            "23428/23428 [==============================] - 1s 60us/step - loss: 11.6372 - binary_accuracy: 0.6925 - val_loss: 11.3876 - val_binary_accuracy: 0.8244\n",
            "Epoch 21/60\n",
            "23428/23428 [==============================] - 1s 61us/step - loss: 11.2109 - binary_accuracy: 0.6947 - val_loss: 10.9650 - val_binary_accuracy: 0.8290\n",
            "Epoch 22/60\n",
            "23428/23428 [==============================] - 1s 59us/step - loss: 10.7917 - binary_accuracy: 0.7044 - val_loss: 10.5506 - val_binary_accuracy: 0.8321\n",
            "Epoch 23/60\n",
            "23428/23428 [==============================] - 1s 60us/step - loss: 10.3806 - binary_accuracy: 0.7159 - val_loss: 10.1444 - val_binary_accuracy: 0.8717\n",
            "Epoch 24/60\n",
            "23428/23428 [==============================] - 1s 60us/step - loss: 9.9785 - binary_accuracy: 0.7248 - val_loss: 9.7464 - val_binary_accuracy: 0.8816\n",
            "Epoch 25/60\n",
            "23428/23428 [==============================] - 1s 59us/step - loss: 9.5843 - binary_accuracy: 0.7305 - val_loss: 9.3567 - val_binary_accuracy: 0.8802\n",
            "Epoch 26/60\n",
            "23428/23428 [==============================] - 1s 58us/step - loss: 9.1991 - binary_accuracy: 0.7360 - val_loss: 8.9751 - val_binary_accuracy: 0.8846\n",
            "Epoch 27/60\n",
            "23428/23428 [==============================] - 1s 58us/step - loss: 8.8211 - binary_accuracy: 0.7440 - val_loss: 8.6018 - val_binary_accuracy: 0.9007\n",
            "Epoch 28/60\n",
            "23428/23428 [==============================] - 1s 57us/step - loss: 8.4519 - binary_accuracy: 0.7521 - val_loss: 8.2366 - val_binary_accuracy: 0.9075\n",
            "Epoch 29/60\n",
            "23428/23428 [==============================] - 1s 60us/step - loss: 8.0907 - binary_accuracy: 0.7560 - val_loss: 7.8797 - val_binary_accuracy: 0.9060\n",
            "Epoch 30/60\n",
            "23428/23428 [==============================] - 1s 58us/step - loss: 7.7389 - binary_accuracy: 0.7610 - val_loss: 7.5312 - val_binary_accuracy: 0.9105\n",
            "Epoch 31/60\n",
            "23428/23428 [==============================] - 1s 61us/step - loss: 7.3936 - binary_accuracy: 0.7683 - val_loss: 7.1908 - val_binary_accuracy: 0.9135\n",
            "Epoch 32/60\n",
            "23428/23428 [==============================] - 1s 59us/step - loss: 7.0575 - binary_accuracy: 0.7765 - val_loss: 6.8588 - val_binary_accuracy: 0.9184\n",
            "Epoch 33/60\n",
            "23428/23428 [==============================] - 1s 58us/step - loss: 6.7310 - binary_accuracy: 0.7789 - val_loss: 6.5352 - val_binary_accuracy: 0.9194\n",
            "Epoch 34/60\n",
            "23428/23428 [==============================] - 1s 58us/step - loss: 6.4112 - binary_accuracy: 0.7824 - val_loss: 6.2199 - val_binary_accuracy: 0.9218\n",
            "Epoch 35/60\n",
            "23428/23428 [==============================] - 1s 57us/step - loss: 6.0992 - binary_accuracy: 0.7903 - val_loss: 5.9128 - val_binary_accuracy: 0.9252\n",
            "Epoch 36/60\n",
            "23428/23428 [==============================] - 1s 59us/step - loss: 5.7965 - binary_accuracy: 0.7929 - val_loss: 5.6142 - val_binary_accuracy: 0.9256\n",
            "Epoch 37/60\n",
            "23428/23428 [==============================] - 1s 58us/step - loss: 5.5037 - binary_accuracy: 0.7923 - val_loss: 5.3240 - val_binary_accuracy: 0.9271\n",
            "Epoch 38/60\n",
            "23428/23428 [==============================] - 1s 59us/step - loss: 5.2170 - binary_accuracy: 0.8006 - val_loss: 5.0419 - val_binary_accuracy: 0.9275\n",
            "Epoch 39/60\n",
            "23428/23428 [==============================] - 1s 58us/step - loss: 4.9396 - binary_accuracy: 0.8040 - val_loss: 4.7685 - val_binary_accuracy: 0.9284\n",
            "Epoch 40/60\n",
            "23428/23428 [==============================] - 1s 58us/step - loss: 4.6710 - binary_accuracy: 0.8078 - val_loss: 4.5035 - val_binary_accuracy: 0.9294\n",
            "Epoch 41/60\n",
            "23428/23428 [==============================] - 1s 58us/step - loss: 4.4103 - binary_accuracy: 0.8134 - val_loss: 4.2468 - val_binary_accuracy: 0.9301\n",
            "Epoch 42/60\n",
            "23428/23428 [==============================] - 1s 58us/step - loss: 4.1589 - binary_accuracy: 0.8128 - val_loss: 3.9990 - val_binary_accuracy: 0.9306\n",
            "Epoch 43/60\n",
            "23428/23428 [==============================] - 1s 58us/step - loss: 3.9144 - binary_accuracy: 0.8229 - val_loss: 3.7593 - val_binary_accuracy: 0.9319\n",
            "Epoch 44/60\n",
            "23428/23428 [==============================] - 1s 59us/step - loss: 3.6812 - binary_accuracy: 0.8202 - val_loss: 3.5286 - val_binary_accuracy: 0.9316\n",
            "Epoch 45/60\n",
            "23428/23428 [==============================] - 1s 58us/step - loss: 3.4549 - binary_accuracy: 0.8232 - val_loss: 3.3063 - val_binary_accuracy: 0.9326\n",
            "Epoch 46/60\n",
            "23428/23428 [==============================] - 1s 59us/step - loss: 3.2365 - binary_accuracy: 0.8303 - val_loss: 3.0925 - val_binary_accuracy: 0.9327\n",
            "Epoch 47/60\n",
            "23428/23428 [==============================] - 1s 58us/step - loss: 3.0284 - binary_accuracy: 0.8293 - val_loss: 2.8870 - val_binary_accuracy: 0.9339\n",
            "Epoch 48/60\n",
            "23428/23428 [==============================] - 1s 58us/step - loss: 2.8289 - binary_accuracy: 0.8330 - val_loss: 2.6908 - val_binary_accuracy: 0.9330\n",
            "Epoch 49/60\n",
            "23428/23428 [==============================] - 1s 57us/step - loss: 2.6348 - binary_accuracy: 0.8358 - val_loss: 2.5025 - val_binary_accuracy: 0.9329\n",
            "Epoch 50/60\n",
            "23428/23428 [==============================] - 1s 58us/step - loss: 2.4522 - binary_accuracy: 0.8377 - val_loss: 2.3233 - val_binary_accuracy: 0.9326\n",
            "Epoch 51/60\n",
            "23428/23428 [==============================] - 1s 58us/step - loss: 2.2788 - binary_accuracy: 0.8374 - val_loss: 2.1529 - val_binary_accuracy: 0.9325\n",
            "Epoch 52/60\n",
            "23428/23428 [==============================] - 1s 60us/step - loss: 2.1127 - binary_accuracy: 0.8443 - val_loss: 1.9911 - val_binary_accuracy: 0.9326\n",
            "Epoch 53/60\n",
            "23428/23428 [==============================] - 1s 58us/step - loss: 1.9555 - binary_accuracy: 0.8439 - val_loss: 1.8383 - val_binary_accuracy: 0.9316\n",
            "Epoch 54/60\n",
            "23428/23428 [==============================] - 1s 58us/step - loss: 1.8090 - binary_accuracy: 0.8407 - val_loss: 1.6943 - val_binary_accuracy: 0.9295\n",
            "Epoch 55/60\n",
            "23428/23428 [==============================] - 1s 59us/step - loss: 1.6687 - binary_accuracy: 0.8433 - val_loss: 1.5584 - val_binary_accuracy: 0.9295\n",
            "Epoch 56/60\n",
            "23428/23428 [==============================] - 1s 58us/step - loss: 1.5374 - binary_accuracy: 0.8448 - val_loss: 1.4307 - val_binary_accuracy: 0.9301\n",
            "Epoch 57/60\n",
            "23428/23428 [==============================] - 1s 58us/step - loss: 1.4154 - binary_accuracy: 0.8471 - val_loss: 1.3131 - val_binary_accuracy: 0.9266\n",
            "Epoch 58/60\n",
            "23428/23428 [==============================] - 1s 59us/step - loss: 1.3014 - binary_accuracy: 0.8468 - val_loss: 1.2021 - val_binary_accuracy: 0.9275\n",
            "Epoch 59/60\n",
            "23428/23428 [==============================] - 1s 59us/step - loss: 1.1958 - binary_accuracy: 0.8460 - val_loss: 1.1012 - val_binary_accuracy: 0.9256\n",
            "Epoch 60/60\n",
            "23428/23428 [==============================] - 1s 59us/step - loss: 1.0985 - binary_accuracy: 0.8502 - val_loss: 1.0088 - val_binary_accuracy: 0.9233\n",
            "23428/23428 [==============================] - 2s 99us/step\n",
            "8000/8000 [==============================] - 1s 99us/step\n",
            "(23428, 1422) (8000, 1422) (13470, 1422) (23428,) (8000,) (13470,)\n",
            "Train on 23428 samples, validate on 8000 samples\n",
            "Epoch 1/60\n",
            "23428/23428 [==============================] - 1s 58us/step - loss: 20.8633 - binary_accuracy: 0.5088 - val_loss: 20.6729 - val_binary_accuracy: 0.5341\n",
            "Epoch 2/60\n",
            "23428/23428 [==============================] - 1s 58us/step - loss: 20.4870 - binary_accuracy: 0.5266 - val_loss: 20.2179 - val_binary_accuracy: 0.5271\n",
            "Epoch 3/60\n",
            "23428/23428 [==============================] - 1s 58us/step - loss: 20.0001 - binary_accuracy: 0.5409 - val_loss: 19.7005 - val_binary_accuracy: 0.5204\n",
            "Epoch 4/60\n",
            "23428/23428 [==============================] - 1s 58us/step - loss: 19.4708 - binary_accuracy: 0.5496 - val_loss: 19.1611 - val_binary_accuracy: 0.5195\n",
            "Epoch 5/60\n",
            "23428/23428 [==============================] - 1s 60us/step - loss: 18.9289 - binary_accuracy: 0.5495 - val_loss: 18.6169 - val_binary_accuracy: 0.5192\n",
            "Epoch 6/60\n",
            "23428/23428 [==============================] - 1s 58us/step - loss: 18.3855 - binary_accuracy: 0.5544 - val_loss: 18.0756 - val_binary_accuracy: 0.5195\n",
            "Epoch 7/60\n",
            "23428/23428 [==============================] - 1s 58us/step - loss: 17.8466 - binary_accuracy: 0.5612 - val_loss: 17.5398 - val_binary_accuracy: 0.5203\n",
            "Epoch 8/60\n",
            "23428/23428 [==============================] - 1s 58us/step - loss: 17.3138 - binary_accuracy: 0.5716 - val_loss: 17.0109 - val_binary_accuracy: 0.5254\n",
            "Epoch 9/60\n",
            "23428/23428 [==============================] - 1s 59us/step - loss: 16.7878 - binary_accuracy: 0.5905 - val_loss: 16.4894 - val_binary_accuracy: 0.5566\n",
            "Epoch 10/60\n",
            "23428/23428 [==============================] - 1s 58us/step - loss: 16.2710 - binary_accuracy: 0.6025 - val_loss: 15.9759 - val_binary_accuracy: 0.6209\n",
            "Epoch 11/60\n",
            "23428/23428 [==============================] - 1s 59us/step - loss: 15.7610 - binary_accuracy: 0.6184 - val_loss: 15.4706 - val_binary_accuracy: 0.6582\n",
            "Epoch 12/60\n",
            "23428/23428 [==============================] - 1s 59us/step - loss: 15.2595 - binary_accuracy: 0.6279 - val_loss: 14.9735 - val_binary_accuracy: 0.7054\n",
            "Epoch 13/60\n",
            "23428/23428 [==============================] - 1s 61us/step - loss: 14.7663 - binary_accuracy: 0.6388 - val_loss: 14.4845 - val_binary_accuracy: 0.7268\n",
            "Epoch 14/60\n",
            "23428/23428 [==============================] - 1s 61us/step - loss: 14.2817 - binary_accuracy: 0.6504 - val_loss: 14.0040 - val_binary_accuracy: 0.7437\n",
            "Epoch 15/60\n",
            "23428/23428 [==============================] - 1s 60us/step - loss: 13.8045 - binary_accuracy: 0.6626 - val_loss: 13.5318 - val_binary_accuracy: 0.7769\n",
            "Epoch 16/60\n",
            "23428/23428 [==============================] - 1s 59us/step - loss: 13.3367 - binary_accuracy: 0.6666 - val_loss: 13.0679 - val_binary_accuracy: 0.7855\n",
            "Epoch 17/60\n",
            "23428/23428 [==============================] - 1s 59us/step - loss: 12.8762 - binary_accuracy: 0.6784 - val_loss: 12.6122 - val_binary_accuracy: 0.7874\n",
            "Epoch 18/60\n",
            "23428/23428 [==============================] - 1s 58us/step - loss: 12.4240 - binary_accuracy: 0.6858 - val_loss: 12.1646 - val_binary_accuracy: 0.8016\n",
            "Epoch 19/60\n",
            "23428/23428 [==============================] - 1s 58us/step - loss: 11.9795 - binary_accuracy: 0.6983 - val_loss: 11.7252 - val_binary_accuracy: 0.8424\n",
            "Epoch 20/60\n",
            "23428/23428 [==============================] - 1s 61us/step - loss: 11.5441 - binary_accuracy: 0.7052 - val_loss: 11.2939 - val_binary_accuracy: 0.8529\n",
            "Epoch 21/60\n",
            "23428/23428 [==============================] - 1s 58us/step - loss: 11.1174 - binary_accuracy: 0.7092 - val_loss: 10.8708 - val_binary_accuracy: 0.8554\n",
            "Epoch 22/60\n",
            "23428/23428 [==============================] - 1s 58us/step - loss: 10.6981 - binary_accuracy: 0.7199 - val_loss: 10.4559 - val_binary_accuracy: 0.8730\n",
            "Epoch 23/60\n",
            "23428/23428 [==============================] - 1s 58us/step - loss: 10.2868 - binary_accuracy: 0.7281 - val_loss: 10.0491 - val_binary_accuracy: 0.8882\n",
            "Epoch 24/60\n",
            "23428/23428 [==============================] - 1s 58us/step - loss: 9.8845 - binary_accuracy: 0.7342 - val_loss: 9.6506 - val_binary_accuracy: 0.8901\n",
            "Epoch 25/60\n",
            "23428/23428 [==============================] - 1s 58us/step - loss: 9.4888 - binary_accuracy: 0.7447 - val_loss: 9.2603 - val_binary_accuracy: 0.8986\n",
            "Epoch 26/60\n",
            "23428/23428 [==============================] - 1s 59us/step - loss: 9.1034 - binary_accuracy: 0.7532 - val_loss: 8.8782 - val_binary_accuracy: 0.9026\n",
            "Epoch 27/60\n",
            "23428/23428 [==============================] - 1s 59us/step - loss: 8.7258 - binary_accuracy: 0.7551 - val_loss: 8.5042 - val_binary_accuracy: 0.9087\n",
            "Epoch 28/60\n",
            "23428/23428 [==============================] - 1s 59us/step - loss: 8.3556 - binary_accuracy: 0.7623 - val_loss: 8.1384 - val_binary_accuracy: 0.9136\n",
            "Epoch 29/60\n",
            "23428/23428 [==============================] - 1s 59us/step - loss: 7.9942 - binary_accuracy: 0.7678 - val_loss: 7.7808 - val_binary_accuracy: 0.9175\n",
            "Epoch 30/60\n",
            "23428/23428 [==============================] - 1s 58us/step - loss: 7.6415 - binary_accuracy: 0.7738 - val_loss: 7.4317 - val_binary_accuracy: 0.9186\n",
            "Epoch 31/60\n",
            "23428/23428 [==============================] - 1s 58us/step - loss: 7.2959 - binary_accuracy: 0.7807 - val_loss: 7.0906 - val_binary_accuracy: 0.9218\n",
            "Epoch 32/60\n",
            "23428/23428 [==============================] - 1s 59us/step - loss: 6.9602 - binary_accuracy: 0.7827 - val_loss: 6.7582 - val_binary_accuracy: 0.9234\n",
            "Epoch 33/60\n",
            "23428/23428 [==============================] - 1s 58us/step - loss: 6.6305 - binary_accuracy: 0.7940 - val_loss: 6.4338 - val_binary_accuracy: 0.9254\n",
            "Epoch 34/60\n",
            "23428/23428 [==============================] - 1s 58us/step - loss: 6.3106 - binary_accuracy: 0.7930 - val_loss: 6.1179 - val_binary_accuracy: 0.9256\n",
            "Epoch 35/60\n",
            "23428/23428 [==============================] - 1s 60us/step - loss: 6.0009 - binary_accuracy: 0.7978 - val_loss: 5.8104 - val_binary_accuracy: 0.9269\n",
            "Epoch 36/60\n",
            "23428/23428 [==============================] - 1s 58us/step - loss: 5.6964 - binary_accuracy: 0.8038 - val_loss: 5.5111 - val_binary_accuracy: 0.9276\n",
            "Epoch 37/60\n",
            "23428/23428 [==============================] - 1s 58us/step - loss: 5.4039 - binary_accuracy: 0.8054 - val_loss: 5.2206 - val_binary_accuracy: 0.9281\n",
            "Epoch 38/60\n",
            "23428/23428 [==============================] - 1s 58us/step - loss: 5.1171 - binary_accuracy: 0.8079 - val_loss: 4.9379 - val_binary_accuracy: 0.9289\n",
            "Epoch 39/60\n",
            "23428/23428 [==============================] - 1s 58us/step - loss: 4.8405 - binary_accuracy: 0.8103 - val_loss: 4.6645 - val_binary_accuracy: 0.9295\n",
            "Epoch 40/60\n",
            "23428/23428 [==============================] - 1s 57us/step - loss: 4.5708 - binary_accuracy: 0.8171 - val_loss: 4.3988 - val_binary_accuracy: 0.9311\n",
            "Epoch 41/60\n",
            "23428/23428 [==============================] - 1s 60us/step - loss: 4.3091 - binary_accuracy: 0.8250 - val_loss: 4.1422 - val_binary_accuracy: 0.9315\n",
            "Epoch 42/60\n",
            "23428/23428 [==============================] - 1s 58us/step - loss: 4.0571 - binary_accuracy: 0.8254 - val_loss: 3.8935 - val_binary_accuracy: 0.9324\n",
            "Epoch 43/60\n",
            "23428/23428 [==============================] - 1s 58us/step - loss: 3.8149 - binary_accuracy: 0.8276 - val_loss: 3.6542 - val_binary_accuracy: 0.9329\n",
            "Epoch 44/60\n",
            "23428/23428 [==============================] - 1s 59us/step - loss: 3.5798 - binary_accuracy: 0.8332 - val_loss: 3.4228 - val_binary_accuracy: 0.9334\n",
            "Epoch 45/60\n",
            "23428/23428 [==============================] - 1s 58us/step - loss: 3.3534 - binary_accuracy: 0.8314 - val_loss: 3.2006 - val_binary_accuracy: 0.9330\n",
            "Epoch 46/60\n",
            "23428/23428 [==============================] - 1s 58us/step - loss: 3.1359 - binary_accuracy: 0.8354 - val_loss: 2.9869 - val_binary_accuracy: 0.9337\n",
            "Epoch 47/60\n",
            "23428/23428 [==============================] - 1s 58us/step - loss: 2.9276 - binary_accuracy: 0.8382 - val_loss: 2.7818 - val_binary_accuracy: 0.9344\n",
            "Epoch 48/60\n",
            "23428/23428 [==============================] - 1s 58us/step - loss: 2.7259 - binary_accuracy: 0.8397 - val_loss: 2.5848 - val_binary_accuracy: 0.9349\n",
            "Epoch 49/60\n",
            "23428/23428 [==============================] - 1s 58us/step - loss: 2.5348 - binary_accuracy: 0.8428 - val_loss: 2.3971 - val_binary_accuracy: 0.9350\n",
            "Epoch 50/60\n",
            "23428/23428 [==============================] - 1s 58us/step - loss: 2.3510 - binary_accuracy: 0.8474 - val_loss: 2.2181 - val_binary_accuracy: 0.9350\n",
            "Epoch 51/60\n",
            "23428/23428 [==============================] - 1s 59us/step - loss: 2.1761 - binary_accuracy: 0.8469 - val_loss: 2.0481 - val_binary_accuracy: 0.9330\n",
            "Epoch 52/60\n",
            "23428/23428 [==============================] - 1s 57us/step - loss: 2.0123 - binary_accuracy: 0.8463 - val_loss: 1.8859 - val_binary_accuracy: 0.9342\n",
            "Epoch 53/60\n",
            "23428/23428 [==============================] - 1s 58us/step - loss: 1.8546 - binary_accuracy: 0.8489 - val_loss: 1.7339 - val_binary_accuracy: 0.9320\n",
            "Epoch 54/60\n",
            "23428/23428 [==============================] - 1s 58us/step - loss: 1.7081 - binary_accuracy: 0.8471 - val_loss: 1.5894 - val_binary_accuracy: 0.9326\n",
            "Epoch 55/60\n",
            "23428/23428 [==============================] - 1s 57us/step - loss: 1.5675 - binary_accuracy: 0.8518 - val_loss: 1.4544 - val_binary_accuracy: 0.9310\n",
            "Epoch 56/60\n",
            "23428/23428 [==============================] - 1s 59us/step - loss: 1.4392 - binary_accuracy: 0.8504 - val_loss: 1.3276 - val_binary_accuracy: 0.9298\n",
            "Epoch 57/60\n",
            "23428/23428 [==============================] - 1s 61us/step - loss: 1.3153 - binary_accuracy: 0.8508 - val_loss: 1.2097 - val_binary_accuracy: 0.9284\n",
            "Epoch 58/60\n",
            "23428/23428 [==============================] - 1s 60us/step - loss: 1.2020 - binary_accuracy: 0.8527 - val_loss: 1.0996 - val_binary_accuracy: 0.9290\n",
            "Epoch 59/60\n",
            "23428/23428 [==============================] - 1s 58us/step - loss: 1.0957 - binary_accuracy: 0.8539 - val_loss: 0.9974 - val_binary_accuracy: 0.9264\n",
            "Epoch 60/60\n",
            "23428/23428 [==============================] - 1s 58us/step - loss: 1.0037 - binary_accuracy: 0.8544 - val_loss: 0.9151 - val_binary_accuracy: 0.9244\n",
            "23428/23428 [==============================] - 2s 99us/step\n",
            "8000/8000 [==============================] - 1s 99us/step\n",
            "(23428, 1422) (8000, 1422) (13470, 1422) (23428,) (8000,) (13470,)\n",
            "Train on 23428 samples, validate on 8000 samples\n",
            "Epoch 1/60\n",
            "23428/23428 [==============================] - 1s 60us/step - loss: 20.8652 - binary_accuracy: 0.5077 - val_loss: 20.6774 - val_binary_accuracy: 0.5340\n",
            "Epoch 2/60\n",
            "23428/23428 [==============================] - 1s 58us/step - loss: 20.4922 - binary_accuracy: 0.5290 - val_loss: 20.2242 - val_binary_accuracy: 0.5294\n",
            "Epoch 3/60\n",
            "23428/23428 [==============================] - 1s 58us/step - loss: 20.0070 - binary_accuracy: 0.5378 - val_loss: 19.7077 - val_binary_accuracy: 0.5206\n",
            "Epoch 4/60\n",
            "23428/23428 [==============================] - 1s 60us/step - loss: 19.4784 - binary_accuracy: 0.5458 - val_loss: 19.1687 - val_binary_accuracy: 0.5192\n",
            "Epoch 5/60\n",
            "23428/23428 [==============================] - 1s 58us/step - loss: 18.9362 - binary_accuracy: 0.5512 - val_loss: 18.6247 - val_binary_accuracy: 0.5191\n",
            "Epoch 6/60\n",
            "23428/23428 [==============================] - 1s 58us/step - loss: 18.3931 - binary_accuracy: 0.5558 - val_loss: 18.0834 - val_binary_accuracy: 0.5194\n",
            "Epoch 7/60\n",
            "23428/23428 [==============================] - 1s 58us/step - loss: 17.8543 - binary_accuracy: 0.5627 - val_loss: 17.5476 - val_binary_accuracy: 0.5209\n",
            "Epoch 8/60\n",
            "23428/23428 [==============================] - 1s 58us/step - loss: 17.3216 - binary_accuracy: 0.5753 - val_loss: 17.0187 - val_binary_accuracy: 0.5296\n",
            "Epoch 9/60\n",
            "23428/23428 [==============================] - 1s 58us/step - loss: 16.7963 - binary_accuracy: 0.5869 - val_loss: 16.4974 - val_binary_accuracy: 0.5584\n",
            "Epoch 10/60\n",
            "23428/23428 [==============================] - 1s 59us/step - loss: 16.2791 - binary_accuracy: 0.5976 - val_loss: 15.9842 - val_binary_accuracy: 0.6071\n",
            "Epoch 11/60\n",
            "23428/23428 [==============================] - 1s 58us/step - loss: 15.7694 - binary_accuracy: 0.6143 - val_loss: 15.4793 - val_binary_accuracy: 0.6506\n",
            "Epoch 12/60\n",
            "23428/23428 [==============================] - 1s 59us/step - loss: 15.2684 - binary_accuracy: 0.6253 - val_loss: 14.9825 - val_binary_accuracy: 0.6684\n",
            "Epoch 13/60\n",
            "23428/23428 [==============================] - 1s 58us/step - loss: 14.7753 - binary_accuracy: 0.6364 - val_loss: 14.4939 - val_binary_accuracy: 0.6780\n",
            "Epoch 14/60\n",
            "23428/23428 [==============================] - 1s 59us/step - loss: 14.2902 - binary_accuracy: 0.6473 - val_loss: 14.0135 - val_binary_accuracy: 0.7079\n",
            "Epoch 15/60\n",
            "23428/23428 [==============================] - 1s 58us/step - loss: 13.8137 - binary_accuracy: 0.6574 - val_loss: 13.5413 - val_binary_accuracy: 0.7414\n",
            "Epoch 16/60\n",
            "23428/23428 [==============================] - 1s 58us/step - loss: 13.3452 - binary_accuracy: 0.6686 - val_loss: 13.0774 - val_binary_accuracy: 0.7775\n",
            "Epoch 17/60\n",
            "23428/23428 [==============================] - 1s 59us/step - loss: 12.8852 - binary_accuracy: 0.6777 - val_loss: 12.6218 - val_binary_accuracy: 0.7936\n",
            "Epoch 18/60\n",
            "23428/23428 [==============================] - 1s 58us/step - loss: 12.4340 - binary_accuracy: 0.6847 - val_loss: 12.1744 - val_binary_accuracy: 0.8058\n",
            "Epoch 19/60\n",
            "23428/23428 [==============================] - 1s 58us/step - loss: 11.9902 - binary_accuracy: 0.6882 - val_loss: 11.7351 - val_binary_accuracy: 0.8138\n",
            "Epoch 20/60\n",
            "23428/23428 [==============================] - 1s 59us/step - loss: 11.5542 - binary_accuracy: 0.7024 - val_loss: 11.3040 - val_binary_accuracy: 0.8396\n",
            "Epoch 21/60\n",
            "23428/23428 [==============================] - 1s 58us/step - loss: 11.1265 - binary_accuracy: 0.7109 - val_loss: 10.8810 - val_binary_accuracy: 0.8639\n",
            "Epoch 22/60\n",
            "23428/23428 [==============================] - 1s 58us/step - loss: 10.7085 - binary_accuracy: 0.7131 - val_loss: 10.4663 - val_binary_accuracy: 0.8671\n",
            "Epoch 23/60\n",
            "23428/23428 [==============================] - 1s 58us/step - loss: 10.2974 - binary_accuracy: 0.7226 - val_loss: 10.0597 - val_binary_accuracy: 0.8776\n",
            "Epoch 24/60\n",
            "23428/23428 [==============================] - 1s 58us/step - loss: 9.8940 - binary_accuracy: 0.7337 - val_loss: 9.6613 - val_binary_accuracy: 0.8874\n",
            "Epoch 25/60\n",
            "23428/23428 [==============================] - 1s 57us/step - loss: 9.5009 - binary_accuracy: 0.7391 - val_loss: 9.2712 - val_binary_accuracy: 0.8910\n",
            "Epoch 26/60\n",
            "23428/23428 [==============================] - 1s 58us/step - loss: 9.1146 - binary_accuracy: 0.7445 - val_loss: 8.8893 - val_binary_accuracy: 0.9032\n",
            "Epoch 27/60\n",
            "23428/23428 [==============================] - 1s 59us/step - loss: 8.7367 - binary_accuracy: 0.7526 - val_loss: 8.5155 - val_binary_accuracy: 0.9084\n",
            "Epoch 28/60\n",
            "23428/23428 [==============================] - 1s 59us/step - loss: 8.3668 - binary_accuracy: 0.7566 - val_loss: 8.1500 - val_binary_accuracy: 0.9099\n",
            "Epoch 29/60\n",
            "23428/23428 [==============================] - 1s 58us/step - loss: 8.0047 - binary_accuracy: 0.7694 - val_loss: 7.7927 - val_binary_accuracy: 0.9122\n",
            "Epoch 30/60\n",
            "23428/23428 [==============================] - 1s 59us/step - loss: 7.6529 - binary_accuracy: 0.7685 - val_loss: 7.4436 - val_binary_accuracy: 0.9174\n",
            "Epoch 31/60\n",
            "23428/23428 [==============================] - 1s 58us/step - loss: 7.3070 - binary_accuracy: 0.7739 - val_loss: 7.1029 - val_binary_accuracy: 0.9183\n",
            "Epoch 32/60\n",
            "23428/23428 [==============================] - 1s 59us/step - loss: 6.9702 - binary_accuracy: 0.7847 - val_loss: 6.7705 - val_binary_accuracy: 0.9214\n",
            "Epoch 33/60\n",
            "23428/23428 [==============================] - 1s 58us/step - loss: 6.6437 - binary_accuracy: 0.7852 - val_loss: 6.4463 - val_binary_accuracy: 0.9246\n",
            "Epoch 34/60\n",
            "23428/23428 [==============================] - 1s 58us/step - loss: 6.3234 - binary_accuracy: 0.7896 - val_loss: 6.1305 - val_binary_accuracy: 0.9262\n",
            "Epoch 35/60\n",
            "23428/23428 [==============================] - 1s 59us/step - loss: 6.0120 - binary_accuracy: 0.7946 - val_loss: 5.8231 - val_binary_accuracy: 0.9271\n",
            "Epoch 36/60\n",
            "23428/23428 [==============================] - 1s 58us/step - loss: 5.7084 - binary_accuracy: 0.8036 - val_loss: 5.5241 - val_binary_accuracy: 0.9270\n",
            "Epoch 37/60\n",
            "23428/23428 [==============================] - 1s 58us/step - loss: 5.4158 - binary_accuracy: 0.8012 - val_loss: 5.2336 - val_binary_accuracy: 0.9279\n",
            "Epoch 38/60\n",
            "23428/23428 [==============================] - 1s 59us/step - loss: 5.1296 - binary_accuracy: 0.8089 - val_loss: 4.9512 - val_binary_accuracy: 0.9289\n",
            "Epoch 39/60\n",
            "23428/23428 [==============================] - 1s 60us/step - loss: 4.8508 - binary_accuracy: 0.8125 - val_loss: 4.6774 - val_binary_accuracy: 0.9301\n",
            "Epoch 40/60\n",
            "23428/23428 [==============================] - 1s 58us/step - loss: 4.5824 - binary_accuracy: 0.8154 - val_loss: 4.4123 - val_binary_accuracy: 0.9304\n",
            "Epoch 41/60\n",
            "23428/23428 [==============================] - 1s 58us/step - loss: 4.3212 - binary_accuracy: 0.8186 - val_loss: 4.1556 - val_binary_accuracy: 0.9309\n",
            "Epoch 42/60\n",
            "23428/23428 [==============================] - 1s 58us/step - loss: 4.0711 - binary_accuracy: 0.8177 - val_loss: 3.9071 - val_binary_accuracy: 0.9323\n",
            "Epoch 43/60\n",
            "23428/23428 [==============================] - 1s 60us/step - loss: 3.8274 - binary_accuracy: 0.8234 - val_loss: 3.6677 - val_binary_accuracy: 0.9327\n",
            "Epoch 44/60\n",
            "23428/23428 [==============================] - 1s 59us/step - loss: 3.5892 - binary_accuracy: 0.8302 - val_loss: 3.4365 - val_binary_accuracy: 0.9331\n",
            "Epoch 45/60\n",
            "23428/23428 [==============================] - 1s 60us/step - loss: 3.3633 - binary_accuracy: 0.8358 - val_loss: 3.2140 - val_binary_accuracy: 0.9329\n",
            "Epoch 46/60\n",
            "23428/23428 [==============================] - 1s 59us/step - loss: 3.1481 - binary_accuracy: 0.8336 - val_loss: 3.0001 - val_binary_accuracy: 0.9333\n",
            "Epoch 47/60\n",
            "23428/23428 [==============================] - 1s 58us/step - loss: 2.9404 - binary_accuracy: 0.8316 - val_loss: 2.7949 - val_binary_accuracy: 0.9341\n",
            "Epoch 48/60\n",
            "23428/23428 [==============================] - 1s 58us/step - loss: 2.7375 - binary_accuracy: 0.8420 - val_loss: 2.5980 - val_binary_accuracy: 0.9334\n",
            "Epoch 49/60\n",
            "23428/23428 [==============================] - 1s 58us/step - loss: 2.5477 - binary_accuracy: 0.8388 - val_loss: 2.4103 - val_binary_accuracy: 0.9337\n",
            "Epoch 50/60\n",
            "23428/23428 [==============================] - 1s 58us/step - loss: 2.3640 - binary_accuracy: 0.8424 - val_loss: 2.2317 - val_binary_accuracy: 0.9337\n",
            "Epoch 51/60\n",
            "23428/23428 [==============================] - 1s 59us/step - loss: 2.1906 - binary_accuracy: 0.8443 - val_loss: 2.0610 - val_binary_accuracy: 0.9348\n",
            "Epoch 52/60\n",
            "23428/23428 [==============================] - 1s 58us/step - loss: 2.0239 - binary_accuracy: 0.8445 - val_loss: 1.8992 - val_binary_accuracy: 0.9342\n",
            "Epoch 53/60\n",
            "23428/23428 [==============================] - 1s 58us/step - loss: 1.8688 - binary_accuracy: 0.8451 - val_loss: 1.7473 - val_binary_accuracy: 0.9316\n",
            "Epoch 54/60\n",
            "23428/23428 [==============================] - 1s 58us/step - loss: 1.7219 - binary_accuracy: 0.8465 - val_loss: 1.6030 - val_binary_accuracy: 0.9309\n",
            "Epoch 55/60\n",
            "23428/23428 [==============================] - 1s 58us/step - loss: 1.5793 - binary_accuracy: 0.8501 - val_loss: 1.4672 - val_binary_accuracy: 0.9301\n",
            "Epoch 56/60\n",
            "23428/23428 [==============================] - 1s 58us/step - loss: 1.4472 - binary_accuracy: 0.8540 - val_loss: 1.3399 - val_binary_accuracy: 0.9296\n",
            "Epoch 57/60\n",
            "23428/23428 [==============================] - 1s 58us/step - loss: 1.3255 - binary_accuracy: 0.8499 - val_loss: 1.2215 - val_binary_accuracy: 0.9283\n",
            "Epoch 58/60\n",
            "23428/23428 [==============================] - 1s 58us/step - loss: 1.2145 - binary_accuracy: 0.8482 - val_loss: 1.1120 - val_binary_accuracy: 0.9265\n",
            "Epoch 59/60\n",
            "23428/23428 [==============================] - 1s 59us/step - loss: 1.1068 - binary_accuracy: 0.8538 - val_loss: 1.0092 - val_binary_accuracy: 0.9268\n",
            "Epoch 60/60\n",
            "23428/23428 [==============================] - 1s 58us/step - loss: 1.0107 - binary_accuracy: 0.8506 - val_loss: 0.9176 - val_binary_accuracy: 0.9249\n",
            "23428/23428 [==============================] - 2s 101us/step\n",
            "8000/8000 [==============================] - 1s 99us/step\n",
            "\n",
            "\n",
            "Ensayando modelo con estructura [512, 10, 1] y optimizador <keras.optimizers.Adam object at 0x7f535fc21be0>\n",
            "(23428, 1422) (8000, 1422) (13470, 1422) (23428,) (8000,) (13470,)\n",
            "Train on 23428 samples, validate on 8000 samples\n",
            "Epoch 1/60\n",
            "23428/23428 [==============================] - 2s 67us/step - loss: 18.9185 - binary_accuracy: 0.5322 - val_loss: 16.0607 - val_binary_accuracy: 0.5406\n",
            "Epoch 2/60\n",
            "23428/23428 [==============================] - 1s 59us/step - loss: 14.0754 - binary_accuracy: 0.5800 - val_loss: 11.5568 - val_binary_accuracy: 0.6062\n",
            "Epoch 3/60\n",
            "23428/23428 [==============================] - 1s 60us/step - loss: 9.9078 - binary_accuracy: 0.6124 - val_loss: 7.8475 - val_binary_accuracy: 0.6810\n",
            "Epoch 4/60\n",
            "23428/23428 [==============================] - 1s 59us/step - loss: 6.5480 - binary_accuracy: 0.6549 - val_loss: 4.9558 - val_binary_accuracy: 0.8035\n",
            "Epoch 5/60\n",
            "23428/23428 [==============================] - 1s 60us/step - loss: 4.0081 - binary_accuracy: 0.7062 - val_loss: 2.8855 - val_binary_accuracy: 0.8919\n",
            "Epoch 6/60\n",
            "23428/23428 [==============================] - 1s 59us/step - loss: 2.2920 - binary_accuracy: 0.7564 - val_loss: 1.6425 - val_binary_accuracy: 0.8900\n",
            "Epoch 7/60\n",
            "23428/23428 [==============================] - 1s 59us/step - loss: 1.4106 - binary_accuracy: 0.7665 - val_loss: 1.2392 - val_binary_accuracy: 0.9000\n",
            "Epoch 8/60\n",
            "23428/23428 [==============================] - 1s 59us/step - loss: 1.1781 - binary_accuracy: 0.7814 - val_loss: 1.0194 - val_binary_accuracy: 0.8832\n",
            "Epoch 9/60\n",
            "23428/23428 [==============================] - 1s 59us/step - loss: 0.9364 - binary_accuracy: 0.7955 - val_loss: 0.8493 - val_binary_accuracy: 0.8711\n",
            "Epoch 10/60\n",
            "23428/23428 [==============================] - 1s 59us/step - loss: 0.8189 - binary_accuracy: 0.7974 - val_loss: 0.7444 - val_binary_accuracy: 0.8763\n",
            "Epoch 11/60\n",
            "23428/23428 [==============================] - 1s 60us/step - loss: 0.7406 - binary_accuracy: 0.8083 - val_loss: 0.6854 - val_binary_accuracy: 0.8774\n",
            "Epoch 12/60\n",
            "23428/23428 [==============================] - 1s 60us/step - loss: 0.6875 - binary_accuracy: 0.8232 - val_loss: 0.6415 - val_binary_accuracy: 0.8780\n",
            "Epoch 13/60\n",
            "23428/23428 [==============================] - 1s 59us/step - loss: 0.6593 - binary_accuracy: 0.8233 - val_loss: 0.6098 - val_binary_accuracy: 0.8851\n",
            "Epoch 14/60\n",
            "23428/23428 [==============================] - 1s 59us/step - loss: 0.6362 - binary_accuracy: 0.8359 - val_loss: 0.5870 - val_binary_accuracy: 0.8848\n",
            "Epoch 15/60\n",
            "23428/23428 [==============================] - 1s 59us/step - loss: 0.6190 - binary_accuracy: 0.8427 - val_loss: 0.5684 - val_binary_accuracy: 0.8880\n",
            "Epoch 16/60\n",
            "23428/23428 [==============================] - 1s 60us/step - loss: 0.6060 - binary_accuracy: 0.8495 - val_loss: 0.5544 - val_binary_accuracy: 0.8914\n",
            "Epoch 17/60\n",
            "23428/23428 [==============================] - 1s 59us/step - loss: 0.5975 - binary_accuracy: 0.8524 - val_loss: 0.5432 - val_binary_accuracy: 0.8939\n",
            "Epoch 18/60\n",
            "23428/23428 [==============================] - 1s 60us/step - loss: 0.5877 - binary_accuracy: 0.8596 - val_loss: 0.5329 - val_binary_accuracy: 0.8963\n",
            "Epoch 19/60\n",
            "23428/23428 [==============================] - 1s 62us/step - loss: 0.5811 - binary_accuracy: 0.8630 - val_loss: 0.5252 - val_binary_accuracy: 0.8986\n",
            "Epoch 20/60\n",
            "23428/23428 [==============================] - 1s 62us/step - loss: 0.5720 - binary_accuracy: 0.8700 - val_loss: 0.5164 - val_binary_accuracy: 0.9003\n",
            "Epoch 21/60\n",
            "23428/23428 [==============================] - 1s 60us/step - loss: 0.5650 - binary_accuracy: 0.8704 - val_loss: 0.5092 - val_binary_accuracy: 0.9038\n",
            "Epoch 22/60\n",
            "23428/23428 [==============================] - 1s 59us/step - loss: 0.5586 - binary_accuracy: 0.8772 - val_loss: 0.5022 - val_binary_accuracy: 0.9064\n",
            "Epoch 23/60\n",
            "23428/23428 [==============================] - 1s 59us/step - loss: 0.5536 - binary_accuracy: 0.8766 - val_loss: 0.4950 - val_binary_accuracy: 0.9085\n",
            "Epoch 24/60\n",
            "23428/23428 [==============================] - 1s 59us/step - loss: 0.5465 - binary_accuracy: 0.8798 - val_loss: 0.4886 - val_binary_accuracy: 0.9124\n",
            "Epoch 25/60\n",
            "23428/23428 [==============================] - 1s 60us/step - loss: 0.5394 - binary_accuracy: 0.8849 - val_loss: 0.4817 - val_binary_accuracy: 0.9151\n",
            "Epoch 26/60\n",
            "23428/23428 [==============================] - 1s 60us/step - loss: 0.5344 - binary_accuracy: 0.8854 - val_loss: 0.4759 - val_binary_accuracy: 0.9199\n",
            "Epoch 27/60\n",
            "23428/23428 [==============================] - 1s 59us/step - loss: 0.5346 - binary_accuracy: 0.8874 - val_loss: 0.4717 - val_binary_accuracy: 0.9201\n",
            "Epoch 28/60\n",
            "23428/23428 [==============================] - 1s 59us/step - loss: 0.5276 - binary_accuracy: 0.8932 - val_loss: 0.4668 - val_binary_accuracy: 0.9206\n",
            "Epoch 29/60\n",
            "23428/23428 [==============================] - 1s 59us/step - loss: 0.5197 - binary_accuracy: 0.8959 - val_loss: 0.4587 - val_binary_accuracy: 0.9247\n",
            "Epoch 30/60\n",
            "23428/23428 [==============================] - 1s 59us/step - loss: 0.5148 - binary_accuracy: 0.8943 - val_loss: 0.4533 - val_binary_accuracy: 0.9262\n",
            "Epoch 31/60\n",
            "23428/23428 [==============================] - 1s 58us/step - loss: 0.5115 - binary_accuracy: 0.8964 - val_loss: 0.4522 - val_binary_accuracy: 0.9279\n",
            "Epoch 32/60\n",
            "23428/23428 [==============================] - 1s 59us/step - loss: 0.5044 - binary_accuracy: 0.9026 - val_loss: 0.4447 - val_binary_accuracy: 0.9281\n",
            "Epoch 33/60\n",
            "23428/23428 [==============================] - 1s 60us/step - loss: 0.5026 - binary_accuracy: 0.8999 - val_loss: 0.4395 - val_binary_accuracy: 0.9304\n",
            "Epoch 34/60\n",
            "23428/23428 [==============================] - 1s 59us/step - loss: 0.5012 - binary_accuracy: 0.9008 - val_loss: 0.4379 - val_binary_accuracy: 0.9314\n",
            "Epoch 35/60\n",
            "23428/23428 [==============================] - 1s 60us/step - loss: 0.4974 - binary_accuracy: 0.9052 - val_loss: 0.4326 - val_binary_accuracy: 0.9331\n",
            "Epoch 36/60\n",
            "23428/23428 [==============================] - 1s 59us/step - loss: 0.4933 - binary_accuracy: 0.9050 - val_loss: 0.4292 - val_binary_accuracy: 0.9340\n",
            "Epoch 37/60\n",
            "23428/23428 [==============================] - 1s 59us/step - loss: 0.4900 - binary_accuracy: 0.9073 - val_loss: 0.4259 - val_binary_accuracy: 0.9359\n",
            "Epoch 38/60\n",
            "23428/23428 [==============================] - 1s 59us/step - loss: 0.4855 - binary_accuracy: 0.9091 - val_loss: 0.4216 - val_binary_accuracy: 0.9367\n",
            "Epoch 39/60\n",
            "23428/23428 [==============================] - 1s 60us/step - loss: 0.4823 - binary_accuracy: 0.9102 - val_loss: 0.4178 - val_binary_accuracy: 0.9375\n",
            "Epoch 40/60\n",
            "23428/23428 [==============================] - 1s 59us/step - loss: 0.4787 - binary_accuracy: 0.9117 - val_loss: 0.4163 - val_binary_accuracy: 0.9386\n",
            "Epoch 41/60\n",
            "23428/23428 [==============================] - 1s 61us/step - loss: 0.4758 - binary_accuracy: 0.9110 - val_loss: 0.4118 - val_binary_accuracy: 0.9391\n",
            "Epoch 42/60\n",
            "23428/23428 [==============================] - 1s 59us/step - loss: 0.4730 - binary_accuracy: 0.9136 - val_loss: 0.4100 - val_binary_accuracy: 0.9410\n",
            "Epoch 43/60\n",
            "23428/23428 [==============================] - 1s 59us/step - loss: 0.4700 - binary_accuracy: 0.9136 - val_loss: 0.4059 - val_binary_accuracy: 0.9415\n",
            "Epoch 44/60\n",
            "23428/23428 [==============================] - 1s 58us/step - loss: 0.4637 - binary_accuracy: 0.9163 - val_loss: 0.4034 - val_binary_accuracy: 0.9395\n",
            "Epoch 45/60\n",
            "23428/23428 [==============================] - 1s 59us/step - loss: 0.4645 - binary_accuracy: 0.9164 - val_loss: 0.4003 - val_binary_accuracy: 0.9444\n",
            "Epoch 46/60\n",
            "23428/23428 [==============================] - 1s 59us/step - loss: 0.4581 - binary_accuracy: 0.9204 - val_loss: 0.3965 - val_binary_accuracy: 0.9444\n",
            "Epoch 47/60\n",
            "23428/23428 [==============================] - 1s 60us/step - loss: 0.4605 - binary_accuracy: 0.9171 - val_loss: 0.3947 - val_binary_accuracy: 0.9455\n",
            "Epoch 48/60\n",
            "23428/23428 [==============================] - 1s 60us/step - loss: 0.4547 - binary_accuracy: 0.9208 - val_loss: 0.3930 - val_binary_accuracy: 0.9450\n",
            "Epoch 49/60\n",
            "23428/23428 [==============================] - 1s 60us/step - loss: 0.4521 - binary_accuracy: 0.9218 - val_loss: 0.3900 - val_binary_accuracy: 0.9467\n",
            "Epoch 50/60\n",
            "23428/23428 [==============================] - 1s 59us/step - loss: 0.4536 - binary_accuracy: 0.9225 - val_loss: 0.3876 - val_binary_accuracy: 0.9463\n",
            "Epoch 51/60\n",
            "23428/23428 [==============================] - 1s 59us/step - loss: 0.4500 - binary_accuracy: 0.9228 - val_loss: 0.3842 - val_binary_accuracy: 0.9473\n",
            "Epoch 52/60\n",
            "23428/23428 [==============================] - 1s 60us/step - loss: 0.4479 - binary_accuracy: 0.9241 - val_loss: 0.3841 - val_binary_accuracy: 0.9473\n",
            "Epoch 53/60\n",
            "23428/23428 [==============================] - 1s 59us/step - loss: 0.4474 - binary_accuracy: 0.9242 - val_loss: 0.3811 - val_binary_accuracy: 0.9495\n",
            "Epoch 54/60\n",
            "23428/23428 [==============================] - 1s 59us/step - loss: 0.4427 - binary_accuracy: 0.9256 - val_loss: 0.3778 - val_binary_accuracy: 0.9505\n",
            "Epoch 55/60\n",
            "23428/23428 [==============================] - 1s 60us/step - loss: 0.4375 - binary_accuracy: 0.9254 - val_loss: 0.3755 - val_binary_accuracy: 0.9504\n",
            "Epoch 56/60\n",
            "23428/23428 [==============================] - 1s 60us/step - loss: 0.4388 - binary_accuracy: 0.9258 - val_loss: 0.3732 - val_binary_accuracy: 0.9509\n",
            "Epoch 57/60\n",
            "23428/23428 [==============================] - 1s 61us/step - loss: 0.4360 - binary_accuracy: 0.9256 - val_loss: 0.3717 - val_binary_accuracy: 0.9507\n",
            "Epoch 58/60\n",
            "23428/23428 [==============================] - 1s 59us/step - loss: 0.4313 - binary_accuracy: 0.9300 - val_loss: 0.3693 - val_binary_accuracy: 0.9515\n",
            "Epoch 59/60\n",
            "23428/23428 [==============================] - 1s 61us/step - loss: 0.4309 - binary_accuracy: 0.9274 - val_loss: 0.3666 - val_binary_accuracy: 0.9526\n",
            "Epoch 60/60\n",
            "23428/23428 [==============================] - 1s 60us/step - loss: 0.4276 - binary_accuracy: 0.9292 - val_loss: 0.3650 - val_binary_accuracy: 0.9539\n",
            "23428/23428 [==============================] - 3s 107us/step\n",
            "8000/8000 [==============================] - 1s 116us/step\n",
            "(23428, 1422) (8000, 1422) (13470, 1422) (23428,) (8000,) (13470,)\n",
            "Train on 23428 samples, validate on 8000 samples\n",
            "Epoch 1/60\n",
            "23428/23428 [==============================] - 1s 63us/step - loss: 20.3456 - binary_accuracy: 0.5362 - val_loss: 18.9390 - val_binary_accuracy: 0.5580\n",
            "Epoch 2/60\n",
            "23428/23428 [==============================] - 1s 59us/step - loss: 17.4122 - binary_accuracy: 0.5874 - val_loss: 15.2260 - val_binary_accuracy: 0.6388\n",
            "Epoch 3/60\n",
            "23428/23428 [==============================] - 1s 59us/step - loss: 13.5475 - binary_accuracy: 0.6212 - val_loss: 11.3328 - val_binary_accuracy: 0.6920\n",
            "Epoch 4/60\n",
            "23428/23428 [==============================] - 1s 60us/step - loss: 9.8049 - binary_accuracy: 0.6674 - val_loss: 7.8586 - val_binary_accuracy: 0.8273\n",
            "Epoch 5/60\n",
            "23428/23428 [==============================] - 1s 59us/step - loss: 6.5946 - binary_accuracy: 0.7252 - val_loss: 5.0248 - val_binary_accuracy: 0.8789\n",
            "Epoch 6/60\n",
            "23428/23428 [==============================] - 1s 60us/step - loss: 4.0876 - binary_accuracy: 0.7607 - val_loss: 2.9563 - val_binary_accuracy: 0.9100\n",
            "Epoch 7/60\n",
            "23428/23428 [==============================] - 1s 60us/step - loss: 2.3565 - binary_accuracy: 0.7966 - val_loss: 1.6841 - val_binary_accuracy: 0.9206\n",
            "Epoch 8/60\n",
            "23428/23428 [==============================] - 1s 59us/step - loss: 1.4318 - binary_accuracy: 0.8095 - val_loss: 1.1950 - val_binary_accuracy: 0.9089\n",
            "Epoch 9/60\n",
            "23428/23428 [==============================] - 1s 60us/step - loss: 1.1371 - binary_accuracy: 0.8147 - val_loss: 0.9722 - val_binary_accuracy: 0.8992\n",
            "Epoch 10/60\n",
            "23428/23428 [==============================] - 1s 59us/step - loss: 0.8918 - binary_accuracy: 0.8253 - val_loss: 0.7691 - val_binary_accuracy: 0.8956\n",
            "Epoch 11/60\n",
            "23428/23428 [==============================] - 1s 59us/step - loss: 0.7578 - binary_accuracy: 0.8317 - val_loss: 0.6595 - val_binary_accuracy: 0.8895\n",
            "Epoch 12/60\n",
            "23428/23428 [==============================] - 1s 59us/step - loss: 0.6749 - binary_accuracy: 0.8388 - val_loss: 0.5953 - val_binary_accuracy: 0.8982\n",
            "Epoch 13/60\n",
            "23428/23428 [==============================] - 1s 59us/step - loss: 0.6234 - binary_accuracy: 0.8539 - val_loss: 0.5525 - val_binary_accuracy: 0.8981\n",
            "Epoch 14/60\n",
            "23428/23428 [==============================] - 1s 59us/step - loss: 0.5946 - binary_accuracy: 0.8614 - val_loss: 0.5249 - val_binary_accuracy: 0.9011\n",
            "Epoch 15/60\n",
            "23428/23428 [==============================] - 1s 59us/step - loss: 0.5741 - binary_accuracy: 0.8664 - val_loss: 0.5051 - val_binary_accuracy: 0.9064\n",
            "Epoch 16/60\n",
            "23428/23428 [==============================] - 1s 59us/step - loss: 0.5594 - binary_accuracy: 0.8744 - val_loss: 0.4910 - val_binary_accuracy: 0.9097\n",
            "Epoch 17/60\n",
            "23428/23428 [==============================] - 1s 60us/step - loss: 0.5457 - binary_accuracy: 0.8805 - val_loss: 0.4781 - val_binary_accuracy: 0.9176\n",
            "Epoch 18/60\n",
            "23428/23428 [==============================] - 1s 60us/step - loss: 0.5362 - binary_accuracy: 0.8829 - val_loss: 0.4672 - val_binary_accuracy: 0.9240\n",
            "Epoch 19/60\n",
            "23428/23428 [==============================] - 1s 59us/step - loss: 0.5243 - binary_accuracy: 0.8895 - val_loss: 0.4573 - val_binary_accuracy: 0.9247\n",
            "Epoch 20/60\n",
            "23428/23428 [==============================] - 1s 59us/step - loss: 0.5134 - binary_accuracy: 0.8943 - val_loss: 0.4482 - val_binary_accuracy: 0.9264\n",
            "Epoch 21/60\n",
            "23428/23428 [==============================] - 1s 59us/step - loss: 0.5120 - binary_accuracy: 0.8928 - val_loss: 0.4418 - val_binary_accuracy: 0.9280\n",
            "Epoch 22/60\n",
            "23428/23428 [==============================] - 1s 59us/step - loss: 0.5029 - binary_accuracy: 0.8994 - val_loss: 0.4343 - val_binary_accuracy: 0.9306\n",
            "Epoch 23/60\n",
            "23428/23428 [==============================] - 1s 59us/step - loss: 0.4946 - binary_accuracy: 0.9031 - val_loss: 0.4269 - val_binary_accuracy: 0.9326\n",
            "Epoch 24/60\n",
            "23428/23428 [==============================] - 1s 60us/step - loss: 0.4894 - binary_accuracy: 0.9042 - val_loss: 0.4216 - val_binary_accuracy: 0.9349\n",
            "Epoch 25/60\n",
            "23428/23428 [==============================] - 1s 58us/step - loss: 0.4828 - binary_accuracy: 0.9070 - val_loss: 0.4162 - val_binary_accuracy: 0.9360\n",
            "Epoch 26/60\n",
            "23428/23428 [==============================] - 1s 59us/step - loss: 0.4804 - binary_accuracy: 0.9070 - val_loss: 0.4123 - val_binary_accuracy: 0.9375\n",
            "Epoch 27/60\n",
            "23428/23428 [==============================] - 1s 59us/step - loss: 0.4742 - binary_accuracy: 0.9100 - val_loss: 0.4069 - val_binary_accuracy: 0.9398\n",
            "Epoch 28/60\n",
            "23428/23428 [==============================] - 1s 59us/step - loss: 0.4708 - binary_accuracy: 0.9115 - val_loss: 0.4012 - val_binary_accuracy: 0.9423\n",
            "Epoch 29/60\n",
            "23428/23428 [==============================] - 1s 60us/step - loss: 0.4627 - binary_accuracy: 0.9165 - val_loss: 0.3967 - val_binary_accuracy: 0.9415\n",
            "Epoch 30/60\n",
            "23428/23428 [==============================] - 1s 58us/step - loss: 0.4632 - binary_accuracy: 0.9148 - val_loss: 0.3934 - val_binary_accuracy: 0.9435\n",
            "Epoch 31/60\n",
            "23428/23428 [==============================] - 1s 59us/step - loss: 0.4574 - binary_accuracy: 0.9169 - val_loss: 0.3926 - val_binary_accuracy: 0.9434\n",
            "Epoch 32/60\n",
            "23428/23428 [==============================] - 1s 60us/step - loss: 0.4537 - binary_accuracy: 0.9185 - val_loss: 0.3867 - val_binary_accuracy: 0.9449\n",
            "Epoch 33/60\n",
            "23428/23428 [==============================] - 1s 58us/step - loss: 0.4516 - binary_accuracy: 0.9186 - val_loss: 0.3827 - val_binary_accuracy: 0.9448\n",
            "Epoch 34/60\n",
            "23428/23428 [==============================] - 1s 59us/step - loss: 0.4476 - binary_accuracy: 0.9224 - val_loss: 0.3809 - val_binary_accuracy: 0.9464\n",
            "Epoch 35/60\n",
            "23428/23428 [==============================] - 1s 59us/step - loss: 0.4456 - binary_accuracy: 0.9212 - val_loss: 0.3776 - val_binary_accuracy: 0.9477\n",
            "Epoch 36/60\n",
            "23428/23428 [==============================] - 1s 58us/step - loss: 0.4395 - binary_accuracy: 0.9249 - val_loss: 0.3747 - val_binary_accuracy: 0.9476\n",
            "Epoch 37/60\n",
            "23428/23428 [==============================] - 1s 58us/step - loss: 0.4426 - binary_accuracy: 0.9220 - val_loss: 0.3729 - val_binary_accuracy: 0.9491\n",
            "Epoch 38/60\n",
            "23428/23428 [==============================] - 1s 58us/step - loss: 0.4386 - binary_accuracy: 0.9242 - val_loss: 0.3714 - val_binary_accuracy: 0.9503\n",
            "Epoch 39/60\n",
            "23428/23428 [==============================] - 1s 58us/step - loss: 0.4347 - binary_accuracy: 0.9258 - val_loss: 0.3690 - val_binary_accuracy: 0.9506\n",
            "Epoch 40/60\n",
            "23428/23428 [==============================] - 1s 59us/step - loss: 0.4331 - binary_accuracy: 0.9249 - val_loss: 0.3658 - val_binary_accuracy: 0.9513\n",
            "Epoch 41/60\n",
            "23428/23428 [==============================] - 1s 60us/step - loss: 0.4308 - binary_accuracy: 0.9277 - val_loss: 0.3629 - val_binary_accuracy: 0.9521\n",
            "Epoch 42/60\n",
            "23428/23428 [==============================] - 1s 60us/step - loss: 0.4290 - binary_accuracy: 0.9282 - val_loss: 0.3632 - val_binary_accuracy: 0.9516\n",
            "Epoch 43/60\n",
            "23428/23428 [==============================] - 1s 60us/step - loss: 0.4268 - binary_accuracy: 0.9297 - val_loss: 0.3589 - val_binary_accuracy: 0.9541\n",
            "Epoch 44/60\n",
            "23428/23428 [==============================] - 1s 59us/step - loss: 0.4245 - binary_accuracy: 0.9301 - val_loss: 0.3571 - val_binary_accuracy: 0.9545\n",
            "Epoch 45/60\n",
            "23428/23428 [==============================] - 1s 60us/step - loss: 0.4200 - binary_accuracy: 0.9312 - val_loss: 0.3543 - val_binary_accuracy: 0.9545\n",
            "Epoch 46/60\n",
            "23428/23428 [==============================] - 1s 60us/step - loss: 0.4160 - binary_accuracy: 0.9326 - val_loss: 0.3522 - val_binary_accuracy: 0.9550\n",
            "Epoch 47/60\n",
            "23428/23428 [==============================] - 1s 59us/step - loss: 0.4157 - binary_accuracy: 0.9307 - val_loss: 0.3507 - val_binary_accuracy: 0.9555\n",
            "Epoch 48/60\n",
            "23428/23428 [==============================] - 1s 60us/step - loss: 0.4143 - binary_accuracy: 0.9329 - val_loss: 0.3488 - val_binary_accuracy: 0.9549\n",
            "Epoch 49/60\n",
            "23428/23428 [==============================] - 2s 65us/step - loss: 0.4130 - binary_accuracy: 0.9330 - val_loss: 0.3486 - val_binary_accuracy: 0.9556\n",
            "Epoch 50/60\n",
            "23428/23428 [==============================] - 1s 61us/step - loss: 0.4115 - binary_accuracy: 0.9340 - val_loss: 0.3459 - val_binary_accuracy: 0.9563\n",
            "Epoch 51/60\n",
            "23428/23428 [==============================] - 1s 61us/step - loss: 0.4092 - binary_accuracy: 0.9341 - val_loss: 0.3436 - val_binary_accuracy: 0.9575\n",
            "Epoch 52/60\n",
            "23428/23428 [==============================] - 1s 59us/step - loss: 0.4059 - binary_accuracy: 0.9354 - val_loss: 0.3427 - val_binary_accuracy: 0.9565\n",
            "Epoch 53/60\n",
            "23428/23428 [==============================] - 1s 59us/step - loss: 0.4046 - binary_accuracy: 0.9355 - val_loss: 0.3404 - val_binary_accuracy: 0.9584\n",
            "Epoch 54/60\n",
            "23428/23428 [==============================] - 1s 59us/step - loss: 0.4048 - binary_accuracy: 0.9361 - val_loss: 0.3402 - val_binary_accuracy: 0.9575\n",
            "Epoch 55/60\n",
            "23428/23428 [==============================] - 1s 58us/step - loss: 0.4005 - binary_accuracy: 0.9376 - val_loss: 0.3387 - val_binary_accuracy: 0.9585\n",
            "Epoch 56/60\n",
            "23428/23428 [==============================] - 1s 59us/step - loss: 0.4020 - binary_accuracy: 0.9358 - val_loss: 0.3408 - val_binary_accuracy: 0.9575\n",
            "Epoch 57/60\n",
            "23428/23428 [==============================] - 1s 59us/step - loss: 0.4049 - binary_accuracy: 0.9369 - val_loss: 0.3363 - val_binary_accuracy: 0.9590\n",
            "Epoch 58/60\n",
            "23428/23428 [==============================] - 1s 58us/step - loss: 0.3976 - binary_accuracy: 0.9398 - val_loss: 0.3342 - val_binary_accuracy: 0.9590\n",
            "Epoch 59/60\n",
            "23428/23428 [==============================] - 1s 58us/step - loss: 0.3958 - binary_accuracy: 0.9403 - val_loss: 0.3324 - val_binary_accuracy: 0.9594\n",
            "Epoch 60/60\n",
            "23428/23428 [==============================] - 1s 58us/step - loss: 0.3944 - binary_accuracy: 0.9413 - val_loss: 0.3307 - val_binary_accuracy: 0.9610\n",
            "23428/23428 [==============================] - 2s 98us/step\n",
            "8000/8000 [==============================] - 1s 95us/step\n",
            "(23428, 1422) (8000, 1422) (13470, 1422) (23428,) (8000,) (13470,)\n",
            "Train on 23428 samples, validate on 8000 samples\n",
            "Epoch 1/60\n",
            "23428/23428 [==============================] - 1s 59us/step - loss: 20.3504 - binary_accuracy: 0.5329 - val_loss: 18.9542 - val_binary_accuracy: 0.5266\n",
            "Epoch 2/60\n",
            "23428/23428 [==============================] - 1s 59us/step - loss: 17.4371 - binary_accuracy: 0.5586 - val_loss: 15.2630 - val_binary_accuracy: 0.5304\n",
            "Epoch 3/60\n",
            "23428/23428 [==============================] - 1s 59us/step - loss: 13.5918 - binary_accuracy: 0.5879 - val_loss: 11.3868 - val_binary_accuracy: 0.5771\n",
            "Epoch 4/60\n",
            "23428/23428 [==============================] - 1s 58us/step - loss: 9.8594 - binary_accuracy: 0.6392 - val_loss: 7.9134 - val_binary_accuracy: 0.7630\n",
            "Epoch 5/60\n",
            "23428/23428 [==============================] - 1s 58us/step - loss: 6.6520 - binary_accuracy: 0.7030 - val_loss: 5.0864 - val_binary_accuracy: 0.8555\n",
            "Epoch 6/60\n",
            "23428/23428 [==============================] - 1s 58us/step - loss: 4.1365 - binary_accuracy: 0.7559 - val_loss: 3.0041 - val_binary_accuracy: 0.8896\n",
            "Epoch 7/60\n",
            "23428/23428 [==============================] - 1s 59us/step - loss: 2.3942 - binary_accuracy: 0.7777 - val_loss: 1.7210 - val_binary_accuracy: 0.9178\n",
            "Epoch 8/60\n",
            "23428/23428 [==============================] - 1s 59us/step - loss: 1.4634 - binary_accuracy: 0.7925 - val_loss: 1.2288 - val_binary_accuracy: 0.9064\n",
            "Epoch 9/60\n",
            "23428/23428 [==============================] - 1s 59us/step - loss: 1.1640 - binary_accuracy: 0.8001 - val_loss: 1.0095 - val_binary_accuracy: 0.8811\n",
            "Epoch 10/60\n",
            "23428/23428 [==============================] - 1s 60us/step - loss: 0.9227 - binary_accuracy: 0.8063 - val_loss: 0.8054 - val_binary_accuracy: 0.8714\n",
            "Epoch 11/60\n",
            "23428/23428 [==============================] - 1s 59us/step - loss: 0.7864 - binary_accuracy: 0.8086 - val_loss: 0.6958 - val_binary_accuracy: 0.8758\n",
            "Epoch 12/60\n",
            "23428/23428 [==============================] - 1s 59us/step - loss: 0.6980 - binary_accuracy: 0.8229 - val_loss: 0.6268 - val_binary_accuracy: 0.8816\n",
            "Epoch 13/60\n",
            "23428/23428 [==============================] - 1s 59us/step - loss: 0.6499 - binary_accuracy: 0.8310 - val_loss: 0.5821 - val_binary_accuracy: 0.8850\n",
            "Epoch 14/60\n",
            "23428/23428 [==============================] - 1s 59us/step - loss: 0.6190 - binary_accuracy: 0.8442 - val_loss: 0.5517 - val_binary_accuracy: 0.8901\n",
            "Epoch 15/60\n",
            "23428/23428 [==============================] - 1s 60us/step - loss: 0.5947 - binary_accuracy: 0.8523 - val_loss: 0.5289 - val_binary_accuracy: 0.8949\n",
            "Epoch 16/60\n",
            "23428/23428 [==============================] - 1s 59us/step - loss: 0.5794 - binary_accuracy: 0.8579 - val_loss: 0.5117 - val_binary_accuracy: 0.9013\n",
            "Epoch 17/60\n",
            "23428/23428 [==============================] - 1s 59us/step - loss: 0.5672 - binary_accuracy: 0.8671 - val_loss: 0.4984 - val_binary_accuracy: 0.9055\n",
            "Epoch 18/60\n",
            "23428/23428 [==============================] - 1s 60us/step - loss: 0.5573 - binary_accuracy: 0.8714 - val_loss: 0.4852 - val_binary_accuracy: 0.9134\n",
            "Epoch 19/60\n",
            "23428/23428 [==============================] - 1s 58us/step - loss: 0.5460 - binary_accuracy: 0.8755 - val_loss: 0.4742 - val_binary_accuracy: 0.9181\n",
            "Epoch 20/60\n",
            "23428/23428 [==============================] - 1s 58us/step - loss: 0.5358 - binary_accuracy: 0.8805 - val_loss: 0.4630 - val_binary_accuracy: 0.9193\n",
            "Epoch 21/60\n",
            "23428/23428 [==============================] - 1s 59us/step - loss: 0.5249 - binary_accuracy: 0.8890 - val_loss: 0.4530 - val_binary_accuracy: 0.9276\n",
            "Epoch 22/60\n",
            "23428/23428 [==============================] - 1s 59us/step - loss: 0.5160 - binary_accuracy: 0.8880 - val_loss: 0.4437 - val_binary_accuracy: 0.9283\n",
            "Epoch 23/60\n",
            "23428/23428 [==============================] - 1s 58us/step - loss: 0.5087 - binary_accuracy: 0.8955 - val_loss: 0.4354 - val_binary_accuracy: 0.9300\n",
            "Epoch 24/60\n",
            "23428/23428 [==============================] - 1s 59us/step - loss: 0.5006 - binary_accuracy: 0.8989 - val_loss: 0.4275 - val_binary_accuracy: 0.9325\n",
            "Epoch 25/60\n",
            "23428/23428 [==============================] - 1s 60us/step - loss: 0.4921 - binary_accuracy: 0.9005 - val_loss: 0.4201 - val_binary_accuracy: 0.9354\n",
            "Epoch 26/60\n",
            "23428/23428 [==============================] - 1s 61us/step - loss: 0.4872 - binary_accuracy: 0.9048 - val_loss: 0.4121 - val_binary_accuracy: 0.9374\n",
            "Epoch 27/60\n",
            "23428/23428 [==============================] - 1s 60us/step - loss: 0.4828 - binary_accuracy: 0.9047 - val_loss: 0.4071 - val_binary_accuracy: 0.9400\n",
            "Epoch 28/60\n",
            "23428/23428 [==============================] - 1s 59us/step - loss: 0.4739 - binary_accuracy: 0.9088 - val_loss: 0.4010 - val_binary_accuracy: 0.9419\n",
            "Epoch 29/60\n",
            "23428/23428 [==============================] - 1s 59us/step - loss: 0.4699 - binary_accuracy: 0.9111 - val_loss: 0.3961 - val_binary_accuracy: 0.9427\n",
            "Epoch 30/60\n",
            "23428/23428 [==============================] - 1s 60us/step - loss: 0.4642 - binary_accuracy: 0.9138 - val_loss: 0.3903 - val_binary_accuracy: 0.9455\n",
            "Epoch 31/60\n",
            "23428/23428 [==============================] - 1s 59us/step - loss: 0.4629 - binary_accuracy: 0.9139 - val_loss: 0.3863 - val_binary_accuracy: 0.9460\n",
            "Epoch 32/60\n",
            "23428/23428 [==============================] - 1s 59us/step - loss: 0.4592 - binary_accuracy: 0.9155 - val_loss: 0.3835 - val_binary_accuracy: 0.9473\n",
            "Epoch 33/60\n",
            "23428/23428 [==============================] - 1s 59us/step - loss: 0.4515 - binary_accuracy: 0.9199 - val_loss: 0.3786 - val_binary_accuracy: 0.9479\n",
            "Epoch 34/60\n",
            "23428/23428 [==============================] - 1s 59us/step - loss: 0.4490 - binary_accuracy: 0.9197 - val_loss: 0.3740 - val_binary_accuracy: 0.9485\n",
            "Epoch 35/60\n",
            "23428/23428 [==============================] - 1s 59us/step - loss: 0.4458 - binary_accuracy: 0.9208 - val_loss: 0.3713 - val_binary_accuracy: 0.9511\n",
            "Epoch 36/60\n",
            "23428/23428 [==============================] - 1s 58us/step - loss: 0.4403 - binary_accuracy: 0.9215 - val_loss: 0.3671 - val_binary_accuracy: 0.9504\n",
            "Epoch 37/60\n",
            "23428/23428 [==============================] - 1s 60us/step - loss: 0.4397 - binary_accuracy: 0.9215 - val_loss: 0.3650 - val_binary_accuracy: 0.9505\n",
            "Epoch 38/60\n",
            "23428/23428 [==============================] - 1s 59us/step - loss: 0.4331 - binary_accuracy: 0.9240 - val_loss: 0.3686 - val_binary_accuracy: 0.9511\n",
            "Epoch 39/60\n",
            "23428/23428 [==============================] - 1s 58us/step - loss: 0.4355 - binary_accuracy: 0.9232 - val_loss: 0.3605 - val_binary_accuracy: 0.9540\n",
            "Epoch 40/60\n",
            "23428/23428 [==============================] - 1s 59us/step - loss: 0.4306 - binary_accuracy: 0.9270 - val_loss: 0.3565 - val_binary_accuracy: 0.9549\n",
            "Epoch 41/60\n",
            "23428/23428 [==============================] - 1s 59us/step - loss: 0.4265 - binary_accuracy: 0.9287 - val_loss: 0.3540 - val_binary_accuracy: 0.9539\n",
            "Epoch 42/60\n",
            "23428/23428 [==============================] - 1s 58us/step - loss: 0.4250 - binary_accuracy: 0.9279 - val_loss: 0.3522 - val_binary_accuracy: 0.9548\n",
            "Epoch 43/60\n",
            "23428/23428 [==============================] - 1s 59us/step - loss: 0.4189 - binary_accuracy: 0.9304 - val_loss: 0.3479 - val_binary_accuracy: 0.9555\n",
            "Epoch 44/60\n",
            "23428/23428 [==============================] - 1s 58us/step - loss: 0.4165 - binary_accuracy: 0.9311 - val_loss: 0.3458 - val_binary_accuracy: 0.9567\n",
            "Epoch 45/60\n",
            "23428/23428 [==============================] - 1s 59us/step - loss: 0.4140 - binary_accuracy: 0.9319 - val_loss: 0.3431 - val_binary_accuracy: 0.9563\n",
            "Epoch 46/60\n",
            "23428/23428 [==============================] - 1s 59us/step - loss: 0.4149 - binary_accuracy: 0.9308 - val_loss: 0.3460 - val_binary_accuracy: 0.9564\n",
            "Epoch 47/60\n",
            "23428/23428 [==============================] - 1s 59us/step - loss: 0.4163 - binary_accuracy: 0.9312 - val_loss: 0.3434 - val_binary_accuracy: 0.9569\n",
            "Epoch 48/60\n",
            "23428/23428 [==============================] - 1s 58us/step - loss: 0.4127 - binary_accuracy: 0.9325 - val_loss: 0.3407 - val_binary_accuracy: 0.9588\n",
            "Epoch 49/60\n",
            "23428/23428 [==============================] - 1s 60us/step - loss: 0.4070 - binary_accuracy: 0.9349 - val_loss: 0.3367 - val_binary_accuracy: 0.9591\n",
            "Epoch 50/60\n",
            "23428/23428 [==============================] - 1s 58us/step - loss: 0.4046 - binary_accuracy: 0.9336 - val_loss: 0.3345 - val_binary_accuracy: 0.9586\n",
            "Epoch 51/60\n",
            "23428/23428 [==============================] - 1s 59us/step - loss: 0.4049 - binary_accuracy: 0.9328 - val_loss: 0.3339 - val_binary_accuracy: 0.9590\n",
            "Epoch 52/60\n",
            "23428/23428 [==============================] - 1s 59us/step - loss: 0.4003 - binary_accuracy: 0.9365 - val_loss: 0.3319 - val_binary_accuracy: 0.9605\n",
            "Epoch 53/60\n",
            "23428/23428 [==============================] - 1s 58us/step - loss: 0.3992 - binary_accuracy: 0.9364 - val_loss: 0.3297 - val_binary_accuracy: 0.9607\n",
            "Epoch 54/60\n",
            "23428/23428 [==============================] - 1s 59us/step - loss: 0.3999 - binary_accuracy: 0.9355 - val_loss: 0.3286 - val_binary_accuracy: 0.9606\n",
            "Epoch 55/60\n",
            "23428/23428 [==============================] - 1s 58us/step - loss: 0.3976 - binary_accuracy: 0.9356 - val_loss: 0.3287 - val_binary_accuracy: 0.9604\n",
            "Epoch 56/60\n",
            "23428/23428 [==============================] - 1s 58us/step - loss: 0.3948 - binary_accuracy: 0.9373 - val_loss: 0.3258 - val_binary_accuracy: 0.9611\n",
            "Epoch 57/60\n",
            "23428/23428 [==============================] - 1s 59us/step - loss: 0.3954 - binary_accuracy: 0.9383 - val_loss: 0.3252 - val_binary_accuracy: 0.9615\n",
            "Epoch 58/60\n",
            "23428/23428 [==============================] - 1s 59us/step - loss: 0.3915 - binary_accuracy: 0.9387 - val_loss: 0.3235 - val_binary_accuracy: 0.9613\n",
            "Epoch 59/60\n",
            "23428/23428 [==============================] - 1s 59us/step - loss: 0.3869 - binary_accuracy: 0.9411 - val_loss: 0.3202 - val_binary_accuracy: 0.9619\n",
            "Epoch 60/60\n",
            "23428/23428 [==============================] - 1s 58us/step - loss: 0.3857 - binary_accuracy: 0.9378 - val_loss: 0.3221 - val_binary_accuracy: 0.9616\n",
            "23428/23428 [==============================] - 2s 105us/step\n",
            "8000/8000 [==============================] - 1s 95us/step\n",
            "\n",
            "\n",
            "Ensayando modelo con estructura [512, 10, 1] y optimizador <keras.optimizers.RMSprop object at 0x7f535fc21390>\n",
            "(23428, 1422) (8000, 1422) (13470, 1422) (23428,) (8000,) (13470,)\n",
            "Train on 23428 samples, validate on 8000 samples\n",
            "Epoch 1/60\n",
            "23428/23428 [==============================] - 2s 65us/step - loss: 16.0530 - binary_accuracy: 0.5323 - val_loss: 11.7277 - val_binary_accuracy: 0.5229\n",
            "Epoch 2/60\n",
            "23428/23428 [==============================] - 1s 59us/step - loss: 9.6898 - binary_accuracy: 0.5860 - val_loss: 7.3102 - val_binary_accuracy: 0.5608\n",
            "Epoch 3/60\n",
            "23428/23428 [==============================] - 1s 57us/step - loss: 5.9453 - binary_accuracy: 0.6651 - val_loss: 4.3231 - val_binary_accuracy: 0.5725\n",
            "Epoch 4/60\n",
            "23428/23428 [==============================] - 1s 58us/step - loss: 3.4067 - binary_accuracy: 0.7092 - val_loss: 2.3481 - val_binary_accuracy: 0.8633\n",
            "Epoch 5/60\n",
            "23428/23428 [==============================] - 1s 58us/step - loss: 1.8181 - binary_accuracy: 0.7485 - val_loss: 1.2633 - val_binary_accuracy: 0.7830\n",
            "Epoch 6/60\n",
            "23428/23428 [==============================] - 1s 58us/step - loss: 1.1035 - binary_accuracy: 0.7379 - val_loss: 1.0073 - val_binary_accuracy: 0.8701\n",
            "Epoch 7/60\n",
            "23428/23428 [==============================] - 1s 59us/step - loss: 1.0070 - binary_accuracy: 0.7669 - val_loss: 0.9944 - val_binary_accuracy: 0.8811\n",
            "Epoch 8/60\n",
            "23428/23428 [==============================] - 1s 60us/step - loss: 0.9973 - binary_accuracy: 0.7708 - val_loss: 0.9729 - val_binary_accuracy: 0.8365\n",
            "Epoch 9/60\n",
            "23428/23428 [==============================] - 1s 58us/step - loss: 0.9794 - binary_accuracy: 0.7985 - val_loss: 0.9627 - val_binary_accuracy: 0.7703\n",
            "Epoch 10/60\n",
            "23428/23428 [==============================] - 1s 59us/step - loss: 0.9719 - binary_accuracy: 0.7926 - val_loss: 0.9498 - val_binary_accuracy: 0.7786\n",
            "Epoch 11/60\n",
            "23428/23428 [==============================] - 1s 59us/step - loss: 0.9548 - binary_accuracy: 0.8123 - val_loss: 0.9258 - val_binary_accuracy: 0.8285\n",
            "Epoch 12/60\n",
            "23428/23428 [==============================] - 1s 58us/step - loss: 0.9405 - binary_accuracy: 0.8192 - val_loss: 0.9198 - val_binary_accuracy: 0.8083\n",
            "Epoch 13/60\n",
            "23428/23428 [==============================] - 1s 58us/step - loss: 0.9360 - binary_accuracy: 0.8141 - val_loss: 0.8909 - val_binary_accuracy: 0.8454\n",
            "Epoch 14/60\n",
            "23428/23428 [==============================] - 1s 58us/step - loss: 0.9169 - binary_accuracy: 0.8269 - val_loss: 0.8889 - val_binary_accuracy: 0.8259\n",
            "Epoch 15/60\n",
            "23428/23428 [==============================] - 1s 58us/step - loss: 0.9115 - binary_accuracy: 0.8250 - val_loss: 0.8652 - val_binary_accuracy: 0.8485\n",
            "Epoch 16/60\n",
            "23428/23428 [==============================] - 1s 59us/step - loss: 0.8955 - binary_accuracy: 0.8345 - val_loss: 0.8580 - val_binary_accuracy: 0.8489\n",
            "Epoch 17/60\n",
            "23428/23428 [==============================] - 1s 59us/step - loss: 0.8908 - binary_accuracy: 0.8363 - val_loss: 0.8498 - val_binary_accuracy: 0.8503\n",
            "Epoch 18/60\n",
            "23428/23428 [==============================] - 1s 58us/step - loss: 0.8833 - binary_accuracy: 0.8401 - val_loss: 0.8402 - val_binary_accuracy: 0.8544\n",
            "Epoch 19/60\n",
            "23428/23428 [==============================] - 1s 58us/step - loss: 0.8764 - binary_accuracy: 0.8438 - val_loss: 0.8351 - val_binary_accuracy: 0.8562\n",
            "Epoch 20/60\n",
            "23428/23428 [==============================] - 1s 58us/step - loss: 0.8723 - binary_accuracy: 0.8439 - val_loss: 0.8272 - val_binary_accuracy: 0.8597\n",
            "Epoch 21/60\n",
            "23428/23428 [==============================] - 1s 59us/step - loss: 0.8645 - binary_accuracy: 0.8502 - val_loss: 0.8157 - val_binary_accuracy: 0.8683\n",
            "Epoch 22/60\n",
            "23428/23428 [==============================] - 1s 58us/step - loss: 0.8676 - binary_accuracy: 0.8468 - val_loss: 0.8103 - val_binary_accuracy: 0.8675\n",
            "Epoch 23/60\n",
            "23428/23428 [==============================] - 1s 58us/step - loss: 0.8534 - binary_accuracy: 0.8529 - val_loss: 0.8131 - val_binary_accuracy: 0.8637\n",
            "Epoch 24/60\n",
            "23428/23428 [==============================] - 1s 58us/step - loss: 0.8438 - binary_accuracy: 0.8600 - val_loss: 0.7919 - val_binary_accuracy: 0.8786\n",
            "Epoch 25/60\n",
            "23428/23428 [==============================] - 1s 58us/step - loss: 0.8633 - binary_accuracy: 0.8498 - val_loss: 0.8140 - val_binary_accuracy: 0.8620\n",
            "Epoch 26/60\n",
            "23428/23428 [==============================] - 1s 58us/step - loss: 0.8450 - binary_accuracy: 0.8553 - val_loss: 0.7919 - val_binary_accuracy: 0.8767\n",
            "Epoch 27/60\n",
            "23428/23428 [==============================] - 1s 58us/step - loss: 0.8402 - binary_accuracy: 0.8595 - val_loss: 0.8084 - val_binary_accuracy: 0.8640\n",
            "Epoch 28/60\n",
            "23428/23428 [==============================] - 1s 58us/step - loss: 0.8391 - binary_accuracy: 0.8587 - val_loss: 0.7754 - val_binary_accuracy: 0.8846\n",
            "Epoch 29/60\n",
            "23428/23428 [==============================] - 1s 57us/step - loss: 0.8330 - binary_accuracy: 0.8629 - val_loss: 0.7850 - val_binary_accuracy: 0.8769\n",
            "Epoch 30/60\n",
            "23428/23428 [==============================] - 1s 58us/step - loss: 0.8304 - binary_accuracy: 0.8666 - val_loss: 0.7838 - val_binary_accuracy: 0.8759\n",
            "Epoch 31/60\n",
            "23428/23428 [==============================] - 1s 58us/step - loss: 0.8289 - binary_accuracy: 0.8657 - val_loss: 0.7794 - val_binary_accuracy: 0.8789\n",
            "Epoch 32/60\n",
            "23428/23428 [==============================] - 1s 57us/step - loss: 0.8248 - binary_accuracy: 0.8669 - val_loss: 0.7750 - val_binary_accuracy: 0.8819\n",
            "Epoch 33/60\n",
            "23428/23428 [==============================] - 1s 59us/step - loss: 0.8174 - binary_accuracy: 0.8700 - val_loss: 0.7602 - val_binary_accuracy: 0.8903\n",
            "Epoch 34/60\n",
            "23428/23428 [==============================] - 1s 57us/step - loss: 0.8228 - binary_accuracy: 0.8684 - val_loss: 0.7742 - val_binary_accuracy: 0.8820\n",
            "Epoch 35/60\n",
            "23428/23428 [==============================] - 1s 58us/step - loss: 0.8233 - binary_accuracy: 0.8699 - val_loss: 0.7616 - val_binary_accuracy: 0.8876\n",
            "Epoch 36/60\n",
            "23428/23428 [==============================] - 1s 59us/step - loss: 0.8177 - binary_accuracy: 0.8715 - val_loss: 0.7553 - val_binary_accuracy: 0.8888\n",
            "Epoch 37/60\n",
            "23428/23428 [==============================] - 1s 57us/step - loss: 0.8080 - binary_accuracy: 0.8772 - val_loss: 0.7495 - val_binary_accuracy: 0.8960\n",
            "Epoch 38/60\n",
            "23428/23428 [==============================] - 1s 58us/step - loss: 0.8053 - binary_accuracy: 0.8749 - val_loss: 0.7629 - val_binary_accuracy: 0.8857\n",
            "Epoch 39/60\n",
            "23428/23428 [==============================] - 1s 58us/step - loss: 0.8115 - binary_accuracy: 0.8749 - val_loss: 0.7424 - val_binary_accuracy: 0.8961\n",
            "Epoch 40/60\n",
            "23428/23428 [==============================] - 1s 58us/step - loss: 0.7968 - binary_accuracy: 0.8815 - val_loss: 0.7495 - val_binary_accuracy: 0.8911\n",
            "Epoch 41/60\n",
            "23428/23428 [==============================] - 1s 58us/step - loss: 0.8005 - binary_accuracy: 0.8781 - val_loss: 0.7306 - val_binary_accuracy: 0.9057\n",
            "Epoch 42/60\n",
            "23428/23428 [==============================] - 1s 58us/step - loss: 0.8030 - binary_accuracy: 0.8777 - val_loss: 0.7506 - val_binary_accuracy: 0.8938\n",
            "Epoch 43/60\n",
            "23428/23428 [==============================] - 1s 58us/step - loss: 0.8011 - binary_accuracy: 0.8789 - val_loss: 0.7466 - val_binary_accuracy: 0.8941\n",
            "Epoch 44/60\n",
            "23428/23428 [==============================] - 1s 57us/step - loss: 0.7954 - binary_accuracy: 0.8794 - val_loss: 0.7276 - val_binary_accuracy: 0.9061\n",
            "Epoch 45/60\n",
            "23428/23428 [==============================] - 1s 58us/step - loss: 0.7838 - binary_accuracy: 0.8869 - val_loss: 0.7471 - val_binary_accuracy: 0.8940\n",
            "Epoch 46/60\n",
            "23428/23428 [==============================] - 1s 57us/step - loss: 0.7972 - binary_accuracy: 0.8808 - val_loss: 0.7305 - val_binary_accuracy: 0.9010\n",
            "Epoch 47/60\n",
            "23428/23428 [==============================] - 1s 57us/step - loss: 0.7861 - binary_accuracy: 0.8837 - val_loss: 0.7261 - val_binary_accuracy: 0.9050\n",
            "Epoch 48/60\n",
            "23428/23428 [==============================] - 1s 58us/step - loss: 0.7824 - binary_accuracy: 0.8867 - val_loss: 0.7281 - val_binary_accuracy: 0.9034\n",
            "Epoch 49/60\n",
            "23428/23428 [==============================] - 1s 59us/step - loss: 0.7844 - binary_accuracy: 0.8878 - val_loss: 0.7281 - val_binary_accuracy: 0.9032\n",
            "Epoch 50/60\n",
            "23428/23428 [==============================] - 1s 58us/step - loss: 0.7807 - binary_accuracy: 0.8882 - val_loss: 0.7331 - val_binary_accuracy: 0.9040\n",
            "Epoch 51/60\n",
            "23428/23428 [==============================] - 1s 58us/step - loss: 0.7780 - binary_accuracy: 0.8908 - val_loss: 0.7211 - val_binary_accuracy: 0.9056\n",
            "Epoch 52/60\n",
            "23428/23428 [==============================] - 1s 60us/step - loss: 0.7760 - binary_accuracy: 0.8900 - val_loss: 0.7192 - val_binary_accuracy: 0.9081\n",
            "Epoch 53/60\n",
            "23428/23428 [==============================] - 1s 59us/step - loss: 0.7658 - binary_accuracy: 0.8962 - val_loss: 0.6945 - val_binary_accuracy: 0.9191\n",
            "Epoch 54/60\n",
            "23428/23428 [==============================] - 1s 57us/step - loss: 0.7742 - binary_accuracy: 0.8901 - val_loss: 0.7284 - val_binary_accuracy: 0.9038\n",
            "Epoch 55/60\n",
            "23428/23428 [==============================] - 1s 59us/step - loss: 0.7747 - binary_accuracy: 0.8926 - val_loss: 0.7380 - val_binary_accuracy: 0.9005\n",
            "Epoch 56/60\n",
            "23428/23428 [==============================] - 1s 58us/step - loss: 0.7647 - binary_accuracy: 0.8970 - val_loss: 0.6863 - val_binary_accuracy: 0.9222\n",
            "Epoch 57/60\n",
            "23428/23428 [==============================] - 1s 59us/step - loss: 0.7628 - binary_accuracy: 0.8947 - val_loss: 0.7308 - val_binary_accuracy: 0.9016\n",
            "Epoch 58/60\n",
            "23428/23428 [==============================] - 1s 58us/step - loss: 0.7602 - binary_accuracy: 0.8977 - val_loss: 0.6967 - val_binary_accuracy: 0.9186\n",
            "Epoch 59/60\n",
            "23428/23428 [==============================] - 1s 58us/step - loss: 0.7692 - binary_accuracy: 0.8915 - val_loss: 0.6925 - val_binary_accuracy: 0.9204\n",
            "Epoch 60/60\n",
            "23428/23428 [==============================] - 1s 58us/step - loss: 0.7587 - binary_accuracy: 0.8978 - val_loss: 0.6940 - val_binary_accuracy: 0.9185\n",
            "23428/23428 [==============================] - 2s 99us/step\n",
            "8000/8000 [==============================] - 1s 98us/step\n",
            "(23428, 1422) (8000, 1422) (13470, 1422) (23428,) (8000,) (13470,)\n",
            "Train on 23428 samples, validate on 8000 samples\n",
            "Epoch 1/60\n",
            "23428/23428 [==============================] - 1s 59us/step - loss: 18.6669 - binary_accuracy: 0.5309 - val_loss: 15.7603 - val_binary_accuracy: 0.5199\n",
            "Epoch 2/60\n",
            "23428/23428 [==============================] - 1s 59us/step - loss: 13.8095 - binary_accuracy: 0.5511 - val_loss: 11.3280 - val_binary_accuracy: 0.5219\n",
            "Epoch 3/60\n",
            "23428/23428 [==============================] - 1s 58us/step - loss: 9.6958 - binary_accuracy: 0.5680 - val_loss: 7.6505 - val_binary_accuracy: 0.5204\n",
            "Epoch 4/60\n",
            "23428/23428 [==============================] - 1s 58us/step - loss: 6.3540 - binary_accuracy: 0.5804 - val_loss: 4.7647 - val_binary_accuracy: 0.5414\n",
            "Epoch 5/60\n",
            "23428/23428 [==============================] - 1s 58us/step - loss: 3.8158 - binary_accuracy: 0.6239 - val_loss: 2.6951 - val_binary_accuracy: 0.6532\n",
            "Epoch 6/60\n",
            "23428/23428 [==============================] - 1s 58us/step - loss: 2.0983 - binary_accuracy: 0.6738 - val_loss: 1.4514 - val_binary_accuracy: 0.8048\n",
            "Epoch 7/60\n",
            "23428/23428 [==============================] - 1s 59us/step - loss: 1.2123 - binary_accuracy: 0.7077 - val_loss: 1.0423 - val_binary_accuracy: 0.8510\n",
            "Epoch 8/60\n",
            "23428/23428 [==============================] - 1s 58us/step - loss: 1.0313 - binary_accuracy: 0.7281 - val_loss: 1.0151 - val_binary_accuracy: 0.8317\n",
            "Epoch 9/60\n",
            "23428/23428 [==============================] - 1s 59us/step - loss: 1.0151 - binary_accuracy: 0.7424 - val_loss: 1.0029 - val_binary_accuracy: 0.8586\n",
            "Epoch 10/60\n",
            "23428/23428 [==============================] - 1s 59us/step - loss: 1.0021 - binary_accuracy: 0.7606 - val_loss: 0.9896 - val_binary_accuracy: 0.8702\n",
            "Epoch 11/60\n",
            "23428/23428 [==============================] - 1s 58us/step - loss: 0.9926 - binary_accuracy: 0.7720 - val_loss: 0.9718 - val_binary_accuracy: 0.8733\n",
            "Epoch 12/60\n",
            "23428/23428 [==============================] - 1s 58us/step - loss: 0.9781 - binary_accuracy: 0.7887 - val_loss: 0.9564 - val_binary_accuracy: 0.8802\n",
            "Epoch 13/60\n",
            "23428/23428 [==============================] - 1s 57us/step - loss: 0.9684 - binary_accuracy: 0.7906 - val_loss: 0.9443 - val_binary_accuracy: 0.8830\n",
            "Epoch 14/60\n",
            "23428/23428 [==============================] - 1s 58us/step - loss: 0.9475 - binary_accuracy: 0.8107 - val_loss: 0.9290 - val_binary_accuracy: 0.8850\n",
            "Epoch 15/60\n",
            "23428/23428 [==============================] - 1s 59us/step - loss: 0.9446 - binary_accuracy: 0.8064 - val_loss: 0.9130 - val_binary_accuracy: 0.8852\n",
            "Epoch 16/60\n",
            "23428/23428 [==============================] - 1s 58us/step - loss: 0.9280 - binary_accuracy: 0.8200 - val_loss: 0.8920 - val_binary_accuracy: 0.8880\n",
            "Epoch 17/60\n",
            "23428/23428 [==============================] - 1s 58us/step - loss: 0.9248 - binary_accuracy: 0.8156 - val_loss: 0.9055 - val_binary_accuracy: 0.8821\n",
            "Epoch 18/60\n",
            "23428/23428 [==============================] - 1s 59us/step - loss: 0.9153 - binary_accuracy: 0.8234 - val_loss: 0.8822 - val_binary_accuracy: 0.8865\n",
            "Epoch 19/60\n",
            "23428/23428 [==============================] - 1s 58us/step - loss: 0.9041 - binary_accuracy: 0.8331 - val_loss: 0.8803 - val_binary_accuracy: 0.8849\n",
            "Epoch 20/60\n",
            "23428/23428 [==============================] - 1s 58us/step - loss: 0.8999 - binary_accuracy: 0.8310 - val_loss: 0.8573 - val_binary_accuracy: 0.8903\n",
            "Epoch 21/60\n",
            "23428/23428 [==============================] - 1s 58us/step - loss: 0.8885 - binary_accuracy: 0.8389 - val_loss: 0.8471 - val_binary_accuracy: 0.8910\n",
            "Epoch 22/60\n",
            "23428/23428 [==============================] - 1s 58us/step - loss: 0.8793 - binary_accuracy: 0.8408 - val_loss: 0.8484 - val_binary_accuracy: 0.8898\n",
            "Epoch 23/60\n",
            "23428/23428 [==============================] - 1s 58us/step - loss: 0.8929 - binary_accuracy: 0.8347 - val_loss: 0.8371 - val_binary_accuracy: 0.8904\n",
            "Epoch 24/60\n",
            "23428/23428 [==============================] - 1s 58us/step - loss: 0.8698 - binary_accuracy: 0.8446 - val_loss: 0.8300 - val_binary_accuracy: 0.8921\n",
            "Epoch 25/60\n",
            "23428/23428 [==============================] - 1s 58us/step - loss: 0.8752 - binary_accuracy: 0.8436 - val_loss: 0.8431 - val_binary_accuracy: 0.8890\n",
            "Epoch 26/60\n",
            "23428/23428 [==============================] - 1s 57us/step - loss: 0.8685 - binary_accuracy: 0.8487 - val_loss: 0.8332 - val_binary_accuracy: 0.8905\n",
            "Epoch 27/60\n",
            "23428/23428 [==============================] - 1s 58us/step - loss: 0.8647 - binary_accuracy: 0.8487 - val_loss: 0.8210 - val_binary_accuracy: 0.8926\n",
            "Epoch 28/60\n",
            "23428/23428 [==============================] - 1s 57us/step - loss: 0.8691 - binary_accuracy: 0.8460 - val_loss: 0.8083 - val_binary_accuracy: 0.8957\n",
            "Epoch 29/60\n",
            "23428/23428 [==============================] - 1s 57us/step - loss: 0.8458 - binary_accuracy: 0.8594 - val_loss: 0.8013 - val_binary_accuracy: 0.8975\n",
            "Epoch 30/60\n",
            "23428/23428 [==============================] - 1s 59us/step - loss: 0.8633 - binary_accuracy: 0.8494 - val_loss: 0.8159 - val_binary_accuracy: 0.8930\n",
            "Epoch 31/60\n",
            "23428/23428 [==============================] - 1s 58us/step - loss: 0.8507 - binary_accuracy: 0.8559 - val_loss: 0.7949 - val_binary_accuracy: 0.8992\n",
            "Epoch 32/60\n",
            "23428/23428 [==============================] - 1s 58us/step - loss: 0.8398 - binary_accuracy: 0.8645 - val_loss: 0.7891 - val_binary_accuracy: 0.8992\n",
            "Epoch 33/60\n",
            "23428/23428 [==============================] - 1s 60us/step - loss: 0.8575 - binary_accuracy: 0.8536 - val_loss: 0.7949 - val_binary_accuracy: 0.8980\n",
            "Epoch 34/60\n",
            "23428/23428 [==============================] - 1s 60us/step - loss: 0.8452 - binary_accuracy: 0.8613 - val_loss: 0.7978 - val_binary_accuracy: 0.8980\n",
            "Epoch 35/60\n",
            "23428/23428 [==============================] - 1s 61us/step - loss: 0.8281 - binary_accuracy: 0.8685 - val_loss: 0.7896 - val_binary_accuracy: 0.9000\n",
            "Epoch 36/60\n",
            "23428/23428 [==============================] - 1s 61us/step - loss: 0.8459 - binary_accuracy: 0.8607 - val_loss: 0.7945 - val_binary_accuracy: 0.8996\n",
            "Epoch 37/60\n",
            "23428/23428 [==============================] - 1s 59us/step - loss: 0.8333 - binary_accuracy: 0.8646 - val_loss: 0.7815 - val_binary_accuracy: 0.9022\n",
            "Epoch 38/60\n",
            "23428/23428 [==============================] - 1s 60us/step - loss: 0.8328 - binary_accuracy: 0.8670 - val_loss: 0.7896 - val_binary_accuracy: 0.9003\n",
            "Epoch 39/60\n",
            "23428/23428 [==============================] - 1s 59us/step - loss: 0.8376 - binary_accuracy: 0.8640 - val_loss: 0.7750 - val_binary_accuracy: 0.9028\n",
            "Epoch 40/60\n",
            "23428/23428 [==============================] - 1s 59us/step - loss: 0.8237 - binary_accuracy: 0.8696 - val_loss: 0.7881 - val_binary_accuracy: 0.9005\n",
            "Epoch 41/60\n",
            "23428/23428 [==============================] - 1s 60us/step - loss: 0.8276 - binary_accuracy: 0.8675 - val_loss: 0.7852 - val_binary_accuracy: 0.9024\n",
            "Epoch 42/60\n",
            "23428/23428 [==============================] - 1s 57us/step - loss: 0.8257 - binary_accuracy: 0.8700 - val_loss: 0.7679 - val_binary_accuracy: 0.9071\n",
            "Epoch 43/60\n",
            "23428/23428 [==============================] - 1s 57us/step - loss: 0.8227 - binary_accuracy: 0.8699 - val_loss: 0.7747 - val_binary_accuracy: 0.9050\n",
            "Epoch 44/60\n",
            "23428/23428 [==============================] - 1s 58us/step - loss: 0.8254 - binary_accuracy: 0.8709 - val_loss: 0.7813 - val_binary_accuracy: 0.9035\n",
            "Epoch 45/60\n",
            "23428/23428 [==============================] - 1s 58us/step - loss: 0.8157 - binary_accuracy: 0.8740 - val_loss: 0.7460 - val_binary_accuracy: 0.9103\n",
            "Epoch 46/60\n",
            "23428/23428 [==============================] - 1s 57us/step - loss: 0.8135 - binary_accuracy: 0.8769 - val_loss: 0.7780 - val_binary_accuracy: 0.9046\n",
            "Epoch 47/60\n",
            "23428/23428 [==============================] - 1s 58us/step - loss: 0.8177 - binary_accuracy: 0.8724 - val_loss: 0.7523 - val_binary_accuracy: 0.9099\n",
            "Epoch 48/60\n",
            "23428/23428 [==============================] - 1s 57us/step - loss: 0.8182 - binary_accuracy: 0.8746 - val_loss: 0.7657 - val_binary_accuracy: 0.9069\n",
            "Epoch 49/60\n",
            "23428/23428 [==============================] - 1s 59us/step - loss: 0.8126 - binary_accuracy: 0.8801 - val_loss: 0.7540 - val_binary_accuracy: 0.9105\n",
            "Epoch 50/60\n",
            "23428/23428 [==============================] - 1s 58us/step - loss: 0.8057 - binary_accuracy: 0.8782 - val_loss: 0.7569 - val_binary_accuracy: 0.9110\n",
            "Epoch 51/60\n",
            "23428/23428 [==============================] - 1s 58us/step - loss: 0.8005 - binary_accuracy: 0.8828 - val_loss: 0.7569 - val_binary_accuracy: 0.9089\n",
            "Epoch 52/60\n",
            "23428/23428 [==============================] - 1s 58us/step - loss: 0.8089 - binary_accuracy: 0.8781 - val_loss: 0.7391 - val_binary_accuracy: 0.9150\n",
            "Epoch 53/60\n",
            "23428/23428 [==============================] - 1s 57us/step - loss: 0.8016 - binary_accuracy: 0.8792 - val_loss: 0.7713 - val_binary_accuracy: 0.9038\n",
            "Epoch 54/60\n",
            "23428/23428 [==============================] - 1s 58us/step - loss: 0.8092 - binary_accuracy: 0.8804 - val_loss: 0.7424 - val_binary_accuracy: 0.9143\n",
            "Epoch 55/60\n",
            "23428/23428 [==============================] - 1s 58us/step - loss: 0.7904 - binary_accuracy: 0.8892 - val_loss: 0.7376 - val_binary_accuracy: 0.9150\n",
            "Epoch 56/60\n",
            "23428/23428 [==============================] - 1s 58us/step - loss: 0.7917 - binary_accuracy: 0.8841 - val_loss: 0.7330 - val_binary_accuracy: 0.9160\n",
            "Epoch 57/60\n",
            "23428/23428 [==============================] - 1s 59us/step - loss: 0.7986 - binary_accuracy: 0.8827 - val_loss: 0.7241 - val_binary_accuracy: 0.9181\n",
            "Epoch 58/60\n",
            "23428/23428 [==============================] - 1s 57us/step - loss: 0.7873 - binary_accuracy: 0.8859 - val_loss: 0.7497 - val_binary_accuracy: 0.9109\n",
            "Epoch 59/60\n",
            "23428/23428 [==============================] - 1s 58us/step - loss: 0.7801 - binary_accuracy: 0.8909 - val_loss: 0.7393 - val_binary_accuracy: 0.9141\n",
            "Epoch 60/60\n",
            "23428/23428 [==============================] - 1s 58us/step - loss: 0.7906 - binary_accuracy: 0.8877 - val_loss: 0.7289 - val_binary_accuracy: 0.9168\n",
            "23428/23428 [==============================] - 2s 101us/step\n",
            "8000/8000 [==============================] - 1s 107us/step\n",
            "(23428, 1422) (8000, 1422) (13470, 1422) (23428,) (8000,) (13470,)\n",
            "Train on 23428 samples, validate on 8000 samples\n",
            "Epoch 1/60\n",
            "23428/23428 [==============================] - 1s 58us/step - loss: 18.7212 - binary_accuracy: 0.5300 - val_loss: 15.8472 - val_binary_accuracy: 0.5216\n",
            "Epoch 2/60\n",
            "23428/23428 [==============================] - 1s 59us/step - loss: 13.8993 - binary_accuracy: 0.5528 - val_loss: 11.4185 - val_binary_accuracy: 0.5205\n",
            "Epoch 3/60\n",
            "23428/23428 [==============================] - 1s 58us/step - loss: 9.7809 - binary_accuracy: 0.5662 - val_loss: 7.7288 - val_binary_accuracy: 0.5245\n",
            "Epoch 4/60\n",
            "23428/23428 [==============================] - 1s 57us/step - loss: 6.4247 - binary_accuracy: 0.5828 - val_loss: 4.8269 - val_binary_accuracy: 0.5282\n",
            "Epoch 5/60\n",
            "23428/23428 [==============================] - 1s 58us/step - loss: 3.8691 - binary_accuracy: 0.6134 - val_loss: 2.7381 - val_binary_accuracy: 0.6411\n",
            "Epoch 6/60\n",
            "23428/23428 [==============================] - 1s 58us/step - loss: 2.1325 - binary_accuracy: 0.6746 - val_loss: 1.4752 - val_binary_accuracy: 0.6867\n",
            "Epoch 7/60\n",
            "23428/23428 [==============================] - 1s 58us/step - loss: 1.2284 - binary_accuracy: 0.7154 - val_loss: 1.0493 - val_binary_accuracy: 0.5968\n",
            "Epoch 8/60\n",
            "23428/23428 [==============================] - 1s 58us/step - loss: 1.0327 - binary_accuracy: 0.7190 - val_loss: 1.0181 - val_binary_accuracy: 0.6485\n",
            "Epoch 9/60\n",
            "23428/23428 [==============================] - 1s 58us/step - loss: 1.0182 - binary_accuracy: 0.7356 - val_loss: 1.0065 - val_binary_accuracy: 0.6684\n",
            "Epoch 10/60\n",
            "23428/23428 [==============================] - 1s 57us/step - loss: 1.0030 - binary_accuracy: 0.7566 - val_loss: 0.9882 - val_binary_accuracy: 0.7329\n",
            "Epoch 11/60\n",
            "23428/23428 [==============================] - 1s 59us/step - loss: 0.9909 - binary_accuracy: 0.7728 - val_loss: 0.9756 - val_binary_accuracy: 0.7356\n",
            "Epoch 12/60\n",
            "23428/23428 [==============================] - 1s 58us/step - loss: 0.9792 - binary_accuracy: 0.7819 - val_loss: 0.9551 - val_binary_accuracy: 0.7866\n",
            "Epoch 13/60\n",
            "23428/23428 [==============================] - 1s 58us/step - loss: 0.9650 - binary_accuracy: 0.7981 - val_loss: 0.9426 - val_binary_accuracy: 0.7841\n",
            "Epoch 14/60\n",
            "23428/23428 [==============================] - 1s 58us/step - loss: 0.9561 - binary_accuracy: 0.7998 - val_loss: 0.9299 - val_binary_accuracy: 0.7886\n",
            "Epoch 15/60\n",
            "23428/23428 [==============================] - 1s 58us/step - loss: 0.9416 - binary_accuracy: 0.8050 - val_loss: 0.9075 - val_binary_accuracy: 0.8240\n",
            "Epoch 16/60\n",
            "23428/23428 [==============================] - 1s 59us/step - loss: 0.9309 - binary_accuracy: 0.8165 - val_loss: 0.9089 - val_binary_accuracy: 0.8014\n",
            "Epoch 17/60\n",
            "23428/23428 [==============================] - 1s 60us/step - loss: 0.9226 - binary_accuracy: 0.8157 - val_loss: 0.8823 - val_binary_accuracy: 0.8367\n",
            "Epoch 18/60\n",
            "23428/23428 [==============================] - 1s 60us/step - loss: 0.9139 - binary_accuracy: 0.8227 - val_loss: 0.8802 - val_binary_accuracy: 0.8255\n",
            "Epoch 19/60\n",
            "23428/23428 [==============================] - 1s 60us/step - loss: 0.9037 - binary_accuracy: 0.8295 - val_loss: 0.8669 - val_binary_accuracy: 0.8381\n",
            "Epoch 20/60\n",
            "23428/23428 [==============================] - 1s 58us/step - loss: 0.9076 - binary_accuracy: 0.8229 - val_loss: 0.8657 - val_binary_accuracy: 0.8356\n",
            "Epoch 21/60\n",
            "23428/23428 [==============================] - 1s 58us/step - loss: 0.8882 - binary_accuracy: 0.8373 - val_loss: 0.8464 - val_binary_accuracy: 0.8501\n",
            "Epoch 22/60\n",
            "23428/23428 [==============================] - 1s 60us/step - loss: 0.8843 - binary_accuracy: 0.8404 - val_loss: 0.8471 - val_binary_accuracy: 0.8468\n",
            "Epoch 23/60\n",
            "23428/23428 [==============================] - 1s 61us/step - loss: 0.8857 - binary_accuracy: 0.8355 - val_loss: 0.8367 - val_binary_accuracy: 0.8539\n",
            "Epoch 24/60\n",
            "23428/23428 [==============================] - 1s 59us/step - loss: 0.8752 - binary_accuracy: 0.8452 - val_loss: 0.8378 - val_binary_accuracy: 0.8504\n",
            "Epoch 25/60\n",
            "23428/23428 [==============================] - 1s 60us/step - loss: 0.8687 - binary_accuracy: 0.8459 - val_loss: 0.8321 - val_binary_accuracy: 0.8544\n",
            "Epoch 26/60\n",
            "23428/23428 [==============================] - 1s 60us/step - loss: 0.8761 - binary_accuracy: 0.8440 - val_loss: 0.8419 - val_binary_accuracy: 0.8471\n",
            "Epoch 27/60\n",
            "23428/23428 [==============================] - 1s 58us/step - loss: 0.8677 - binary_accuracy: 0.8489 - val_loss: 0.8191 - val_binary_accuracy: 0.8594\n",
            "Epoch 28/60\n",
            "23428/23428 [==============================] - 1s 57us/step - loss: 0.8536 - binary_accuracy: 0.8563 - val_loss: 0.8223 - val_binary_accuracy: 0.8570\n",
            "Epoch 29/60\n",
            "23428/23428 [==============================] - 1s 57us/step - loss: 0.8557 - binary_accuracy: 0.8546 - val_loss: 0.8079 - val_binary_accuracy: 0.8641\n",
            "Epoch 30/60\n",
            "23428/23428 [==============================] - 1s 59us/step - loss: 0.8585 - binary_accuracy: 0.8541 - val_loss: 0.8275 - val_binary_accuracy: 0.8545\n",
            "Epoch 31/60\n",
            "23428/23428 [==============================] - 1s 58us/step - loss: 0.8588 - binary_accuracy: 0.8530 - val_loss: 0.8002 - val_binary_accuracy: 0.8699\n",
            "Epoch 32/60\n",
            "23428/23428 [==============================] - 1s 57us/step - loss: 0.8491 - binary_accuracy: 0.8565 - val_loss: 0.8046 - val_binary_accuracy: 0.8676\n",
            "Epoch 33/60\n",
            "23428/23428 [==============================] - 1s 58us/step - loss: 0.8405 - binary_accuracy: 0.8604 - val_loss: 0.7973 - val_binary_accuracy: 0.8690\n",
            "Epoch 34/60\n",
            "23428/23428 [==============================] - 1s 59us/step - loss: 0.8511 - binary_accuracy: 0.8580 - val_loss: 0.8028 - val_binary_accuracy: 0.8709\n",
            "Epoch 35/60\n",
            "23428/23428 [==============================] - 1s 58us/step - loss: 0.8437 - binary_accuracy: 0.8599 - val_loss: 0.7803 - val_binary_accuracy: 0.8810\n",
            "Epoch 36/60\n",
            "23428/23428 [==============================] - 1s 58us/step - loss: 0.8314 - binary_accuracy: 0.8655 - val_loss: 0.7891 - val_binary_accuracy: 0.8756\n",
            "Epoch 37/60\n",
            "23428/23428 [==============================] - 1s 58us/step - loss: 0.8373 - binary_accuracy: 0.8634 - val_loss: 0.8188 - val_binary_accuracy: 0.8627\n",
            "Epoch 38/60\n",
            "23428/23428 [==============================] - 1s 57us/step - loss: 0.8400 - binary_accuracy: 0.8636 - val_loss: 0.7727 - val_binary_accuracy: 0.8824\n",
            "Epoch 39/60\n",
            "23428/23428 [==============================] - 1s 58us/step - loss: 0.8399 - binary_accuracy: 0.8647 - val_loss: 0.7778 - val_binary_accuracy: 0.8820\n",
            "Epoch 40/60\n",
            "23428/23428 [==============================] - 1s 58us/step - loss: 0.8304 - binary_accuracy: 0.8664 - val_loss: 0.7826 - val_binary_accuracy: 0.8816\n",
            "Epoch 41/60\n",
            "23428/23428 [==============================] - 1s 59us/step - loss: 0.8205 - binary_accuracy: 0.8725 - val_loss: 0.7690 - val_binary_accuracy: 0.8854\n",
            "Epoch 42/60\n",
            "23428/23428 [==============================] - 1s 57us/step - loss: 0.8290 - binary_accuracy: 0.8673 - val_loss: 0.7824 - val_binary_accuracy: 0.8804\n",
            "Epoch 43/60\n",
            "23428/23428 [==============================] - 1s 57us/step - loss: 0.8220 - binary_accuracy: 0.8710 - val_loss: 0.7818 - val_binary_accuracy: 0.8799\n",
            "Epoch 44/60\n",
            "23428/23428 [==============================] - 1s 57us/step - loss: 0.8265 - binary_accuracy: 0.8689 - val_loss: 0.7621 - val_binary_accuracy: 0.8871\n",
            "Epoch 45/60\n",
            "23428/23428 [==============================] - 1s 57us/step - loss: 0.8179 - binary_accuracy: 0.8726 - val_loss: 0.7708 - val_binary_accuracy: 0.8869\n",
            "Epoch 46/60\n",
            "23428/23428 [==============================] - 1s 58us/step - loss: 0.8041 - binary_accuracy: 0.8804 - val_loss: 0.7569 - val_binary_accuracy: 0.8885\n",
            "Epoch 47/60\n",
            "23428/23428 [==============================] - 1s 57us/step - loss: 0.8296 - binary_accuracy: 0.8696 - val_loss: 0.7716 - val_binary_accuracy: 0.8855\n",
            "Epoch 48/60\n",
            "23428/23428 [==============================] - 1s 57us/step - loss: 0.8142 - binary_accuracy: 0.8743 - val_loss: 0.7455 - val_binary_accuracy: 0.8970\n",
            "Epoch 49/60\n",
            "23428/23428 [==============================] - 1s 59us/step - loss: 0.8069 - binary_accuracy: 0.8783 - val_loss: 0.7667 - val_binary_accuracy: 0.8890\n",
            "Epoch 50/60\n",
            "23428/23428 [==============================] - 1s 57us/step - loss: 0.8114 - binary_accuracy: 0.8792 - val_loss: 0.7458 - val_binary_accuracy: 0.8949\n",
            "Epoch 51/60\n",
            "23428/23428 [==============================] - 1s 57us/step - loss: 0.7972 - binary_accuracy: 0.8823 - val_loss: 0.7352 - val_binary_accuracy: 0.9024\n",
            "Epoch 52/60\n",
            "23428/23428 [==============================] - 1s 58us/step - loss: 0.7991 - binary_accuracy: 0.8838 - val_loss: 0.7671 - val_binary_accuracy: 0.8870\n",
            "Epoch 53/60\n",
            "23428/23428 [==============================] - 1s 57us/step - loss: 0.8182 - binary_accuracy: 0.8760 - val_loss: 0.7248 - val_binary_accuracy: 0.9075\n",
            "Epoch 54/60\n",
            "23428/23428 [==============================] - 1s 57us/step - loss: 0.7903 - binary_accuracy: 0.8851 - val_loss: 0.7614 - val_binary_accuracy: 0.8909\n",
            "Epoch 55/60\n",
            "23428/23428 [==============================] - 1s 58us/step - loss: 0.8008 - binary_accuracy: 0.8818 - val_loss: 0.7542 - val_binary_accuracy: 0.8931\n",
            "Epoch 56/60\n",
            "23428/23428 [==============================] - 1s 58us/step - loss: 0.7965 - binary_accuracy: 0.8853 - val_loss: 0.7321 - val_binary_accuracy: 0.9015\n",
            "Epoch 57/60\n",
            "23428/23428 [==============================] - 1s 58us/step - loss: 0.7954 - binary_accuracy: 0.8843 - val_loss: 0.7390 - val_binary_accuracy: 0.9005\n",
            "Epoch 58/60\n",
            "23428/23428 [==============================] - 1s 58us/step - loss: 0.7834 - binary_accuracy: 0.8893 - val_loss: 0.7391 - val_binary_accuracy: 0.8981\n",
            "Epoch 59/60\n",
            "23428/23428 [==============================] - 1s 57us/step - loss: 0.7967 - binary_accuracy: 0.8845 - val_loss: 0.7298 - val_binary_accuracy: 0.9044\n",
            "Epoch 60/60\n",
            "23428/23428 [==============================] - 1s 59us/step - loss: 0.7856 - binary_accuracy: 0.8893 - val_loss: 0.7204 - val_binary_accuracy: 0.9075\n",
            "23428/23428 [==============================] - 2s 107us/step\n",
            "8000/8000 [==============================] - 1s 102us/step\n",
            "\n",
            "\n",
            "Ensayando modelo con estructura [64, 32, 10, 1] y optimizador <keras.optimizers.SGD object at 0x7f535fc21320>\n",
            "(23428, 1422) (8000, 1422) (13470, 1422) (23428,) (8000,) (13470,)\n",
            "Train on 23428 samples, validate on 8000 samples\n",
            "Epoch 1/60\n",
            "23428/23428 [==============================] - 1s 27us/step - loss: 3.5725 - binary_accuracy: 0.4989 - val_loss: 3.5503 - val_binary_accuracy: 0.5339\n",
            "Epoch 2/60\n",
            "23428/23428 [==============================] - 0s 20us/step - loss: 3.5286 - binary_accuracy: 0.5143 - val_loss: 3.4955 - val_binary_accuracy: 0.5281\n",
            "Epoch 3/60\n",
            "23428/23428 [==============================] - 0s 21us/step - loss: 3.4691 - binary_accuracy: 0.5230 - val_loss: 3.4313 - val_binary_accuracy: 0.5191\n",
            "Epoch 4/60\n",
            "23428/23428 [==============================] - 0s 20us/step - loss: 3.4025 - binary_accuracy: 0.5336 - val_loss: 3.3637 - val_binary_accuracy: 0.5191\n",
            "Epoch 5/60\n",
            "23428/23428 [==============================] - 0s 21us/step - loss: 3.3344 - binary_accuracy: 0.5349 - val_loss: 3.2951 - val_binary_accuracy: 0.5191\n",
            "Epoch 6/60\n",
            "23428/23428 [==============================] - 0s 20us/step - loss: 3.2655 - binary_accuracy: 0.5404 - val_loss: 3.2264 - val_binary_accuracy: 0.5191\n",
            "Epoch 7/60\n",
            "23428/23428 [==============================] - 0s 21us/step - loss: 3.1972 - binary_accuracy: 0.5425 - val_loss: 3.1580 - val_binary_accuracy: 0.5191\n",
            "Epoch 8/60\n",
            "23428/23428 [==============================] - 0s 21us/step - loss: 3.1291 - binary_accuracy: 0.5478 - val_loss: 3.0903 - val_binary_accuracy: 0.5191\n",
            "Epoch 9/60\n",
            "23428/23428 [==============================] - 0s 21us/step - loss: 3.0627 - binary_accuracy: 0.5507 - val_loss: 3.0234 - val_binary_accuracy: 0.5191\n",
            "Epoch 10/60\n",
            "23428/23428 [==============================] - 0s 21us/step - loss: 2.9961 - binary_accuracy: 0.5605 - val_loss: 2.9575 - val_binary_accuracy: 0.5191\n",
            "Epoch 11/60\n",
            "23428/23428 [==============================] - 0s 21us/step - loss: 2.9314 - binary_accuracy: 0.5578 - val_loss: 2.8925 - val_binary_accuracy: 0.5192\n",
            "Epoch 12/60\n",
            "23428/23428 [==============================] - 0s 20us/step - loss: 2.8667 - binary_accuracy: 0.5658 - val_loss: 2.8284 - val_binary_accuracy: 0.5194\n",
            "Epoch 13/60\n",
            "23428/23428 [==============================] - 0s 20us/step - loss: 2.8030 - binary_accuracy: 0.5684 - val_loss: 2.7650 - val_binary_accuracy: 0.5196\n",
            "Epoch 14/60\n",
            "23428/23428 [==============================] - 0s 21us/step - loss: 2.7412 - binary_accuracy: 0.5693 - val_loss: 2.7025 - val_binary_accuracy: 0.5199\n",
            "Epoch 15/60\n",
            "23428/23428 [==============================] - 0s 21us/step - loss: 2.6786 - binary_accuracy: 0.5761 - val_loss: 2.6408 - val_binary_accuracy: 0.5206\n",
            "Epoch 16/60\n",
            "23428/23428 [==============================] - 0s 20us/step - loss: 2.6177 - binary_accuracy: 0.5816 - val_loss: 2.5799 - val_binary_accuracy: 0.5226\n",
            "Epoch 17/60\n",
            "23428/23428 [==============================] - 0s 21us/step - loss: 2.5568 - binary_accuracy: 0.5896 - val_loss: 2.5197 - val_binary_accuracy: 0.5278\n",
            "Epoch 18/60\n",
            "23428/23428 [==============================] - 0s 20us/step - loss: 2.4988 - binary_accuracy: 0.5910 - val_loss: 2.4605 - val_binary_accuracy: 0.5326\n",
            "Epoch 19/60\n",
            "23428/23428 [==============================] - 0s 20us/step - loss: 2.4400 - binary_accuracy: 0.5979 - val_loss: 2.4021 - val_binary_accuracy: 0.5405\n",
            "Epoch 20/60\n",
            "23428/23428 [==============================] - 0s 20us/step - loss: 2.3826 - binary_accuracy: 0.6062 - val_loss: 2.3446 - val_binary_accuracy: 0.5573\n",
            "Epoch 21/60\n",
            "23428/23428 [==============================] - 0s 21us/step - loss: 2.3256 - binary_accuracy: 0.6122 - val_loss: 2.2878 - val_binary_accuracy: 0.5871\n",
            "Epoch 22/60\n",
            "23428/23428 [==============================] - 0s 21us/step - loss: 2.2688 - binary_accuracy: 0.6213 - val_loss: 2.2317 - val_binary_accuracy: 0.6264\n",
            "Epoch 23/60\n",
            "23428/23428 [==============================] - 0s 20us/step - loss: 2.2137 - binary_accuracy: 0.6235 - val_loss: 2.1764 - val_binary_accuracy: 0.6529\n",
            "Epoch 24/60\n",
            "23428/23428 [==============================] - 0s 21us/step - loss: 2.1595 - binary_accuracy: 0.6285 - val_loss: 2.1217 - val_binary_accuracy: 0.6848\n",
            "Epoch 25/60\n",
            "23428/23428 [==============================] - 0s 20us/step - loss: 2.1057 - binary_accuracy: 0.6380 - val_loss: 2.0677 - val_binary_accuracy: 0.7219\n",
            "Epoch 26/60\n",
            "23428/23428 [==============================] - 0s 20us/step - loss: 2.0522 - binary_accuracy: 0.6447 - val_loss: 2.0145 - val_binary_accuracy: 0.7318\n",
            "Epoch 27/60\n",
            "23428/23428 [==============================] - 1s 21us/step - loss: 2.0003 - binary_accuracy: 0.6501 - val_loss: 1.9618 - val_binary_accuracy: 0.7738\n",
            "Epoch 28/60\n",
            "23428/23428 [==============================] - 0s 20us/step - loss: 1.9492 - binary_accuracy: 0.6551 - val_loss: 1.9098 - val_binary_accuracy: 0.7853\n",
            "Epoch 29/60\n",
            "23428/23428 [==============================] - 0s 20us/step - loss: 1.8964 - binary_accuracy: 0.6723 - val_loss: 1.8582 - val_binary_accuracy: 0.8254\n",
            "Epoch 30/60\n",
            "23428/23428 [==============================] - 0s 21us/step - loss: 1.8469 - binary_accuracy: 0.6680 - val_loss: 1.8073 - val_binary_accuracy: 0.8326\n",
            "Epoch 31/60\n",
            "23428/23428 [==============================] - 0s 21us/step - loss: 1.7988 - binary_accuracy: 0.6700 - val_loss: 1.7567 - val_binary_accuracy: 0.8587\n",
            "Epoch 32/60\n",
            "23428/23428 [==============================] - 0s 20us/step - loss: 1.7484 - binary_accuracy: 0.6871 - val_loss: 1.7067 - val_binary_accuracy: 0.8783\n",
            "Epoch 33/60\n",
            "23428/23428 [==============================] - 0s 20us/step - loss: 1.7004 - binary_accuracy: 0.6948 - val_loss: 1.6570 - val_binary_accuracy: 0.8863\n",
            "Epoch 34/60\n",
            "23428/23428 [==============================] - 0s 20us/step - loss: 1.6515 - binary_accuracy: 0.7029 - val_loss: 1.6076 - val_binary_accuracy: 0.8940\n",
            "Epoch 35/60\n",
            "23428/23428 [==============================] - 0s 20us/step - loss: 1.6043 - binary_accuracy: 0.7077 - val_loss: 1.5586 - val_binary_accuracy: 0.8971\n",
            "Epoch 36/60\n",
            "23428/23428 [==============================] - 0s 20us/step - loss: 1.5580 - binary_accuracy: 0.7122 - val_loss: 1.5095 - val_binary_accuracy: 0.9051\n",
            "Epoch 37/60\n",
            "23428/23428 [==============================] - 0s 21us/step - loss: 1.5127 - binary_accuracy: 0.7211 - val_loss: 1.4608 - val_binary_accuracy: 0.9101\n",
            "Epoch 38/60\n",
            "23428/23428 [==============================] - 0s 21us/step - loss: 1.4678 - binary_accuracy: 0.7239 - val_loss: 1.4120 - val_binary_accuracy: 0.9136\n",
            "Epoch 39/60\n",
            "23428/23428 [==============================] - 0s 20us/step - loss: 1.4191 - binary_accuracy: 0.7407 - val_loss: 1.3628 - val_binary_accuracy: 0.9211\n",
            "Epoch 40/60\n",
            "23428/23428 [==============================] - 0s 20us/step - loss: 1.3752 - binary_accuracy: 0.7397 - val_loss: 1.3144 - val_binary_accuracy: 0.9214\n",
            "Epoch 41/60\n",
            "23428/23428 [==============================] - 0s 21us/step - loss: 1.3304 - binary_accuracy: 0.7531 - val_loss: 1.2659 - val_binary_accuracy: 0.9266\n",
            "Epoch 42/60\n",
            "23428/23428 [==============================] - 0s 21us/step - loss: 1.2869 - binary_accuracy: 0.7568 - val_loss: 1.2180 - val_binary_accuracy: 0.9289\n",
            "Epoch 43/60\n",
            "23428/23428 [==============================] - 0s 20us/step - loss: 1.2435 - binary_accuracy: 0.7642 - val_loss: 1.1702 - val_binary_accuracy: 0.9329\n",
            "Epoch 44/60\n",
            "23428/23428 [==============================] - 0s 21us/step - loss: 1.2016 - binary_accuracy: 0.7715 - val_loss: 1.1230 - val_binary_accuracy: 0.9341\n",
            "Epoch 45/60\n",
            "23428/23428 [==============================] - 0s 21us/step - loss: 1.1582 - binary_accuracy: 0.7752 - val_loss: 1.0758 - val_binary_accuracy: 0.9360\n",
            "Epoch 46/60\n",
            "23428/23428 [==============================] - 0s 20us/step - loss: 1.1179 - binary_accuracy: 0.7854 - val_loss: 1.0303 - val_binary_accuracy: 0.9366\n",
            "Epoch 47/60\n",
            "23428/23428 [==============================] - 0s 20us/step - loss: 1.0762 - binary_accuracy: 0.7924 - val_loss: 0.9850 - val_binary_accuracy: 0.9389\n",
            "Epoch 48/60\n",
            "23428/23428 [==============================] - 0s 20us/step - loss: 1.0353 - binary_accuracy: 0.8049 - val_loss: 0.9414 - val_binary_accuracy: 0.9394\n",
            "Epoch 49/60\n",
            "23428/23428 [==============================] - 1s 22us/step - loss: 0.9970 - binary_accuracy: 0.8085 - val_loss: 0.8976 - val_binary_accuracy: 0.9398\n",
            "Epoch 50/60\n",
            "23428/23428 [==============================] - 0s 20us/step - loss: 0.9625 - binary_accuracy: 0.8127 - val_loss: 0.8567 - val_binary_accuracy: 0.9408\n",
            "Epoch 51/60\n",
            "23428/23428 [==============================] - 0s 20us/step - loss: 0.9266 - binary_accuracy: 0.8165 - val_loss: 0.8173 - val_binary_accuracy: 0.9415\n",
            "Epoch 52/60\n",
            "23428/23428 [==============================] - 0s 21us/step - loss: 0.8868 - binary_accuracy: 0.8289 - val_loss: 0.7788 - val_binary_accuracy: 0.9427\n",
            "Epoch 53/60\n",
            "23428/23428 [==============================] - 0s 20us/step - loss: 0.8527 - binary_accuracy: 0.8317 - val_loss: 0.7412 - val_binary_accuracy: 0.9433\n",
            "Epoch 54/60\n",
            "23428/23428 [==============================] - 0s 20us/step - loss: 0.8212 - binary_accuracy: 0.8360 - val_loss: 0.7056 - val_binary_accuracy: 0.9440\n",
            "Epoch 55/60\n",
            "23428/23428 [==============================] - 0s 20us/step - loss: 0.7885 - binary_accuracy: 0.8432 - val_loss: 0.6714 - val_binary_accuracy: 0.9448\n",
            "Epoch 56/60\n",
            "23428/23428 [==============================] - 0s 21us/step - loss: 0.7580 - binary_accuracy: 0.8455 - val_loss: 0.6389 - val_binary_accuracy: 0.9456\n",
            "Epoch 57/60\n",
            "23428/23428 [==============================] - 0s 20us/step - loss: 0.7295 - binary_accuracy: 0.8472 - val_loss: 0.6082 - val_binary_accuracy: 0.9465\n",
            "Epoch 58/60\n",
            "23428/23428 [==============================] - 0s 21us/step - loss: 0.7010 - binary_accuracy: 0.8517 - val_loss: 0.5794 - val_binary_accuracy: 0.9464\n",
            "Epoch 59/60\n",
            "23428/23428 [==============================] - 0s 21us/step - loss: 0.6762 - binary_accuracy: 0.8577 - val_loss: 0.5527 - val_binary_accuracy: 0.9466\n",
            "Epoch 60/60\n",
            "23428/23428 [==============================] - 0s 20us/step - loss: 0.6541 - binary_accuracy: 0.8598 - val_loss: 0.5266 - val_binary_accuracy: 0.9471\n",
            "23428/23428 [==============================] - 1s 41us/step\n",
            "8000/8000 [==============================] - 0s 42us/step\n",
            "(23428, 1422) (8000, 1422) (13470, 1422) (23428,) (8000,) (13470,)\n",
            "Train on 23428 samples, validate on 8000 samples\n",
            "Epoch 1/60\n",
            "23428/23428 [==============================] - 0s 20us/step - loss: 3.5671 - binary_accuracy: 0.5000 - val_loss: 3.5401 - val_binary_accuracy: 0.5518\n",
            "Epoch 2/60\n",
            "23428/23428 [==============================] - 0s 20us/step - loss: 3.5160 - binary_accuracy: 0.5244 - val_loss: 3.4809 - val_binary_accuracy: 0.5229\n",
            "Epoch 3/60\n",
            "23428/23428 [==============================] - 0s 20us/step - loss: 3.4535 - binary_accuracy: 0.5292 - val_loss: 3.4148 - val_binary_accuracy: 0.5191\n",
            "Epoch 4/60\n",
            "23428/23428 [==============================] - 0s 21us/step - loss: 3.3854 - binary_accuracy: 0.5384 - val_loss: 3.3462 - val_binary_accuracy: 0.5191\n",
            "Epoch 5/60\n",
            "23428/23428 [==============================] - 0s 20us/step - loss: 3.3168 - binary_accuracy: 0.5405 - val_loss: 3.2770 - val_binary_accuracy: 0.5191\n",
            "Epoch 6/60\n",
            "23428/23428 [==============================] - 0s 21us/step - loss: 3.2478 - binary_accuracy: 0.5403 - val_loss: 3.2079 - val_binary_accuracy: 0.5191\n",
            "Epoch 7/60\n",
            "23428/23428 [==============================] - 0s 20us/step - loss: 3.1792 - binary_accuracy: 0.5483 - val_loss: 3.1392 - val_binary_accuracy: 0.5191\n",
            "Epoch 8/60\n",
            "23428/23428 [==============================] - 1s 22us/step - loss: 3.1110 - binary_accuracy: 0.5522 - val_loss: 3.0713 - val_binary_accuracy: 0.5191\n",
            "Epoch 9/60\n",
            "23428/23428 [==============================] - 0s 20us/step - loss: 3.0442 - binary_accuracy: 0.5587 - val_loss: 3.0042 - val_binary_accuracy: 0.5192\n",
            "Epoch 10/60\n",
            "23428/23428 [==============================] - 0s 21us/step - loss: 2.9777 - binary_accuracy: 0.5646 - val_loss: 2.9378 - val_binary_accuracy: 0.5196\n",
            "Epoch 11/60\n",
            "23428/23428 [==============================] - 0s 21us/step - loss: 2.9120 - binary_accuracy: 0.5723 - val_loss: 2.8725 - val_binary_accuracy: 0.5206\n",
            "Epoch 12/60\n",
            "23428/23428 [==============================] - 0s 20us/step - loss: 2.8470 - binary_accuracy: 0.5776 - val_loss: 2.8080 - val_binary_accuracy: 0.5210\n",
            "Epoch 13/60\n",
            "23428/23428 [==============================] - 0s 20us/step - loss: 2.7834 - binary_accuracy: 0.5811 - val_loss: 2.7443 - val_binary_accuracy: 0.5217\n",
            "Epoch 14/60\n",
            "23428/23428 [==============================] - 0s 21us/step - loss: 2.7206 - binary_accuracy: 0.5820 - val_loss: 2.6813 - val_binary_accuracy: 0.5261\n",
            "Epoch 15/60\n",
            "23428/23428 [==============================] - 0s 21us/step - loss: 2.6588 - binary_accuracy: 0.5861 - val_loss: 2.6192 - val_binary_accuracy: 0.5282\n",
            "Epoch 16/60\n",
            "23428/23428 [==============================] - 0s 21us/step - loss: 2.5968 - binary_accuracy: 0.5965 - val_loss: 2.5578 - val_binary_accuracy: 0.5433\n",
            "Epoch 17/60\n",
            "23428/23428 [==============================] - 0s 20us/step - loss: 2.5363 - binary_accuracy: 0.6053 - val_loss: 2.4972 - val_binary_accuracy: 0.5626\n",
            "Epoch 18/60\n",
            "23428/23428 [==============================] - 0s 21us/step - loss: 2.4758 - binary_accuracy: 0.6140 - val_loss: 2.4373 - val_binary_accuracy: 0.5997\n",
            "Epoch 19/60\n",
            "23428/23428 [==============================] - 0s 20us/step - loss: 2.4175 - binary_accuracy: 0.6139 - val_loss: 2.3784 - val_binary_accuracy: 0.6069\n",
            "Epoch 20/60\n",
            "23428/23428 [==============================] - 0s 21us/step - loss: 2.3595 - binary_accuracy: 0.6222 - val_loss: 2.3202 - val_binary_accuracy: 0.6273\n",
            "Epoch 21/60\n",
            "23428/23428 [==============================] - 0s 20us/step - loss: 2.3015 - binary_accuracy: 0.6299 - val_loss: 2.2624 - val_binary_accuracy: 0.6869\n",
            "Epoch 22/60\n",
            "23428/23428 [==============================] - 0s 20us/step - loss: 2.2457 - binary_accuracy: 0.6345 - val_loss: 2.2053 - val_binary_accuracy: 0.7276\n",
            "Epoch 23/60\n",
            "23428/23428 [==============================] - 0s 20us/step - loss: 2.1883 - binary_accuracy: 0.6495 - val_loss: 2.1489 - val_binary_accuracy: 0.7410\n",
            "Epoch 24/60\n",
            "23428/23428 [==============================] - 0s 20us/step - loss: 2.1344 - binary_accuracy: 0.6498 - val_loss: 2.0931 - val_binary_accuracy: 0.7680\n",
            "Epoch 25/60\n",
            "23428/23428 [==============================] - 0s 21us/step - loss: 2.0799 - binary_accuracy: 0.6563 - val_loss: 2.0379 - val_binary_accuracy: 0.7952\n",
            "Epoch 26/60\n",
            "23428/23428 [==============================] - 0s 20us/step - loss: 2.0263 - binary_accuracy: 0.6637 - val_loss: 1.9832 - val_binary_accuracy: 0.8285\n",
            "Epoch 27/60\n",
            "23428/23428 [==============================] - 0s 20us/step - loss: 1.9720 - binary_accuracy: 0.6746 - val_loss: 1.9289 - val_binary_accuracy: 0.8457\n",
            "Epoch 28/60\n",
            "23428/23428 [==============================] - 0s 20us/step - loss: 1.9186 - binary_accuracy: 0.6823 - val_loss: 1.8752 - val_binary_accuracy: 0.8577\n",
            "Epoch 29/60\n",
            "23428/23428 [==============================] - 0s 21us/step - loss: 1.8664 - binary_accuracy: 0.6899 - val_loss: 1.8217 - val_binary_accuracy: 0.8740\n",
            "Epoch 30/60\n",
            "23428/23428 [==============================] - 0s 21us/step - loss: 1.8155 - binary_accuracy: 0.6945 - val_loss: 1.7686 - val_binary_accuracy: 0.8826\n",
            "Epoch 31/60\n",
            "23428/23428 [==============================] - 0s 21us/step - loss: 1.7659 - binary_accuracy: 0.7007 - val_loss: 1.7157 - val_binary_accuracy: 0.8966\n",
            "Epoch 32/60\n",
            "23428/23428 [==============================] - 0s 20us/step - loss: 1.7121 - binary_accuracy: 0.7153 - val_loss: 1.6630 - val_binary_accuracy: 0.8989\n",
            "Epoch 33/60\n",
            "23428/23428 [==============================] - 0s 21us/step - loss: 1.6635 - binary_accuracy: 0.7194 - val_loss: 1.6103 - val_binary_accuracy: 0.9081\n",
            "Epoch 34/60\n",
            "23428/23428 [==============================] - 0s 20us/step - loss: 1.6122 - binary_accuracy: 0.7286 - val_loss: 1.5575 - val_binary_accuracy: 0.9166\n",
            "Epoch 35/60\n",
            "23428/23428 [==============================] - 0s 21us/step - loss: 1.5653 - binary_accuracy: 0.7308 - val_loss: 1.5049 - val_binary_accuracy: 0.9206\n",
            "Epoch 36/60\n",
            "23428/23428 [==============================] - 0s 20us/step - loss: 1.5146 - binary_accuracy: 0.7406 - val_loss: 1.4523 - val_binary_accuracy: 0.9251\n",
            "Epoch 37/60\n",
            "23428/23428 [==============================] - 0s 21us/step - loss: 1.4658 - binary_accuracy: 0.7491 - val_loss: 1.3998 - val_binary_accuracy: 0.9265\n",
            "Epoch 38/60\n",
            "23428/23428 [==============================] - 0s 20us/step - loss: 1.4191 - binary_accuracy: 0.7536 - val_loss: 1.3474 - val_binary_accuracy: 0.9287\n",
            "Epoch 39/60\n",
            "23428/23428 [==============================] - 0s 21us/step - loss: 1.3712 - binary_accuracy: 0.7637 - val_loss: 1.2956 - val_binary_accuracy: 0.9312\n",
            "Epoch 40/60\n",
            "23428/23428 [==============================] - 0s 20us/step - loss: 1.3213 - binary_accuracy: 0.7765 - val_loss: 1.2435 - val_binary_accuracy: 0.9344\n",
            "Epoch 41/60\n",
            "23428/23428 [==============================] - 0s 20us/step - loss: 1.2782 - binary_accuracy: 0.7774 - val_loss: 1.1931 - val_binary_accuracy: 0.9361\n",
            "Epoch 42/60\n",
            "23428/23428 [==============================] - 0s 21us/step - loss: 1.2321 - binary_accuracy: 0.7868 - val_loss: 1.1426 - val_binary_accuracy: 0.9370\n",
            "Epoch 43/60\n",
            "23428/23428 [==============================] - 0s 21us/step - loss: 1.1838 - binary_accuracy: 0.7990 - val_loss: 1.0924 - val_binary_accuracy: 0.9385\n",
            "Epoch 44/60\n",
            "23428/23428 [==============================] - 0s 21us/step - loss: 1.1445 - binary_accuracy: 0.8002 - val_loss: 1.0446 - val_binary_accuracy: 0.9404\n",
            "Epoch 45/60\n",
            "23428/23428 [==============================] - 0s 20us/step - loss: 1.0991 - binary_accuracy: 0.8084 - val_loss: 0.9975 - val_binary_accuracy: 0.9406\n",
            "Epoch 46/60\n",
            "23428/23428 [==============================] - 0s 20us/step - loss: 1.0590 - binary_accuracy: 0.8149 - val_loss: 0.9510 - val_binary_accuracy: 0.9408\n",
            "Epoch 47/60\n",
            "23428/23428 [==============================] - 0s 20us/step - loss: 1.0167 - binary_accuracy: 0.8237 - val_loss: 0.9073 - val_binary_accuracy: 0.9431\n",
            "Epoch 48/60\n",
            "23428/23428 [==============================] - 0s 20us/step - loss: 0.9789 - binary_accuracy: 0.8277 - val_loss: 0.8643 - val_binary_accuracy: 0.9433\n",
            "Epoch 49/60\n",
            "23428/23428 [==============================] - 0s 20us/step - loss: 0.9395 - binary_accuracy: 0.8334 - val_loss: 0.8231 - val_binary_accuracy: 0.9444\n",
            "Epoch 50/60\n",
            "23428/23428 [==============================] - 0s 21us/step - loss: 0.9062 - binary_accuracy: 0.8358 - val_loss: 0.7838 - val_binary_accuracy: 0.9450\n",
            "Epoch 51/60\n",
            "23428/23428 [==============================] - 0s 21us/step - loss: 0.8633 - binary_accuracy: 0.8449 - val_loss: 0.7462 - val_binary_accuracy: 0.9466\n",
            "Epoch 52/60\n",
            "23428/23428 [==============================] - 1s 23us/step - loss: 0.8327 - binary_accuracy: 0.8465 - val_loss: 0.7102 - val_binary_accuracy: 0.9467\n",
            "Epoch 53/60\n",
            "23428/23428 [==============================] - 0s 21us/step - loss: 0.7998 - binary_accuracy: 0.8547 - val_loss: 0.6756 - val_binary_accuracy: 0.9470\n",
            "Epoch 54/60\n",
            "23428/23428 [==============================] - 0s 21us/step - loss: 0.7702 - binary_accuracy: 0.8558 - val_loss: 0.6429 - val_binary_accuracy: 0.9476\n",
            "Epoch 55/60\n",
            "23428/23428 [==============================] - 0s 20us/step - loss: 0.7378 - binary_accuracy: 0.8582 - val_loss: 0.6119 - val_binary_accuracy: 0.9475\n",
            "Epoch 56/60\n",
            "23428/23428 [==============================] - 0s 21us/step - loss: 0.7136 - binary_accuracy: 0.8611 - val_loss: 0.5826 - val_binary_accuracy: 0.9476\n",
            "Epoch 57/60\n",
            "23428/23428 [==============================] - 0s 20us/step - loss: 0.6810 - binary_accuracy: 0.8658 - val_loss: 0.5543 - val_binary_accuracy: 0.9484\n",
            "Epoch 58/60\n",
            "23428/23428 [==============================] - 1s 23us/step - loss: 0.6551 - binary_accuracy: 0.8712 - val_loss: 0.5276 - val_binary_accuracy: 0.9490\n",
            "Epoch 59/60\n",
            "23428/23428 [==============================] - 0s 20us/step - loss: 0.6281 - binary_accuracy: 0.8743 - val_loss: 0.5027 - val_binary_accuracy: 0.9494\n",
            "Epoch 60/60\n",
            "23428/23428 [==============================] - 1s 22us/step - loss: 0.6060 - binary_accuracy: 0.8763 - val_loss: 0.4803 - val_binary_accuracy: 0.9500\n",
            "23428/23428 [==============================] - 1s 41us/step\n",
            "8000/8000 [==============================] - 0s 40us/step\n",
            "(23428, 1422) (8000, 1422) (13470, 1422) (23428,) (8000,) (13470,)\n",
            "Train on 23428 samples, validate on 8000 samples\n",
            "Epoch 1/60\n",
            "23428/23428 [==============================] - 0s 20us/step - loss: 3.5674 - binary_accuracy: 0.5059 - val_loss: 3.5406 - val_binary_accuracy: 0.5404\n",
            "Epoch 2/60\n",
            "23428/23428 [==============================] - 0s 21us/step - loss: 3.5169 - binary_accuracy: 0.5213 - val_loss: 3.4817 - val_binary_accuracy: 0.5197\n",
            "Epoch 3/60\n",
            "23428/23428 [==============================] - 0s 20us/step - loss: 3.4541 - binary_accuracy: 0.5278 - val_loss: 3.4158 - val_binary_accuracy: 0.5191\n",
            "Epoch 4/60\n",
            "23428/23428 [==============================] - 0s 20us/step - loss: 3.3867 - binary_accuracy: 0.5370 - val_loss: 3.3474 - val_binary_accuracy: 0.5191\n",
            "Epoch 5/60\n",
            "23428/23428 [==============================] - 0s 20us/step - loss: 3.3179 - binary_accuracy: 0.5362 - val_loss: 3.2784 - val_binary_accuracy: 0.5191\n",
            "Epoch 6/60\n",
            "23428/23428 [==============================] - 0s 20us/step - loss: 3.2487 - binary_accuracy: 0.5401 - val_loss: 3.2093 - val_binary_accuracy: 0.5191\n",
            "Epoch 7/60\n",
            "23428/23428 [==============================] - 0s 20us/step - loss: 3.1805 - binary_accuracy: 0.5444 - val_loss: 3.1407 - val_binary_accuracy: 0.5191\n",
            "Epoch 8/60\n",
            "23428/23428 [==============================] - 0s 21us/step - loss: 3.1122 - binary_accuracy: 0.5510 - val_loss: 3.0728 - val_binary_accuracy: 0.5191\n",
            "Epoch 9/60\n",
            "23428/23428 [==============================] - 0s 20us/step - loss: 3.0454 - binary_accuracy: 0.5502 - val_loss: 3.0059 - val_binary_accuracy: 0.5191\n",
            "Epoch 10/60\n",
            "23428/23428 [==============================] - 0s 20us/step - loss: 2.9789 - binary_accuracy: 0.5573 - val_loss: 2.9397 - val_binary_accuracy: 0.5192\n",
            "Epoch 11/60\n",
            "23428/23428 [==============================] - 0s 20us/step - loss: 2.9131 - binary_accuracy: 0.5670 - val_loss: 2.8745 - val_binary_accuracy: 0.5192\n",
            "Epoch 12/60\n",
            "23428/23428 [==============================] - 0s 21us/step - loss: 2.8490 - binary_accuracy: 0.5671 - val_loss: 2.8101 - val_binary_accuracy: 0.5199\n",
            "Epoch 13/60\n",
            "23428/23428 [==============================] - 0s 20us/step - loss: 2.7850 - binary_accuracy: 0.5726 - val_loss: 2.7465 - val_binary_accuracy: 0.5205\n",
            "Epoch 14/60\n",
            "23428/23428 [==============================] - 0s 20us/step - loss: 2.7231 - binary_accuracy: 0.5758 - val_loss: 2.6836 - val_binary_accuracy: 0.5228\n",
            "Epoch 15/60\n",
            "23428/23428 [==============================] - 0s 20us/step - loss: 2.6600 - binary_accuracy: 0.5877 - val_loss: 2.6215 - val_binary_accuracy: 0.5300\n",
            "Epoch 16/60\n",
            "23428/23428 [==============================] - 0s 21us/step - loss: 2.5983 - binary_accuracy: 0.5965 - val_loss: 2.5601 - val_binary_accuracy: 0.5411\n",
            "Epoch 17/60\n",
            "23428/23428 [==============================] - 0s 21us/step - loss: 2.5384 - binary_accuracy: 0.5991 - val_loss: 2.4996 - val_binary_accuracy: 0.5483\n",
            "Epoch 18/60\n",
            "23428/23428 [==============================] - 0s 20us/step - loss: 2.4787 - binary_accuracy: 0.6019 - val_loss: 2.4400 - val_binary_accuracy: 0.5531\n",
            "Epoch 19/60\n",
            "23428/23428 [==============================] - 0s 20us/step - loss: 2.4201 - binary_accuracy: 0.6081 - val_loss: 2.3810 - val_binary_accuracy: 0.5893\n",
            "Epoch 20/60\n",
            "23428/23428 [==============================] - 0s 20us/step - loss: 2.3614 - binary_accuracy: 0.6192 - val_loss: 2.3227 - val_binary_accuracy: 0.6360\n",
            "Epoch 21/60\n",
            "23428/23428 [==============================] - 0s 20us/step - loss: 2.3046 - binary_accuracy: 0.6251 - val_loss: 2.2652 - val_binary_accuracy: 0.6793\n",
            "Epoch 22/60\n",
            "23428/23428 [==============================] - 0s 20us/step - loss: 2.2479 - binary_accuracy: 0.6324 - val_loss: 2.2085 - val_binary_accuracy: 0.7024\n",
            "Epoch 23/60\n",
            "23428/23428 [==============================] - 0s 21us/step - loss: 2.1914 - binary_accuracy: 0.6399 - val_loss: 2.1524 - val_binary_accuracy: 0.7344\n",
            "Epoch 24/60\n",
            "23428/23428 [==============================] - 0s 20us/step - loss: 2.1370 - binary_accuracy: 0.6456 - val_loss: 2.0971 - val_binary_accuracy: 0.7380\n",
            "Epoch 25/60\n",
            "23428/23428 [==============================] - 0s 21us/step - loss: 2.0823 - binary_accuracy: 0.6540 - val_loss: 2.0421 - val_binary_accuracy: 0.7828\n",
            "Epoch 26/60\n",
            "23428/23428 [==============================] - 0s 20us/step - loss: 2.0295 - binary_accuracy: 0.6610 - val_loss: 1.9879 - val_binary_accuracy: 0.7974\n",
            "Epoch 27/60\n",
            "23428/23428 [==============================] - 0s 20us/step - loss: 1.9759 - binary_accuracy: 0.6653 - val_loss: 1.9342 - val_binary_accuracy: 0.8223\n",
            "Epoch 28/60\n",
            "23428/23428 [==============================] - 0s 20us/step - loss: 1.9235 - binary_accuracy: 0.6728 - val_loss: 1.8807 - val_binary_accuracy: 0.8558\n",
            "Epoch 29/60\n",
            "23428/23428 [==============================] - 0s 20us/step - loss: 1.8707 - binary_accuracy: 0.6873 - val_loss: 1.8280 - val_binary_accuracy: 0.8564\n",
            "Epoch 30/60\n",
            "23428/23428 [==============================] - 0s 21us/step - loss: 1.8198 - binary_accuracy: 0.6937 - val_loss: 1.7752 - val_binary_accuracy: 0.8755\n",
            "Epoch 31/60\n",
            "23428/23428 [==============================] - 0s 20us/step - loss: 1.7694 - binary_accuracy: 0.6952 - val_loss: 1.7230 - val_binary_accuracy: 0.8845\n",
            "Epoch 32/60\n",
            "23428/23428 [==============================] - 0s 20us/step - loss: 1.7203 - binary_accuracy: 0.6995 - val_loss: 1.6708 - val_binary_accuracy: 0.8920\n",
            "Epoch 33/60\n",
            "23428/23428 [==============================] - 0s 20us/step - loss: 1.6693 - binary_accuracy: 0.7138 - val_loss: 1.6188 - val_binary_accuracy: 0.9025\n",
            "Epoch 34/60\n",
            "23428/23428 [==============================] - 0s 20us/step - loss: 1.6206 - binary_accuracy: 0.7182 - val_loss: 1.5670 - val_binary_accuracy: 0.9129\n",
            "Epoch 35/60\n",
            "23428/23428 [==============================] - 0s 20us/step - loss: 1.5731 - binary_accuracy: 0.7232 - val_loss: 1.5155 - val_binary_accuracy: 0.9149\n",
            "Epoch 36/60\n",
            "23428/23428 [==============================] - 1s 22us/step - loss: 1.5238 - binary_accuracy: 0.7358 - val_loss: 1.4638 - val_binary_accuracy: 0.9199\n",
            "Epoch 37/60\n",
            "23428/23428 [==============================] - 0s 21us/step - loss: 1.4763 - binary_accuracy: 0.7437 - val_loss: 1.4123 - val_binary_accuracy: 0.9262\n",
            "Epoch 38/60\n",
            "23428/23428 [==============================] - 0s 21us/step - loss: 1.4278 - binary_accuracy: 0.7509 - val_loss: 1.3608 - val_binary_accuracy: 0.9276\n",
            "Epoch 39/60\n",
            "23428/23428 [==============================] - 0s 20us/step - loss: 1.3821 - binary_accuracy: 0.7582 - val_loss: 1.3094 - val_binary_accuracy: 0.9295\n",
            "Epoch 40/60\n",
            "23428/23428 [==============================] - 0s 20us/step - loss: 1.3352 - binary_accuracy: 0.7633 - val_loss: 1.2586 - val_binary_accuracy: 0.9321\n",
            "Epoch 41/60\n",
            "23428/23428 [==============================] - 0s 21us/step - loss: 1.2855 - binary_accuracy: 0.7760 - val_loss: 1.2072 - val_binary_accuracy: 0.9346\n",
            "Epoch 42/60\n",
            "23428/23428 [==============================] - 0s 21us/step - loss: 1.2425 - binary_accuracy: 0.7788 - val_loss: 1.1579 - val_binary_accuracy: 0.9364\n",
            "Epoch 43/60\n",
            "23428/23428 [==============================] - 0s 20us/step - loss: 1.1978 - binary_accuracy: 0.7899 - val_loss: 1.1087 - val_binary_accuracy: 0.9376\n",
            "Epoch 44/60\n",
            "23428/23428 [==============================] - 1s 21us/step - loss: 1.1535 - binary_accuracy: 0.7982 - val_loss: 1.0595 - val_binary_accuracy: 0.9390\n",
            "Epoch 45/60\n",
            "23428/23428 [==============================] - 0s 20us/step - loss: 1.1105 - binary_accuracy: 0.8020 - val_loss: 1.0126 - val_binary_accuracy: 0.9394\n",
            "Epoch 46/60\n",
            "23428/23428 [==============================] - 0s 20us/step - loss: 1.0691 - binary_accuracy: 0.8094 - val_loss: 0.9663 - val_binary_accuracy: 0.9413\n",
            "Epoch 47/60\n",
            "23428/23428 [==============================] - 0s 20us/step - loss: 1.0312 - binary_accuracy: 0.8104 - val_loss: 0.9216 - val_binary_accuracy: 0.9409\n",
            "Epoch 48/60\n",
            "23428/23428 [==============================] - 0s 21us/step - loss: 0.9860 - binary_accuracy: 0.8243 - val_loss: 0.8779 - val_binary_accuracy: 0.9415\n",
            "Epoch 49/60\n",
            "23428/23428 [==============================] - 0s 20us/step - loss: 0.9511 - binary_accuracy: 0.8259 - val_loss: 0.8368 - val_binary_accuracy: 0.9440\n",
            "Epoch 50/60\n",
            "23428/23428 [==============================] - 0s 20us/step - loss: 0.9125 - binary_accuracy: 0.8323 - val_loss: 0.7963 - val_binary_accuracy: 0.9440\n",
            "Epoch 51/60\n",
            "23428/23428 [==============================] - 0s 21us/step - loss: 0.8807 - binary_accuracy: 0.8366 - val_loss: 0.7583 - val_binary_accuracy: 0.9452\n",
            "Epoch 52/60\n",
            "23428/23428 [==============================] - 0s 20us/step - loss: 0.8413 - binary_accuracy: 0.8431 - val_loss: 0.7218 - val_binary_accuracy: 0.9459\n",
            "Epoch 53/60\n",
            "23428/23428 [==============================] - 0s 20us/step - loss: 0.8089 - binary_accuracy: 0.8469 - val_loss: 0.6865 - val_binary_accuracy: 0.9474\n",
            "Epoch 54/60\n",
            "23428/23428 [==============================] - 0s 20us/step - loss: 0.7789 - binary_accuracy: 0.8508 - val_loss: 0.6532 - val_binary_accuracy: 0.9477\n",
            "Epoch 55/60\n",
            "23428/23428 [==============================] - 0s 21us/step - loss: 0.7456 - binary_accuracy: 0.8565 - val_loss: 0.6215 - val_binary_accuracy: 0.9479\n",
            "Epoch 56/60\n",
            "23428/23428 [==============================] - 0s 20us/step - loss: 0.7154 - binary_accuracy: 0.8611 - val_loss: 0.5902 - val_binary_accuracy: 0.9474\n",
            "Epoch 57/60\n",
            "23428/23428 [==============================] - 0s 20us/step - loss: 0.6883 - binary_accuracy: 0.8656 - val_loss: 0.5622 - val_binary_accuracy: 0.9484\n",
            "Epoch 58/60\n",
            "23428/23428 [==============================] - 0s 20us/step - loss: 0.6638 - binary_accuracy: 0.8636 - val_loss: 0.5348 - val_binary_accuracy: 0.9486\n",
            "Epoch 59/60\n",
            "23428/23428 [==============================] - 1s 21us/step - loss: 0.6356 - binary_accuracy: 0.8700 - val_loss: 0.5094 - val_binary_accuracy: 0.9489\n",
            "Epoch 60/60\n",
            "23428/23428 [==============================] - 0s 20us/step - loss: 0.6133 - binary_accuracy: 0.8725 - val_loss: 0.4865 - val_binary_accuracy: 0.9491\n",
            "23428/23428 [==============================] - 1s 42us/step\n",
            "8000/8000 [==============================] - 0s 40us/step\n",
            "\n",
            "\n",
            "Ensayando modelo con estructura [64, 32, 10, 1] y optimizador <keras.optimizers.Adam object at 0x7f535fc21be0>\n",
            "(23428, 1422) (8000, 1422) (13470, 1422) (23428,) (8000,) (13470,)\n",
            "Train on 23428 samples, validate on 8000 samples\n",
            "Epoch 1/60\n",
            "23428/23428 [==============================] - 1s 29us/step - loss: 2.6143 - binary_accuracy: 0.5161 - val_loss: 1.4370 - val_binary_accuracy: 0.5191\n",
            "Epoch 2/60\n",
            "23428/23428 [==============================] - 0s 20us/step - loss: 1.2553 - binary_accuracy: 0.5738 - val_loss: 1.2051 - val_binary_accuracy: 0.7782\n",
            "Epoch 3/60\n",
            "23428/23428 [==============================] - 0s 20us/step - loss: 1.0364 - binary_accuracy: 0.7124 - val_loss: 0.7848 - val_binary_accuracy: 0.9160\n",
            "Epoch 4/60\n",
            "23428/23428 [==============================] - 0s 20us/step - loss: 0.7686 - binary_accuracy: 0.8056 - val_loss: 0.5695 - val_binary_accuracy: 0.9541\n",
            "Epoch 5/60\n",
            "23428/23428 [==============================] - 0s 21us/step - loss: 0.5957 - binary_accuracy: 0.8596 - val_loss: 0.4170 - val_binary_accuracy: 0.9525\n",
            "Epoch 6/60\n",
            "23428/23428 [==============================] - 0s 20us/step - loss: 0.4986 - binary_accuracy: 0.8864 - val_loss: 0.3428 - val_binary_accuracy: 0.9541\n",
            "Epoch 7/60\n",
            "23428/23428 [==============================] - 0s 21us/step - loss: 0.4372 - binary_accuracy: 0.9057 - val_loss: 0.3129 - val_binary_accuracy: 0.9524\n",
            "Epoch 8/60\n",
            "23428/23428 [==============================] - 0s 21us/step - loss: 0.4124 - binary_accuracy: 0.9083 - val_loss: 0.2958 - val_binary_accuracy: 0.9595\n",
            "Epoch 9/60\n",
            "23428/23428 [==============================] - 0s 20us/step - loss: 0.3889 - binary_accuracy: 0.9166 - val_loss: 0.2864 - val_binary_accuracy: 0.9615\n",
            "Epoch 10/60\n",
            "23428/23428 [==============================] - 0s 20us/step - loss: 0.3823 - binary_accuracy: 0.9202 - val_loss: 0.2762 - val_binary_accuracy: 0.9640\n",
            "Epoch 11/60\n",
            "23428/23428 [==============================] - 0s 21us/step - loss: 0.3685 - binary_accuracy: 0.9235 - val_loss: 0.2671 - val_binary_accuracy: 0.9657\n",
            "Epoch 12/60\n",
            "23428/23428 [==============================] - 0s 21us/step - loss: 0.3557 - binary_accuracy: 0.9244 - val_loss: 0.2613 - val_binary_accuracy: 0.9660\n",
            "Epoch 13/60\n",
            "23428/23428 [==============================] - 0s 21us/step - loss: 0.3470 - binary_accuracy: 0.9273 - val_loss: 0.2558 - val_binary_accuracy: 0.9691\n",
            "Epoch 14/60\n",
            "23428/23428 [==============================] - 0s 20us/step - loss: 0.3435 - binary_accuracy: 0.9277 - val_loss: 0.2570 - val_binary_accuracy: 0.9686\n",
            "Epoch 15/60\n",
            "23428/23428 [==============================] - 0s 21us/step - loss: 0.3439 - binary_accuracy: 0.9303 - val_loss: 0.2538 - val_binary_accuracy: 0.9678\n",
            "Epoch 16/60\n",
            "23428/23428 [==============================] - 0s 20us/step - loss: 0.3310 - binary_accuracy: 0.9359 - val_loss: 0.2450 - val_binary_accuracy: 0.9696\n",
            "Epoch 17/60\n",
            "23428/23428 [==============================] - 0s 21us/step - loss: 0.3254 - binary_accuracy: 0.9414 - val_loss: 0.2537 - val_binary_accuracy: 0.9669\n",
            "Epoch 18/60\n",
            "23428/23428 [==============================] - 0s 21us/step - loss: 0.3264 - binary_accuracy: 0.9425 - val_loss: 0.2535 - val_binary_accuracy: 0.9674\n",
            "Epoch 19/60\n",
            "23428/23428 [==============================] - 0s 20us/step - loss: 0.3207 - binary_accuracy: 0.9471 - val_loss: 0.2428 - val_binary_accuracy: 0.9704\n",
            "Epoch 20/60\n",
            "23428/23428 [==============================] - 0s 21us/step - loss: 0.3109 - binary_accuracy: 0.9468 - val_loss: 0.2376 - val_binary_accuracy: 0.9715\n",
            "Epoch 21/60\n",
            "23428/23428 [==============================] - 0s 21us/step - loss: 0.3061 - binary_accuracy: 0.9495 - val_loss: 0.2398 - val_binary_accuracy: 0.9712\n",
            "Epoch 22/60\n",
            "23428/23428 [==============================] - 0s 21us/step - loss: 0.3082 - binary_accuracy: 0.9477 - val_loss: 0.2393 - val_binary_accuracy: 0.9719\n",
            "Epoch 23/60\n",
            "23428/23428 [==============================] - 0s 20us/step - loss: 0.3043 - binary_accuracy: 0.9513 - val_loss: 0.2402 - val_binary_accuracy: 0.9705\n",
            "Epoch 24/60\n",
            "23428/23428 [==============================] - 0s 21us/step - loss: 0.3014 - binary_accuracy: 0.9516 - val_loss: 0.2423 - val_binary_accuracy: 0.9699\n",
            "Epoch 25/60\n",
            "23428/23428 [==============================] - 0s 20us/step - loss: 0.3040 - binary_accuracy: 0.9491 - val_loss: 0.2497 - val_binary_accuracy: 0.9693\n",
            "Epoch 26/60\n",
            "23428/23428 [==============================] - 0s 21us/step - loss: 0.3021 - binary_accuracy: 0.9507 - val_loss: 0.2390 - val_binary_accuracy: 0.9719\n",
            "Epoch 27/60\n",
            "23428/23428 [==============================] - 0s 21us/step - loss: 0.2904 - binary_accuracy: 0.9542 - val_loss: 0.2336 - val_binary_accuracy: 0.9719\n",
            "Epoch 28/60\n",
            "23428/23428 [==============================] - 0s 21us/step - loss: 0.2888 - binary_accuracy: 0.9530 - val_loss: 0.2302 - val_binary_accuracy: 0.9732\n",
            "Epoch 29/60\n",
            "23428/23428 [==============================] - 0s 20us/step - loss: 0.2920 - binary_accuracy: 0.9524 - val_loss: 0.2349 - val_binary_accuracy: 0.9719\n",
            "Epoch 30/60\n",
            "23428/23428 [==============================] - 0s 21us/step - loss: 0.2900 - binary_accuracy: 0.9543 - val_loss: 0.2322 - val_binary_accuracy: 0.9719\n",
            "Epoch 31/60\n",
            "23428/23428 [==============================] - 0s 20us/step - loss: 0.2853 - binary_accuracy: 0.9542 - val_loss: 0.2321 - val_binary_accuracy: 0.9735\n",
            "Epoch 32/60\n",
            "23428/23428 [==============================] - 0s 21us/step - loss: 0.2896 - binary_accuracy: 0.9536 - val_loss: 0.2317 - val_binary_accuracy: 0.9732\n",
            "Epoch 33/60\n",
            "23428/23428 [==============================] - 0s 20us/step - loss: 0.2847 - binary_accuracy: 0.9554 - val_loss: 0.2343 - val_binary_accuracy: 0.9721\n",
            "Epoch 34/60\n",
            "23428/23428 [==============================] - 0s 21us/step - loss: 0.2880 - binary_accuracy: 0.9542 - val_loss: 0.2315 - val_binary_accuracy: 0.9739\n",
            "Epoch 35/60\n",
            "23428/23428 [==============================] - 0s 20us/step - loss: 0.2819 - binary_accuracy: 0.9553 - val_loss: 0.2301 - val_binary_accuracy: 0.9732\n",
            "Epoch 36/60\n",
            "23428/23428 [==============================] - 0s 21us/step - loss: 0.2852 - binary_accuracy: 0.9560 - val_loss: 0.2332 - val_binary_accuracy: 0.9721\n",
            "Epoch 37/60\n",
            "23428/23428 [==============================] - 0s 21us/step - loss: 0.2801 - binary_accuracy: 0.9565 - val_loss: 0.2309 - val_binary_accuracy: 0.9743\n",
            "Epoch 38/60\n",
            "23428/23428 [==============================] - 0s 21us/step - loss: 0.2812 - binary_accuracy: 0.9538 - val_loss: 0.2399 - val_binary_accuracy: 0.9705\n",
            "Epoch 39/60\n",
            "23428/23428 [==============================] - 0s 20us/step - loss: 0.2786 - binary_accuracy: 0.9595 - val_loss: 0.2325 - val_binary_accuracy: 0.9712\n",
            "Epoch 40/60\n",
            "23428/23428 [==============================] - 0s 21us/step - loss: 0.2753 - binary_accuracy: 0.9595 - val_loss: 0.2288 - val_binary_accuracy: 0.9726\n",
            "Epoch 41/60\n",
            "23428/23428 [==============================] - 0s 21us/step - loss: 0.2735 - binary_accuracy: 0.9574 - val_loss: 0.2326 - val_binary_accuracy: 0.9715\n",
            "Epoch 42/60\n",
            "23428/23428 [==============================] - 0s 21us/step - loss: 0.2749 - binary_accuracy: 0.9565 - val_loss: 0.2381 - val_binary_accuracy: 0.9695\n",
            "Epoch 43/60\n",
            "23428/23428 [==============================] - 0s 21us/step - loss: 0.2789 - binary_accuracy: 0.9556 - val_loss: 0.2340 - val_binary_accuracy: 0.9735\n",
            "Epoch 44/60\n",
            "23428/23428 [==============================] - 0s 20us/step - loss: 0.2734 - binary_accuracy: 0.9574 - val_loss: 0.2267 - val_binary_accuracy: 0.9732\n",
            "Epoch 45/60\n",
            "23428/23428 [==============================] - 0s 21us/step - loss: 0.2708 - binary_accuracy: 0.9579 - val_loss: 0.2239 - val_binary_accuracy: 0.9737\n",
            "Epoch 46/60\n",
            "23428/23428 [==============================] - 0s 20us/step - loss: 0.2661 - binary_accuracy: 0.9599 - val_loss: 0.2252 - val_binary_accuracy: 0.9736\n",
            "Epoch 47/60\n",
            "23428/23428 [==============================] - 0s 20us/step - loss: 0.2667 - binary_accuracy: 0.9610 - val_loss: 0.2245 - val_binary_accuracy: 0.9750\n",
            "Epoch 48/60\n",
            "23428/23428 [==============================] - 0s 21us/step - loss: 0.2662 - binary_accuracy: 0.9593 - val_loss: 0.2233 - val_binary_accuracy: 0.9745\n",
            "Epoch 49/60\n",
            "23428/23428 [==============================] - 0s 21us/step - loss: 0.2665 - binary_accuracy: 0.9589 - val_loss: 0.2238 - val_binary_accuracy: 0.9750\n",
            "Epoch 50/60\n",
            "23428/23428 [==============================] - 0s 20us/step - loss: 0.2601 - binary_accuracy: 0.9602 - val_loss: 0.2256 - val_binary_accuracy: 0.9731\n",
            "Epoch 51/60\n",
            "23428/23428 [==============================] - 0s 21us/step - loss: 0.2675 - binary_accuracy: 0.9580 - val_loss: 0.2202 - val_binary_accuracy: 0.9744\n",
            "Epoch 52/60\n",
            "23428/23428 [==============================] - 1s 21us/step - loss: 0.2688 - binary_accuracy: 0.9566 - val_loss: 0.2266 - val_binary_accuracy: 0.9746\n",
            "Epoch 53/60\n",
            "23428/23428 [==============================] - 0s 21us/step - loss: 0.2714 - binary_accuracy: 0.9586 - val_loss: 0.2293 - val_binary_accuracy: 0.9744\n",
            "Epoch 54/60\n",
            "23428/23428 [==============================] - 0s 21us/step - loss: 0.2699 - binary_accuracy: 0.9603 - val_loss: 0.2337 - val_binary_accuracy: 0.9722\n",
            "Epoch 55/60\n",
            "23428/23428 [==============================] - 1s 22us/step - loss: 0.2664 - binary_accuracy: 0.9596 - val_loss: 0.2233 - val_binary_accuracy: 0.9749\n",
            "Epoch 56/60\n",
            "23428/23428 [==============================] - 0s 21us/step - loss: 0.2623 - binary_accuracy: 0.9601 - val_loss: 0.2221 - val_binary_accuracy: 0.9751\n",
            "Epoch 57/60\n",
            "23428/23428 [==============================] - 0s 21us/step - loss: 0.2592 - binary_accuracy: 0.9617 - val_loss: 0.2217 - val_binary_accuracy: 0.9741\n",
            "Epoch 58/60\n",
            "23428/23428 [==============================] - 1s 22us/step - loss: 0.2599 - binary_accuracy: 0.9612 - val_loss: 0.2219 - val_binary_accuracy: 0.9740\n",
            "Epoch 59/60\n",
            "23428/23428 [==============================] - 0s 21us/step - loss: 0.2601 - binary_accuracy: 0.9606 - val_loss: 0.2219 - val_binary_accuracy: 0.9746\n",
            "Epoch 60/60\n",
            "23428/23428 [==============================] - 0s 20us/step - loss: 0.2614 - binary_accuracy: 0.9605 - val_loss: 0.2248 - val_binary_accuracy: 0.9735\n",
            "23428/23428 [==============================] - 1s 42us/step\n",
            "8000/8000 [==============================] - 0s 40us/step\n",
            "(23428, 1422) (8000, 1422) (13470, 1422) (23428,) (8000,) (13470,)\n",
            "Train on 23428 samples, validate on 8000 samples\n",
            "Epoch 1/60\n",
            "23428/23428 [==============================] - 0s 20us/step - loss: 3.5102 - binary_accuracy: 0.5001 - val_loss: 3.3190 - val_binary_accuracy: 0.5280\n",
            "Epoch 2/60\n",
            "23428/23428 [==============================] - 1s 21us/step - loss: 3.1142 - binary_accuracy: 0.5250 - val_loss: 2.8221 - val_binary_accuracy: 0.5192\n",
            "Epoch 3/60\n",
            "23428/23428 [==============================] - 0s 20us/step - loss: 2.6009 - binary_accuracy: 0.5376 - val_loss: 2.3090 - val_binary_accuracy: 0.5196\n",
            "Epoch 4/60\n",
            "23428/23428 [==============================] - 0s 20us/step - loss: 2.1134 - binary_accuracy: 0.5619 - val_loss: 1.8597 - val_binary_accuracy: 0.5275\n",
            "Epoch 5/60\n",
            "23428/23428 [==============================] - 0s 21us/step - loss: 1.7057 - binary_accuracy: 0.6095 - val_loss: 1.4984 - val_binary_accuracy: 0.7210\n",
            "Epoch 6/60\n",
            "23428/23428 [==============================] - 0s 21us/step - loss: 1.3933 - binary_accuracy: 0.6760 - val_loss: 1.2300 - val_binary_accuracy: 0.8574\n",
            "Epoch 7/60\n",
            "23428/23428 [==============================] - 0s 21us/step - loss: 1.1822 - binary_accuracy: 0.7303 - val_loss: 1.0312 - val_binary_accuracy: 0.9283\n",
            "Epoch 8/60\n",
            "23428/23428 [==============================] - 0s 21us/step - loss: 1.0018 - binary_accuracy: 0.7874 - val_loss: 0.8260 - val_binary_accuracy: 0.9455\n",
            "Epoch 9/60\n",
            "23428/23428 [==============================] - 0s 21us/step - loss: 0.8352 - binary_accuracy: 0.8329 - val_loss: 0.6542 - val_binary_accuracy: 0.9504\n",
            "Epoch 10/60\n",
            "23428/23428 [==============================] - 0s 21us/step - loss: 0.7010 - binary_accuracy: 0.8699 - val_loss: 0.5204 - val_binary_accuracy: 0.9541\n",
            "Epoch 11/60\n",
            "23428/23428 [==============================] - 0s 21us/step - loss: 0.5991 - binary_accuracy: 0.8853 - val_loss: 0.4320 - val_binary_accuracy: 0.9570\n",
            "Epoch 12/60\n",
            "23428/23428 [==============================] - 0s 21us/step - loss: 0.5260 - binary_accuracy: 0.8978 - val_loss: 0.3760 - val_binary_accuracy: 0.9582\n",
            "Epoch 13/60\n",
            "23428/23428 [==============================] - 0s 21us/step - loss: 0.4711 - binary_accuracy: 0.9069 - val_loss: 0.3353 - val_binary_accuracy: 0.9610\n",
            "Epoch 14/60\n",
            "23428/23428 [==============================] - 0s 20us/step - loss: 0.4323 - binary_accuracy: 0.9137 - val_loss: 0.3076 - val_binary_accuracy: 0.9632\n",
            "Epoch 15/60\n",
            "23428/23428 [==============================] - 0s 21us/step - loss: 0.4047 - binary_accuracy: 0.9192 - val_loss: 0.2884 - val_binary_accuracy: 0.9634\n",
            "Epoch 16/60\n",
            "23428/23428 [==============================] - 0s 21us/step - loss: 0.3830 - binary_accuracy: 0.9192 - val_loss: 0.2756 - val_binary_accuracy: 0.9601\n",
            "Epoch 17/60\n",
            "23428/23428 [==============================] - 0s 21us/step - loss: 0.3614 - binary_accuracy: 0.9249 - val_loss: 0.2589 - val_binary_accuracy: 0.9664\n",
            "Epoch 18/60\n",
            "23428/23428 [==============================] - 0s 20us/step - loss: 0.3457 - binary_accuracy: 0.9230 - val_loss: 0.2477 - val_binary_accuracy: 0.9653\n",
            "Epoch 19/60\n",
            "23428/23428 [==============================] - 0s 20us/step - loss: 0.3318 - binary_accuracy: 0.9260 - val_loss: 0.2440 - val_binary_accuracy: 0.9638\n",
            "Epoch 20/60\n",
            "23428/23428 [==============================] - 0s 21us/step - loss: 0.3215 - binary_accuracy: 0.9294 - val_loss: 0.2341 - val_binary_accuracy: 0.9665\n",
            "Epoch 21/60\n",
            "23428/23428 [==============================] - 0s 21us/step - loss: 0.3112 - binary_accuracy: 0.9280 - val_loss: 0.2258 - val_binary_accuracy: 0.9686\n",
            "Epoch 22/60\n",
            "23428/23428 [==============================] - 0s 20us/step - loss: 0.3028 - binary_accuracy: 0.9310 - val_loss: 0.2210 - val_binary_accuracy: 0.9688\n",
            "Epoch 23/60\n",
            "23428/23428 [==============================] - 1s 22us/step - loss: 0.2951 - binary_accuracy: 0.9308 - val_loss: 0.2186 - val_binary_accuracy: 0.9690\n",
            "Epoch 24/60\n",
            "23428/23428 [==============================] - 0s 21us/step - loss: 0.2900 - binary_accuracy: 0.9321 - val_loss: 0.2149 - val_binary_accuracy: 0.9694\n",
            "Epoch 25/60\n",
            "23428/23428 [==============================] - 0s 20us/step - loss: 0.2854 - binary_accuracy: 0.9441 - val_loss: 0.2117 - val_binary_accuracy: 0.9706\n",
            "Epoch 26/60\n",
            "23428/23428 [==============================] - 0s 21us/step - loss: 0.2844 - binary_accuracy: 0.9476 - val_loss: 0.2094 - val_binary_accuracy: 0.9704\n",
            "Epoch 27/60\n",
            "23428/23428 [==============================] - 0s 21us/step - loss: 0.2810 - binary_accuracy: 0.9500 - val_loss: 0.2090 - val_binary_accuracy: 0.9704\n",
            "Epoch 28/60\n",
            "23428/23428 [==============================] - 0s 21us/step - loss: 0.2741 - binary_accuracy: 0.9522 - val_loss: 0.2059 - val_binary_accuracy: 0.9716\n",
            "Epoch 29/60\n",
            "23428/23428 [==============================] - 0s 20us/step - loss: 0.2730 - binary_accuracy: 0.9503 - val_loss: 0.2045 - val_binary_accuracy: 0.9721\n",
            "Epoch 30/60\n",
            "23428/23428 [==============================] - 0s 20us/step - loss: 0.2735 - binary_accuracy: 0.9517 - val_loss: 0.2041 - val_binary_accuracy: 0.9712\n",
            "Epoch 31/60\n",
            "23428/23428 [==============================] - 0s 21us/step - loss: 0.2712 - binary_accuracy: 0.9516 - val_loss: 0.2054 - val_binary_accuracy: 0.9719\n",
            "Epoch 32/60\n",
            "23428/23428 [==============================] - 0s 21us/step - loss: 0.2673 - binary_accuracy: 0.9540 - val_loss: 0.2063 - val_binary_accuracy: 0.9706\n",
            "Epoch 33/60\n",
            "23428/23428 [==============================] - 0s 21us/step - loss: 0.2707 - binary_accuracy: 0.9530 - val_loss: 0.2071 - val_binary_accuracy: 0.9712\n",
            "Epoch 34/60\n",
            "23428/23428 [==============================] - 0s 21us/step - loss: 0.2702 - binary_accuracy: 0.9522 - val_loss: 0.2036 - val_binary_accuracy: 0.9721\n",
            "Epoch 35/60\n",
            "23428/23428 [==============================] - 0s 21us/step - loss: 0.2678 - binary_accuracy: 0.9546 - val_loss: 0.2034 - val_binary_accuracy: 0.9731\n",
            "Epoch 36/60\n",
            "23428/23428 [==============================] - 0s 21us/step - loss: 0.2600 - binary_accuracy: 0.9556 - val_loss: 0.2010 - val_binary_accuracy: 0.9728\n",
            "Epoch 37/60\n",
            "23428/23428 [==============================] - 0s 21us/step - loss: 0.2639 - binary_accuracy: 0.9525 - val_loss: 0.1996 - val_binary_accuracy: 0.9739\n",
            "Epoch 38/60\n",
            "23428/23428 [==============================] - 0s 21us/step - loss: 0.2628 - binary_accuracy: 0.9549 - val_loss: 0.2000 - val_binary_accuracy: 0.9735\n",
            "Epoch 39/60\n",
            "23428/23428 [==============================] - 0s 21us/step - loss: 0.2598 - binary_accuracy: 0.9538 - val_loss: 0.2054 - val_binary_accuracy: 0.9711\n",
            "Epoch 40/60\n",
            "23428/23428 [==============================] - 0s 21us/step - loss: 0.2652 - binary_accuracy: 0.9542 - val_loss: 0.2060 - val_binary_accuracy: 0.9712\n",
            "Epoch 41/60\n",
            "23428/23428 [==============================] - 0s 21us/step - loss: 0.2656 - binary_accuracy: 0.9533 - val_loss: 0.2088 - val_binary_accuracy: 0.9725\n",
            "Epoch 42/60\n",
            "23428/23428 [==============================] - 0s 20us/step - loss: 0.2630 - binary_accuracy: 0.9550 - val_loss: 0.2028 - val_binary_accuracy: 0.9737\n",
            "Epoch 43/60\n",
            "23428/23428 [==============================] - 0s 20us/step - loss: 0.2644 - binary_accuracy: 0.9548 - val_loss: 0.2040 - val_binary_accuracy: 0.9721\n",
            "Epoch 44/60\n",
            "23428/23428 [==============================] - 0s 21us/step - loss: 0.2581 - binary_accuracy: 0.9571 - val_loss: 0.2013 - val_binary_accuracy: 0.9732\n",
            "Epoch 45/60\n",
            "23428/23428 [==============================] - 1s 22us/step - loss: 0.2572 - binary_accuracy: 0.9576 - val_loss: 0.2004 - val_binary_accuracy: 0.9749\n",
            "Epoch 46/60\n",
            "23428/23428 [==============================] - 0s 20us/step - loss: 0.2512 - binary_accuracy: 0.9580 - val_loss: 0.1993 - val_binary_accuracy: 0.9728\n",
            "Epoch 47/60\n",
            "23428/23428 [==============================] - 0s 21us/step - loss: 0.2506 - binary_accuracy: 0.9576 - val_loss: 0.1995 - val_binary_accuracy: 0.9740\n",
            "Epoch 48/60\n",
            "23428/23428 [==============================] - 0s 21us/step - loss: 0.2494 - binary_accuracy: 0.9589 - val_loss: 0.2005 - val_binary_accuracy: 0.9735\n",
            "Epoch 49/60\n",
            "23428/23428 [==============================] - 0s 21us/step - loss: 0.2521 - binary_accuracy: 0.9587 - val_loss: 0.2011 - val_binary_accuracy: 0.9719\n",
            "Epoch 50/60\n",
            "23428/23428 [==============================] - 0s 21us/step - loss: 0.2535 - binary_accuracy: 0.9586 - val_loss: 0.2008 - val_binary_accuracy: 0.9728\n",
            "Epoch 51/60\n",
            "23428/23428 [==============================] - 0s 21us/step - loss: 0.2525 - binary_accuracy: 0.9586 - val_loss: 0.1990 - val_binary_accuracy: 0.9745\n",
            "Epoch 52/60\n",
            "23428/23428 [==============================] - 0s 20us/step - loss: 0.2520 - binary_accuracy: 0.9592 - val_loss: 0.2010 - val_binary_accuracy: 0.9734\n",
            "Epoch 53/60\n",
            "23428/23428 [==============================] - 0s 21us/step - loss: 0.2492 - binary_accuracy: 0.9583 - val_loss: 0.2001 - val_binary_accuracy: 0.9735\n",
            "Epoch 54/60\n",
            "23428/23428 [==============================] - 0s 21us/step - loss: 0.2478 - binary_accuracy: 0.9603 - val_loss: 0.2003 - val_binary_accuracy: 0.9739\n",
            "Epoch 55/60\n",
            "23428/23428 [==============================] - 0s 21us/step - loss: 0.2473 - binary_accuracy: 0.9588 - val_loss: 0.1966 - val_binary_accuracy: 0.9753\n",
            "Epoch 56/60\n",
            "23428/23428 [==============================] - 0s 21us/step - loss: 0.2467 - binary_accuracy: 0.9600 - val_loss: 0.2061 - val_binary_accuracy: 0.9721\n",
            "Epoch 57/60\n",
            "23428/23428 [==============================] - 0s 21us/step - loss: 0.2483 - binary_accuracy: 0.9601 - val_loss: 0.2028 - val_binary_accuracy: 0.9736\n",
            "Epoch 58/60\n",
            "23428/23428 [==============================] - 0s 20us/step - loss: 0.2499 - binary_accuracy: 0.9599 - val_loss: 0.2043 - val_binary_accuracy: 0.9731\n",
            "Epoch 59/60\n",
            "23428/23428 [==============================] - 0s 20us/step - loss: 0.2475 - binary_accuracy: 0.9599 - val_loss: 0.2045 - val_binary_accuracy: 0.9735\n",
            "Epoch 60/60\n",
            "23428/23428 [==============================] - 0s 21us/step - loss: 0.2510 - binary_accuracy: 0.9604 - val_loss: 0.2028 - val_binary_accuracy: 0.9741\n",
            "23428/23428 [==============================] - 1s 41us/step\n",
            "8000/8000 [==============================] - 0s 42us/step\n",
            "(23428, 1422) (8000, 1422) (13470, 1422) (23428,) (8000,) (13470,)\n",
            "Train on 23428 samples, validate on 8000 samples\n",
            "Epoch 1/60\n",
            "23428/23428 [==============================] - 0s 21us/step - loss: 3.5322 - binary_accuracy: 0.5018 - val_loss: 3.3892 - val_binary_accuracy: 0.5275\n",
            "Epoch 2/60\n",
            "23428/23428 [==============================] - 0s 20us/step - loss: 3.2326 - binary_accuracy: 0.5222 - val_loss: 3.0054 - val_binary_accuracy: 0.5191\n",
            "Epoch 3/60\n",
            "23428/23428 [==============================] - 0s 21us/step - loss: 2.8270 - binary_accuracy: 0.5286 - val_loss: 2.5886 - val_binary_accuracy: 0.5191\n",
            "Epoch 4/60\n",
            "23428/23428 [==============================] - 0s 21us/step - loss: 2.4183 - binary_accuracy: 0.5301 - val_loss: 2.1967 - val_binary_accuracy: 0.5191\n",
            "Epoch 5/60\n",
            "23428/23428 [==============================] - 0s 21us/step - loss: 2.0461 - binary_accuracy: 0.5300 - val_loss: 1.8546 - val_binary_accuracy: 0.5191\n",
            "Epoch 6/60\n",
            "23428/23428 [==============================] - 0s 21us/step - loss: 1.7297 - binary_accuracy: 0.5362 - val_loss: 1.5706 - val_binary_accuracy: 0.5191\n",
            "Epoch 7/60\n",
            "23428/23428 [==============================] - 0s 21us/step - loss: 1.4731 - binary_accuracy: 0.5521 - val_loss: 1.3506 - val_binary_accuracy: 0.5196\n",
            "Epoch 8/60\n",
            "23428/23428 [==============================] - 0s 20us/step - loss: 1.2817 - binary_accuracy: 0.5792 - val_loss: 1.1897 - val_binary_accuracy: 0.5523\n",
            "Epoch 9/60\n",
            "23428/23428 [==============================] - 0s 21us/step - loss: 1.1350 - binary_accuracy: 0.6191 - val_loss: 1.0512 - val_binary_accuracy: 0.6888\n",
            "Epoch 10/60\n",
            "23428/23428 [==============================] - 0s 20us/step - loss: 1.0091 - binary_accuracy: 0.6745 - val_loss: 0.9287 - val_binary_accuracy: 0.8394\n",
            "Epoch 11/60\n",
            "23428/23428 [==============================] - 0s 21us/step - loss: 0.8990 - binary_accuracy: 0.7202 - val_loss: 0.8087 - val_binary_accuracy: 0.8967\n",
            "Epoch 12/60\n",
            "23428/23428 [==============================] - 0s 20us/step - loss: 0.7979 - binary_accuracy: 0.7585 - val_loss: 0.6885 - val_binary_accuracy: 0.9277\n",
            "Epoch 13/60\n",
            "23428/23428 [==============================] - 0s 21us/step - loss: 0.7019 - binary_accuracy: 0.8015 - val_loss: 0.5727 - val_binary_accuracy: 0.9455\n",
            "Epoch 14/60\n",
            "23428/23428 [==============================] - 0s 20us/step - loss: 0.6183 - binary_accuracy: 0.8304 - val_loss: 0.4684 - val_binary_accuracy: 0.9510\n",
            "Epoch 15/60\n",
            "23428/23428 [==============================] - 1s 22us/step - loss: 0.5528 - binary_accuracy: 0.8521 - val_loss: 0.3921 - val_binary_accuracy: 0.9554\n",
            "Epoch 16/60\n",
            "23428/23428 [==============================] - 0s 20us/step - loss: 0.4941 - binary_accuracy: 0.8759 - val_loss: 0.3361 - val_binary_accuracy: 0.9578\n",
            "Epoch 17/60\n",
            "23428/23428 [==============================] - 0s 21us/step - loss: 0.4497 - binary_accuracy: 0.8902 - val_loss: 0.2960 - val_binary_accuracy: 0.9613\n",
            "Epoch 18/60\n",
            "23428/23428 [==============================] - 0s 21us/step - loss: 0.4166 - binary_accuracy: 0.9016 - val_loss: 0.2714 - val_binary_accuracy: 0.9614\n",
            "Epoch 19/60\n",
            "23428/23428 [==============================] - 0s 21us/step - loss: 0.3898 - binary_accuracy: 0.9105 - val_loss: 0.2530 - val_binary_accuracy: 0.9622\n",
            "Epoch 20/60\n",
            "23428/23428 [==============================] - 0s 20us/step - loss: 0.3670 - binary_accuracy: 0.9179 - val_loss: 0.2383 - val_binary_accuracy: 0.9631\n",
            "Epoch 21/60\n",
            "23428/23428 [==============================] - 0s 20us/step - loss: 0.3473 - binary_accuracy: 0.9242 - val_loss: 0.2287 - val_binary_accuracy: 0.9634\n",
            "Epoch 22/60\n",
            "23428/23428 [==============================] - 0s 20us/step - loss: 0.3337 - binary_accuracy: 0.9293 - val_loss: 0.2220 - val_binary_accuracy: 0.9649\n",
            "Epoch 23/60\n",
            "23428/23428 [==============================] - 0s 20us/step - loss: 0.3237 - binary_accuracy: 0.9315 - val_loss: 0.2168 - val_binary_accuracy: 0.9666\n",
            "Epoch 24/60\n",
            "23428/23428 [==============================] - 0s 21us/step - loss: 0.3150 - binary_accuracy: 0.9343 - val_loss: 0.2135 - val_binary_accuracy: 0.9672\n",
            "Epoch 25/60\n",
            "23428/23428 [==============================] - 1s 23us/step - loss: 0.3067 - binary_accuracy: 0.9360 - val_loss: 0.2104 - val_binary_accuracy: 0.9672\n",
            "Epoch 26/60\n",
            "23428/23428 [==============================] - 0s 21us/step - loss: 0.3046 - binary_accuracy: 0.9392 - val_loss: 0.2085 - val_binary_accuracy: 0.9664\n",
            "Epoch 27/60\n",
            "23428/23428 [==============================] - 0s 20us/step - loss: 0.2974 - binary_accuracy: 0.9417 - val_loss: 0.2044 - val_binary_accuracy: 0.9679\n",
            "Epoch 28/60\n",
            "23428/23428 [==============================] - 0s 21us/step - loss: 0.2923 - binary_accuracy: 0.9414 - val_loss: 0.2033 - val_binary_accuracy: 0.9681\n",
            "Epoch 29/60\n",
            "23428/23428 [==============================] - 0s 21us/step - loss: 0.2873 - binary_accuracy: 0.9446 - val_loss: 0.2015 - val_binary_accuracy: 0.9685\n",
            "Epoch 30/60\n",
            "23428/23428 [==============================] - 0s 21us/step - loss: 0.2810 - binary_accuracy: 0.9459 - val_loss: 0.1992 - val_binary_accuracy: 0.9693\n",
            "Epoch 31/60\n",
            "23428/23428 [==============================] - 0s 20us/step - loss: 0.2787 - binary_accuracy: 0.9455 - val_loss: 0.1978 - val_binary_accuracy: 0.9696\n",
            "Epoch 32/60\n",
            "23428/23428 [==============================] - 1s 22us/step - loss: 0.2798 - binary_accuracy: 0.9474 - val_loss: 0.1987 - val_binary_accuracy: 0.9694\n",
            "Epoch 33/60\n",
            "23428/23428 [==============================] - 0s 20us/step - loss: 0.2757 - binary_accuracy: 0.9463 - val_loss: 0.1971 - val_binary_accuracy: 0.9697\n",
            "Epoch 34/60\n",
            "23428/23428 [==============================] - 0s 21us/step - loss: 0.2709 - binary_accuracy: 0.9496 - val_loss: 0.1950 - val_binary_accuracy: 0.9714\n",
            "Epoch 35/60\n",
            "23428/23428 [==============================] - 0s 20us/step - loss: 0.2675 - binary_accuracy: 0.9497 - val_loss: 0.1948 - val_binary_accuracy: 0.9720\n",
            "Epoch 36/60\n",
            "23428/23428 [==============================] - 0s 21us/step - loss: 0.2648 - binary_accuracy: 0.9505 - val_loss: 0.1927 - val_binary_accuracy: 0.9706\n",
            "Epoch 37/60\n",
            "23428/23428 [==============================] - 0s 20us/step - loss: 0.2621 - binary_accuracy: 0.9542 - val_loss: 0.1916 - val_binary_accuracy: 0.9711\n",
            "Epoch 38/60\n",
            "23428/23428 [==============================] - 0s 21us/step - loss: 0.2618 - binary_accuracy: 0.9519 - val_loss: 0.1918 - val_binary_accuracy: 0.9714\n",
            "Epoch 39/60\n",
            "23428/23428 [==============================] - 0s 20us/step - loss: 0.2616 - binary_accuracy: 0.9506 - val_loss: 0.1935 - val_binary_accuracy: 0.9701\n",
            "Epoch 40/60\n",
            "23428/23428 [==============================] - 0s 21us/step - loss: 0.2628 - binary_accuracy: 0.9521 - val_loss: 0.1943 - val_binary_accuracy: 0.9712\n",
            "Epoch 41/60\n",
            "23428/23428 [==============================] - 0s 20us/step - loss: 0.2610 - binary_accuracy: 0.9502 - val_loss: 0.1932 - val_binary_accuracy: 0.9724\n",
            "Epoch 42/60\n",
            "23428/23428 [==============================] - 0s 21us/step - loss: 0.2561 - binary_accuracy: 0.9544 - val_loss: 0.1931 - val_binary_accuracy: 0.9714\n",
            "Epoch 43/60\n",
            "23428/23428 [==============================] - 0s 20us/step - loss: 0.2538 - binary_accuracy: 0.9559 - val_loss: 0.1911 - val_binary_accuracy: 0.9722\n",
            "Epoch 44/60\n",
            "23428/23428 [==============================] - 0s 21us/step - loss: 0.2558 - binary_accuracy: 0.9533 - val_loss: 0.1925 - val_binary_accuracy: 0.9718\n",
            "Epoch 45/60\n",
            "23428/23428 [==============================] - 0s 20us/step - loss: 0.2531 - binary_accuracy: 0.9561 - val_loss: 0.1921 - val_binary_accuracy: 0.9736\n",
            "Epoch 46/60\n",
            "23428/23428 [==============================] - 0s 21us/step - loss: 0.2536 - binary_accuracy: 0.9526 - val_loss: 0.2007 - val_binary_accuracy: 0.9684\n",
            "Epoch 47/60\n",
            "23428/23428 [==============================] - 1s 22us/step - loss: 0.2539 - binary_accuracy: 0.9551 - val_loss: 0.1953 - val_binary_accuracy: 0.9726\n",
            "Epoch 48/60\n",
            "23428/23428 [==============================] - 0s 21us/step - loss: 0.2591 - binary_accuracy: 0.9552 - val_loss: 0.1919 - val_binary_accuracy: 0.9724\n",
            "Epoch 49/60\n",
            "23428/23428 [==============================] - 0s 21us/step - loss: 0.2571 - binary_accuracy: 0.9528 - val_loss: 0.1938 - val_binary_accuracy: 0.9725\n",
            "Epoch 50/60\n",
            "23428/23428 [==============================] - 0s 21us/step - loss: 0.2552 - binary_accuracy: 0.9551 - val_loss: 0.1950 - val_binary_accuracy: 0.9737\n",
            "Epoch 51/60\n",
            "23428/23428 [==============================] - 0s 21us/step - loss: 0.2529 - binary_accuracy: 0.9565 - val_loss: 0.1954 - val_binary_accuracy: 0.9745\n",
            "Epoch 52/60\n",
            "23428/23428 [==============================] - 1s 22us/step - loss: 0.2516 - binary_accuracy: 0.9573 - val_loss: 0.1965 - val_binary_accuracy: 0.9737\n",
            "Epoch 53/60\n",
            "23428/23428 [==============================] - 0s 21us/step - loss: 0.2538 - binary_accuracy: 0.9568 - val_loss: 0.1979 - val_binary_accuracy: 0.9732\n",
            "Epoch 54/60\n",
            "23428/23428 [==============================] - 1s 22us/step - loss: 0.2548 - binary_accuracy: 0.9562 - val_loss: 0.2078 - val_binary_accuracy: 0.9703\n",
            "Epoch 55/60\n",
            "23428/23428 [==============================] - 0s 21us/step - loss: 0.2555 - binary_accuracy: 0.9562 - val_loss: 0.2008 - val_binary_accuracy: 0.9719\n",
            "Epoch 56/60\n",
            "23428/23428 [==============================] - 0s 21us/step - loss: 0.2549 - binary_accuracy: 0.9583 - val_loss: 0.1984 - val_binary_accuracy: 0.9721\n",
            "Epoch 57/60\n",
            "23428/23428 [==============================] - 0s 21us/step - loss: 0.2525 - binary_accuracy: 0.9562 - val_loss: 0.2014 - val_binary_accuracy: 0.9719\n",
            "Epoch 58/60\n",
            "23428/23428 [==============================] - 0s 20us/step - loss: 0.2493 - binary_accuracy: 0.9577 - val_loss: 0.1989 - val_binary_accuracy: 0.9729\n",
            "Epoch 59/60\n",
            "23428/23428 [==============================] - 0s 21us/step - loss: 0.2480 - binary_accuracy: 0.9599 - val_loss: 0.1956 - val_binary_accuracy: 0.9721\n",
            "Epoch 60/60\n",
            "23428/23428 [==============================] - 0s 21us/step - loss: 0.2498 - binary_accuracy: 0.9565 - val_loss: 0.1972 - val_binary_accuracy: 0.9731\n",
            "23428/23428 [==============================] - 1s 41us/step\n",
            "8000/8000 [==============================] - 0s 41us/step\n",
            "\n",
            "\n",
            "Ensayando modelo con estructura [64, 32, 10, 1] y optimizador <keras.optimizers.RMSprop object at 0x7f535fc21390>\n",
            "(23428, 1422) (8000, 1422) (13470, 1422) (23428,) (8000,) (13470,)\n",
            "Train on 23428 samples, validate on 8000 samples\n",
            "Epoch 1/60\n",
            "23428/23428 [==============================] - 1s 29us/step - loss: 2.9627 - binary_accuracy: 0.5202 - val_loss: 2.4058 - val_binary_accuracy: 0.5191\n",
            "Epoch 2/60\n",
            "23428/23428 [==============================] - 1s 22us/step - loss: 2.1350 - binary_accuracy: 0.5244 - val_loss: 1.8112 - val_binary_accuracy: 0.5191\n",
            "Epoch 3/60\n",
            "23428/23428 [==============================] - 1s 22us/step - loss: 1.6178 - binary_accuracy: 0.5283 - val_loss: 1.3735 - val_binary_accuracy: 0.5255\n",
            "Epoch 4/60\n",
            "23428/23428 [==============================] - 1s 22us/step - loss: 1.2331 - binary_accuracy: 0.5697 - val_loss: 1.0450 - val_binary_accuracy: 0.5856\n",
            "Epoch 5/60\n",
            "23428/23428 [==============================] - 0s 21us/step - loss: 0.9510 - binary_accuracy: 0.6517 - val_loss: 0.8131 - val_binary_accuracy: 0.6646\n",
            "Epoch 6/60\n",
            "23428/23428 [==============================] - 0s 21us/step - loss: 0.7691 - binary_accuracy: 0.7013 - val_loss: 0.6761 - val_binary_accuracy: 0.7521\n",
            "Epoch 7/60\n",
            "23428/23428 [==============================] - 0s 21us/step - loss: 0.6758 - binary_accuracy: 0.7437 - val_loss: 0.6263 - val_binary_accuracy: 0.8018\n",
            "Epoch 8/60\n",
            "23428/23428 [==============================] - 1s 22us/step - loss: 0.6502 - binary_accuracy: 0.7771 - val_loss: 0.6065 - val_binary_accuracy: 0.7969\n",
            "Epoch 9/60\n",
            "23428/23428 [==============================] - 0s 21us/step - loss: 0.6310 - binary_accuracy: 0.8034 - val_loss: 0.5800 - val_binary_accuracy: 0.8503\n",
            "Epoch 10/60\n",
            "23428/23428 [==============================] - 0s 21us/step - loss: 0.6117 - binary_accuracy: 0.8266 - val_loss: 0.5552 - val_binary_accuracy: 0.8865\n",
            "Epoch 11/60\n",
            "23428/23428 [==============================] - 0s 21us/step - loss: 0.5918 - binary_accuracy: 0.8489 - val_loss: 0.5354 - val_binary_accuracy: 0.8849\n",
            "Epoch 12/60\n",
            "23428/23428 [==============================] - 1s 22us/step - loss: 0.5773 - binary_accuracy: 0.8583 - val_loss: 0.5144 - val_binary_accuracy: 0.8957\n",
            "Epoch 13/60\n",
            "23428/23428 [==============================] - 0s 21us/step - loss: 0.5581 - binary_accuracy: 0.8749 - val_loss: 0.4964 - val_binary_accuracy: 0.9053\n",
            "Epoch 14/60\n",
            "23428/23428 [==============================] - 0s 21us/step - loss: 0.5415 - binary_accuracy: 0.8824 - val_loss: 0.4744 - val_binary_accuracy: 0.9231\n",
            "Epoch 15/60\n",
            "23428/23428 [==============================] - 1s 22us/step - loss: 0.5233 - binary_accuracy: 0.8928 - val_loss: 0.4551 - val_binary_accuracy: 0.9249\n",
            "Epoch 16/60\n",
            "23428/23428 [==============================] - 0s 21us/step - loss: 0.5083 - binary_accuracy: 0.8982 - val_loss: 0.4337 - val_binary_accuracy: 0.9356\n",
            "Epoch 17/60\n",
            "23428/23428 [==============================] - 0s 21us/step - loss: 0.4902 - binary_accuracy: 0.9036 - val_loss: 0.4164 - val_binary_accuracy: 0.9416\n",
            "Epoch 18/60\n",
            "23428/23428 [==============================] - 0s 21us/step - loss: 0.4727 - binary_accuracy: 0.9103 - val_loss: 0.4033 - val_binary_accuracy: 0.9390\n",
            "Epoch 19/60\n",
            "23428/23428 [==============================] - 1s 21us/step - loss: 0.4645 - binary_accuracy: 0.9121 - val_loss: 0.3784 - val_binary_accuracy: 0.9525\n",
            "Epoch 20/60\n",
            "23428/23428 [==============================] - 0s 21us/step - loss: 0.4423 - binary_accuracy: 0.9167 - val_loss: 0.3888 - val_binary_accuracy: 0.9486\n",
            "Epoch 21/60\n",
            "23428/23428 [==============================] - 0s 21us/step - loss: 0.4463 - binary_accuracy: 0.9154 - val_loss: 0.3580 - val_binary_accuracy: 0.9542\n",
            "Epoch 22/60\n",
            "23428/23428 [==============================] - 0s 21us/step - loss: 0.4214 - binary_accuracy: 0.9242 - val_loss: 0.3533 - val_binary_accuracy: 0.9526\n",
            "Epoch 23/60\n",
            "23428/23428 [==============================] - 0s 20us/step - loss: 0.4213 - binary_accuracy: 0.9218 - val_loss: 0.3425 - val_binary_accuracy: 0.9540\n",
            "Epoch 24/60\n",
            "23428/23428 [==============================] - 0s 21us/step - loss: 0.4135 - binary_accuracy: 0.9225 - val_loss: 0.3376 - val_binary_accuracy: 0.9528\n",
            "Epoch 25/60\n",
            "23428/23428 [==============================] - 0s 21us/step - loss: 0.4008 - binary_accuracy: 0.9289 - val_loss: 0.3291 - val_binary_accuracy: 0.9531\n",
            "Epoch 26/60\n",
            "23428/23428 [==============================] - 1s 22us/step - loss: 0.3941 - binary_accuracy: 0.9295 - val_loss: 0.3251 - val_binary_accuracy: 0.9517\n",
            "Epoch 27/60\n",
            "23428/23428 [==============================] - 0s 21us/step - loss: 0.3833 - binary_accuracy: 0.9332 - val_loss: 0.3017 - val_binary_accuracy: 0.9563\n",
            "Epoch 28/60\n",
            "23428/23428 [==============================] - 0s 21us/step - loss: 0.3859 - binary_accuracy: 0.9327 - val_loss: 0.3116 - val_binary_accuracy: 0.9521\n",
            "Epoch 29/60\n",
            "23428/23428 [==============================] - 0s 21us/step - loss: 0.3796 - binary_accuracy: 0.9318 - val_loss: 0.2952 - val_binary_accuracy: 0.9560\n",
            "Epoch 30/60\n",
            "23428/23428 [==============================] - 0s 20us/step - loss: 0.3707 - binary_accuracy: 0.9360 - val_loss: 0.2960 - val_binary_accuracy: 0.9542\n",
            "Epoch 31/60\n",
            "23428/23428 [==============================] - 0s 20us/step - loss: 0.3645 - binary_accuracy: 0.9375 - val_loss: 0.2960 - val_binary_accuracy: 0.9534\n",
            "Epoch 32/60\n",
            "23428/23428 [==============================] - 0s 21us/step - loss: 0.3722 - binary_accuracy: 0.9356 - val_loss: 0.2759 - val_binary_accuracy: 0.9594\n",
            "Epoch 33/60\n",
            "23428/23428 [==============================] - 0s 21us/step - loss: 0.3465 - binary_accuracy: 0.9427 - val_loss: 0.3179 - val_binary_accuracy: 0.9427\n",
            "Epoch 34/60\n",
            "23428/23428 [==============================] - 0s 21us/step - loss: 0.3630 - binary_accuracy: 0.9365 - val_loss: 0.2717 - val_binary_accuracy: 0.9588\n",
            "Epoch 35/60\n",
            "23428/23428 [==============================] - 0s 20us/step - loss: 0.3487 - binary_accuracy: 0.9418 - val_loss: 0.3032 - val_binary_accuracy: 0.9499\n",
            "Epoch 36/60\n",
            "23428/23428 [==============================] - 0s 21us/step - loss: 0.3537 - binary_accuracy: 0.9393 - val_loss: 0.2644 - val_binary_accuracy: 0.9603\n",
            "Epoch 37/60\n",
            "23428/23428 [==============================] - 0s 20us/step - loss: 0.3328 - binary_accuracy: 0.9472 - val_loss: 0.2640 - val_binary_accuracy: 0.9615\n",
            "Epoch 38/60\n",
            "23428/23428 [==============================] - 0s 20us/step - loss: 0.3361 - binary_accuracy: 0.9458 - val_loss: 0.3069 - val_binary_accuracy: 0.9400\n",
            "Epoch 39/60\n",
            "23428/23428 [==============================] - 0s 21us/step - loss: 0.3589 - binary_accuracy: 0.9382 - val_loss: 0.2546 - val_binary_accuracy: 0.9622\n",
            "Epoch 40/60\n",
            "23428/23428 [==============================] - 0s 21us/step - loss: 0.3400 - binary_accuracy: 0.9428 - val_loss: 0.2751 - val_binary_accuracy: 0.9538\n",
            "Epoch 41/60\n",
            "23428/23428 [==============================] - 0s 20us/step - loss: 0.3390 - binary_accuracy: 0.9444 - val_loss: 0.2593 - val_binary_accuracy: 0.9594\n",
            "Epoch 42/60\n",
            "23428/23428 [==============================] - 0s 21us/step - loss: 0.3332 - binary_accuracy: 0.9460 - val_loss: 0.2600 - val_binary_accuracy: 0.9589\n",
            "Epoch 43/60\n",
            "23428/23428 [==============================] - 0s 21us/step - loss: 0.3214 - binary_accuracy: 0.9501 - val_loss: 0.2488 - val_binary_accuracy: 0.9638\n",
            "Epoch 44/60\n",
            "23428/23428 [==============================] - 0s 20us/step - loss: 0.3368 - binary_accuracy: 0.9431 - val_loss: 0.2631 - val_binary_accuracy: 0.9576\n",
            "Epoch 45/60\n",
            "23428/23428 [==============================] - 0s 20us/step - loss: 0.3178 - binary_accuracy: 0.9499 - val_loss: 0.2546 - val_binary_accuracy: 0.9590\n",
            "Epoch 46/60\n",
            "23428/23428 [==============================] - 0s 20us/step - loss: 0.3271 - binary_accuracy: 0.9475 - val_loss: 0.2498 - val_binary_accuracy: 0.9610\n",
            "Epoch 47/60\n",
            "23428/23428 [==============================] - 0s 20us/step - loss: 0.3144 - binary_accuracy: 0.9520 - val_loss: 0.2409 - val_binary_accuracy: 0.9643\n",
            "Epoch 48/60\n",
            "23428/23428 [==============================] - 1s 22us/step - loss: 0.3194 - binary_accuracy: 0.9500 - val_loss: 0.2716 - val_binary_accuracy: 0.9588\n",
            "Epoch 49/60\n",
            "23428/23428 [==============================] - 0s 21us/step - loss: 0.3103 - binary_accuracy: 0.9529 - val_loss: 0.2550 - val_binary_accuracy: 0.9610\n",
            "Epoch 50/60\n",
            "23428/23428 [==============================] - 0s 20us/step - loss: 0.3249 - binary_accuracy: 0.9463 - val_loss: 0.2474 - val_binary_accuracy: 0.9643\n",
            "Epoch 51/60\n",
            "23428/23428 [==============================] - 0s 21us/step - loss: 0.3087 - binary_accuracy: 0.9529 - val_loss: 0.2475 - val_binary_accuracy: 0.9639\n",
            "Epoch 52/60\n",
            "23428/23428 [==============================] - 0s 20us/step - loss: 0.3148 - binary_accuracy: 0.9512 - val_loss: 0.2322 - val_binary_accuracy: 0.9681\n",
            "Epoch 53/60\n",
            "23428/23428 [==============================] - 0s 21us/step - loss: 0.2948 - binary_accuracy: 0.9580 - val_loss: 0.2358 - val_binary_accuracy: 0.9660\n",
            "Epoch 54/60\n",
            "23428/23428 [==============================] - 0s 20us/step - loss: 0.3193 - binary_accuracy: 0.9475 - val_loss: 0.2330 - val_binary_accuracy: 0.9675\n",
            "Epoch 55/60\n",
            "23428/23428 [==============================] - 0s 21us/step - loss: 0.2994 - binary_accuracy: 0.9547 - val_loss: 0.2707 - val_binary_accuracy: 0.9513\n",
            "Epoch 56/60\n",
            "23428/23428 [==============================] - 0s 21us/step - loss: 0.3104 - binary_accuracy: 0.9515 - val_loss: 0.2364 - val_binary_accuracy: 0.9645\n",
            "Epoch 57/60\n",
            "23428/23428 [==============================] - 0s 21us/step - loss: 0.3037 - binary_accuracy: 0.9529 - val_loss: 0.2590 - val_binary_accuracy: 0.9549\n",
            "Epoch 58/60\n",
            "23428/23428 [==============================] - 0s 20us/step - loss: 0.2914 - binary_accuracy: 0.9572 - val_loss: 0.2344 - val_binary_accuracy: 0.9645\n",
            "Epoch 59/60\n",
            "23428/23428 [==============================] - 0s 20us/step - loss: 0.3130 - binary_accuracy: 0.9487 - val_loss: 0.2399 - val_binary_accuracy: 0.9645\n",
            "Epoch 60/60\n",
            "23428/23428 [==============================] - 0s 20us/step - loss: 0.2943 - binary_accuracy: 0.9576 - val_loss: 0.2429 - val_binary_accuracy: 0.9607\n",
            "23428/23428 [==============================] - 1s 41us/step\n",
            "8000/8000 [==============================] - 0s 41us/step\n",
            "(23428, 1422) (8000, 1422) (13470, 1422) (23428,) (8000,) (13470,)\n",
            "Train on 23428 samples, validate on 8000 samples\n",
            "Epoch 1/60\n",
            "23428/23428 [==============================] - 0s 21us/step - loss: 3.3441 - binary_accuracy: 0.5211 - val_loss: 3.0272 - val_binary_accuracy: 0.5191\n",
            "Epoch 2/60\n",
            "23428/23428 [==============================] - 0s 20us/step - loss: 2.8017 - binary_accuracy: 0.5230 - val_loss: 2.5062 - val_binary_accuracy: 0.5191\n",
            "Epoch 3/60\n",
            "23428/23428 [==============================] - 0s 21us/step - loss: 2.2993 - binary_accuracy: 0.5239 - val_loss: 2.0315 - val_binary_accuracy: 0.5191\n",
            "Epoch 4/60\n",
            "23428/23428 [==============================] - 0s 20us/step - loss: 1.8494 - binary_accuracy: 0.5248 - val_loss: 1.6158 - val_binary_accuracy: 0.5191\n",
            "Epoch 5/60\n",
            "23428/23428 [==============================] - 0s 21us/step - loss: 1.4633 - binary_accuracy: 0.5262 - val_loss: 1.2707 - val_binary_accuracy: 0.5191\n",
            "Epoch 6/60\n",
            "23428/23428 [==============================] - 0s 20us/step - loss: 1.1521 - binary_accuracy: 0.5444 - val_loss: 1.0006 - val_binary_accuracy: 0.5725\n",
            "Epoch 7/60\n",
            "23428/23428 [==============================] - 1s 22us/step - loss: 0.9197 - binary_accuracy: 0.6074 - val_loss: 0.8095 - val_binary_accuracy: 0.6513\n",
            "Epoch 8/60\n",
            "23428/23428 [==============================] - 0s 20us/step - loss: 0.7728 - binary_accuracy: 0.6797 - val_loss: 0.7032 - val_binary_accuracy: 0.7415\n",
            "Epoch 9/60\n",
            "23428/23428 [==============================] - 0s 21us/step - loss: 0.7022 - binary_accuracy: 0.7415 - val_loss: 0.6510 - val_binary_accuracy: 0.7648\n",
            "Epoch 10/60\n",
            "23428/23428 [==============================] - 0s 20us/step - loss: 0.6625 - binary_accuracy: 0.7825 - val_loss: 0.6095 - val_binary_accuracy: 0.8369\n",
            "Epoch 11/60\n",
            "23428/23428 [==============================] - 0s 21us/step - loss: 0.6333 - binary_accuracy: 0.8140 - val_loss: 0.5793 - val_binary_accuracy: 0.8501\n",
            "Epoch 12/60\n",
            "23428/23428 [==============================] - 0s 20us/step - loss: 0.6105 - binary_accuracy: 0.8317 - val_loss: 0.5474 - val_binary_accuracy: 0.8827\n",
            "Epoch 13/60\n",
            "23428/23428 [==============================] - 0s 20us/step - loss: 0.5836 - binary_accuracy: 0.8482 - val_loss: 0.5155 - val_binary_accuracy: 0.9069\n",
            "Epoch 14/60\n",
            "23428/23428 [==============================] - 0s 20us/step - loss: 0.5630 - binary_accuracy: 0.8637 - val_loss: 0.4935 - val_binary_accuracy: 0.9043\n",
            "Epoch 15/60\n",
            "23428/23428 [==============================] - 0s 21us/step - loss: 0.5435 - binary_accuracy: 0.8720 - val_loss: 0.4651 - val_binary_accuracy: 0.9237\n",
            "Epoch 16/60\n",
            "23428/23428 [==============================] - 0s 20us/step - loss: 0.5268 - binary_accuracy: 0.8811 - val_loss: 0.4420 - val_binary_accuracy: 0.9330\n",
            "Epoch 17/60\n",
            "23428/23428 [==============================] - 0s 20us/step - loss: 0.5044 - binary_accuracy: 0.8911 - val_loss: 0.4300 - val_binary_accuracy: 0.9251\n",
            "Epoch 18/60\n",
            "23428/23428 [==============================] - 0s 21us/step - loss: 0.4927 - binary_accuracy: 0.8936 - val_loss: 0.4131 - val_binary_accuracy: 0.9286\n",
            "Epoch 19/60\n",
            "23428/23428 [==============================] - 0s 20us/step - loss: 0.4675 - binary_accuracy: 0.9020 - val_loss: 0.3922 - val_binary_accuracy: 0.9395\n",
            "Epoch 20/60\n",
            "23428/23428 [==============================] - 0s 20us/step - loss: 0.4619 - binary_accuracy: 0.9048 - val_loss: 0.3727 - val_binary_accuracy: 0.9471\n",
            "Epoch 21/60\n",
            "23428/23428 [==============================] - 0s 21us/step - loss: 0.4550 - binary_accuracy: 0.9068 - val_loss: 0.3694 - val_binary_accuracy: 0.9441\n",
            "Epoch 22/60\n",
            "23428/23428 [==============================] - 0s 21us/step - loss: 0.4415 - binary_accuracy: 0.9143 - val_loss: 0.3478 - val_binary_accuracy: 0.9517\n",
            "Epoch 23/60\n",
            "23428/23428 [==============================] - 0s 20us/step - loss: 0.4239 - binary_accuracy: 0.9169 - val_loss: 0.3529 - val_binary_accuracy: 0.9420\n",
            "Epoch 24/60\n",
            "23428/23428 [==============================] - 0s 21us/step - loss: 0.4178 - binary_accuracy: 0.9201 - val_loss: 0.3334 - val_binary_accuracy: 0.9480\n",
            "Epoch 25/60\n",
            "23428/23428 [==============================] - 0s 20us/step - loss: 0.4159 - binary_accuracy: 0.9187 - val_loss: 0.3237 - val_binary_accuracy: 0.9526\n",
            "Epoch 26/60\n",
            "23428/23428 [==============================] - 0s 21us/step - loss: 0.3962 - binary_accuracy: 0.9259 - val_loss: 0.3197 - val_binary_accuracy: 0.9496\n",
            "Epoch 27/60\n",
            "23428/23428 [==============================] - 0s 20us/step - loss: 0.4020 - binary_accuracy: 0.9232 - val_loss: 0.3177 - val_binary_accuracy: 0.9505\n",
            "Epoch 28/60\n",
            "23428/23428 [==============================] - 0s 21us/step - loss: 0.3951 - binary_accuracy: 0.9257 - val_loss: 0.3003 - val_binary_accuracy: 0.9553\n",
            "Epoch 29/60\n",
            "23428/23428 [==============================] - 0s 21us/step - loss: 0.3823 - binary_accuracy: 0.9319 - val_loss: 0.3093 - val_binary_accuracy: 0.9501\n",
            "Epoch 30/60\n",
            "23428/23428 [==============================] - 0s 21us/step - loss: 0.3813 - binary_accuracy: 0.9304 - val_loss: 0.3120 - val_binary_accuracy: 0.9481\n",
            "Epoch 31/60\n",
            "23428/23428 [==============================] - 0s 20us/step - loss: 0.3737 - binary_accuracy: 0.9331 - val_loss: 0.2971 - val_binary_accuracy: 0.9525\n",
            "Epoch 32/60\n",
            "23428/23428 [==============================] - 0s 21us/step - loss: 0.3770 - binary_accuracy: 0.9318 - val_loss: 0.2879 - val_binary_accuracy: 0.9560\n",
            "Epoch 33/60\n",
            "23428/23428 [==============================] - 0s 20us/step - loss: 0.3653 - binary_accuracy: 0.9367 - val_loss: 0.2981 - val_binary_accuracy: 0.9498\n",
            "Epoch 34/60\n",
            "23428/23428 [==============================] - 0s 21us/step - loss: 0.3718 - binary_accuracy: 0.9341 - val_loss: 0.2783 - val_binary_accuracy: 0.9579\n",
            "Epoch 35/60\n",
            "23428/23428 [==============================] - 0s 20us/step - loss: 0.3640 - binary_accuracy: 0.9361 - val_loss: 0.2820 - val_binary_accuracy: 0.9572\n",
            "Epoch 36/60\n",
            "23428/23428 [==============================] - 0s 20us/step - loss: 0.3446 - binary_accuracy: 0.9429 - val_loss: 0.3006 - val_binary_accuracy: 0.9479\n",
            "Epoch 37/60\n",
            "23428/23428 [==============================] - 0s 21us/step - loss: 0.3561 - binary_accuracy: 0.9402 - val_loss: 0.2686 - val_binary_accuracy: 0.9588\n",
            "Epoch 38/60\n",
            "23428/23428 [==============================] - 0s 20us/step - loss: 0.3563 - binary_accuracy: 0.9359 - val_loss: 0.2915 - val_binary_accuracy: 0.9529\n",
            "Epoch 39/60\n",
            "23428/23428 [==============================] - 0s 20us/step - loss: 0.3478 - binary_accuracy: 0.9420 - val_loss: 0.2949 - val_binary_accuracy: 0.9484\n",
            "Epoch 40/60\n",
            "23428/23428 [==============================] - 0s 20us/step - loss: 0.3424 - binary_accuracy: 0.9416 - val_loss: 0.2697 - val_binary_accuracy: 0.9570\n",
            "Epoch 41/60\n",
            "23428/23428 [==============================] - 0s 20us/step - loss: 0.3462 - binary_accuracy: 0.9413 - val_loss: 0.2685 - val_binary_accuracy: 0.9575\n",
            "Epoch 42/60\n",
            "23428/23428 [==============================] - 0s 21us/step - loss: 0.3350 - binary_accuracy: 0.9455 - val_loss: 0.2824 - val_binary_accuracy: 0.9516\n",
            "Epoch 43/60\n",
            "23428/23428 [==============================] - 0s 21us/step - loss: 0.3479 - binary_accuracy: 0.9405 - val_loss: 0.2630 - val_binary_accuracy: 0.9589\n",
            "Epoch 44/60\n",
            "23428/23428 [==============================] - 0s 20us/step - loss: 0.3391 - binary_accuracy: 0.9438 - val_loss: 0.2632 - val_binary_accuracy: 0.9585\n",
            "Epoch 45/60\n",
            "23428/23428 [==============================] - 0s 20us/step - loss: 0.3273 - binary_accuracy: 0.9480 - val_loss: 0.2842 - val_binary_accuracy: 0.9517\n",
            "Epoch 46/60\n",
            "23428/23428 [==============================] - 0s 21us/step - loss: 0.3418 - binary_accuracy: 0.9422 - val_loss: 0.2662 - val_binary_accuracy: 0.9571\n",
            "Epoch 47/60\n",
            "23428/23428 [==============================] - 0s 21us/step - loss: 0.3335 - binary_accuracy: 0.9456 - val_loss: 0.2522 - val_binary_accuracy: 0.9619\n",
            "Epoch 48/60\n",
            "23428/23428 [==============================] - 0s 20us/step - loss: 0.3215 - binary_accuracy: 0.9487 - val_loss: 0.2735 - val_binary_accuracy: 0.9549\n",
            "Epoch 49/60\n",
            "23428/23428 [==============================] - 1s 21us/step - loss: 0.3311 - binary_accuracy: 0.9480 - val_loss: 0.2608 - val_binary_accuracy: 0.9579\n",
            "Epoch 50/60\n",
            "23428/23428 [==============================] - 1s 22us/step - loss: 0.3370 - binary_accuracy: 0.9454 - val_loss: 0.2443 - val_binary_accuracy: 0.9645\n",
            "Epoch 51/60\n",
            "23428/23428 [==============================] - 0s 21us/step - loss: 0.3137 - binary_accuracy: 0.9519 - val_loss: 0.2651 - val_binary_accuracy: 0.9561\n",
            "Epoch 52/60\n",
            "23428/23428 [==============================] - 0s 21us/step - loss: 0.3337 - binary_accuracy: 0.9437 - val_loss: 0.2393 - val_binary_accuracy: 0.9655\n",
            "Epoch 53/60\n",
            "23428/23428 [==============================] - 0s 20us/step - loss: 0.3014 - binary_accuracy: 0.9559 - val_loss: 0.2526 - val_binary_accuracy: 0.9607\n",
            "Epoch 54/60\n",
            "23428/23428 [==============================] - 0s 21us/step - loss: 0.3346 - binary_accuracy: 0.9440 - val_loss: 0.2373 - val_binary_accuracy: 0.9661\n",
            "Epoch 55/60\n",
            "23428/23428 [==============================] - 0s 21us/step - loss: 0.3153 - binary_accuracy: 0.9496 - val_loss: 0.2504 - val_binary_accuracy: 0.9603\n",
            "Epoch 56/60\n",
            "23428/23428 [==============================] - 0s 20us/step - loss: 0.3050 - binary_accuracy: 0.9537 - val_loss: 0.2627 - val_binary_accuracy: 0.9565\n",
            "Epoch 57/60\n",
            "23428/23428 [==============================] - 0s 20us/step - loss: 0.3208 - binary_accuracy: 0.9492 - val_loss: 0.2404 - val_binary_accuracy: 0.9634\n",
            "Epoch 58/60\n",
            "23428/23428 [==============================] - 0s 20us/step - loss: 0.3138 - binary_accuracy: 0.9504 - val_loss: 0.2460 - val_binary_accuracy: 0.9614\n",
            "Epoch 59/60\n",
            "23428/23428 [==============================] - 0s 21us/step - loss: 0.3154 - binary_accuracy: 0.9496 - val_loss: 0.2502 - val_binary_accuracy: 0.9604\n",
            "Epoch 60/60\n",
            "23428/23428 [==============================] - 0s 21us/step - loss: 0.3036 - binary_accuracy: 0.9545 - val_loss: 0.2438 - val_binary_accuracy: 0.9628\n",
            "23428/23428 [==============================] - 1s 41us/step\n",
            "8000/8000 [==============================] - 0s 44us/step\n",
            "(23428, 1422) (8000, 1422) (13470, 1422) (23428,) (8000,) (13470,)\n",
            "Train on 23428 samples, validate on 8000 samples\n",
            "Epoch 1/60\n",
            "23428/23428 [==============================] - 0s 20us/step - loss: 3.3528 - binary_accuracy: 0.5137 - val_loss: 3.0438 - val_binary_accuracy: 0.5191\n",
            "Epoch 2/60\n",
            "23428/23428 [==============================] - 0s 20us/step - loss: 2.8214 - binary_accuracy: 0.5223 - val_loss: 2.5292 - val_binary_accuracy: 0.5191\n",
            "Epoch 3/60\n",
            "23428/23428 [==============================] - 0s 20us/step - loss: 2.3233 - binary_accuracy: 0.5265 - val_loss: 2.0560 - val_binary_accuracy: 0.5191\n",
            "Epoch 4/60\n",
            "23428/23428 [==============================] - 0s 20us/step - loss: 1.8726 - binary_accuracy: 0.5252 - val_loss: 1.6386 - val_binary_accuracy: 0.5191\n",
            "Epoch 5/60\n",
            "23428/23428 [==============================] - 0s 21us/step - loss: 1.4839 - binary_accuracy: 0.5247 - val_loss: 1.2891 - val_binary_accuracy: 0.5191\n",
            "Epoch 6/60\n",
            "23428/23428 [==============================] - 0s 21us/step - loss: 1.1682 - binary_accuracy: 0.5370 - val_loss: 1.0104 - val_binary_accuracy: 0.5452\n",
            "Epoch 7/60\n",
            "23428/23428 [==============================] - 0s 21us/step - loss: 0.9284 - binary_accuracy: 0.5927 - val_loss: 0.8135 - val_binary_accuracy: 0.8018\n",
            "Epoch 8/60\n",
            "23428/23428 [==============================] - 0s 20us/step - loss: 0.7758 - binary_accuracy: 0.6640 - val_loss: 0.7105 - val_binary_accuracy: 0.8917\n",
            "Epoch 9/60\n",
            "23428/23428 [==============================] - 0s 20us/step - loss: 0.7023 - binary_accuracy: 0.7182 - val_loss: 0.6600 - val_binary_accuracy: 0.9064\n",
            "Epoch 10/60\n",
            "23428/23428 [==============================] - 1s 22us/step - loss: 0.6687 - binary_accuracy: 0.7555 - val_loss: 0.6278 - val_binary_accuracy: 0.9185\n",
            "Epoch 11/60\n",
            "23428/23428 [==============================] - 0s 20us/step - loss: 0.6444 - binary_accuracy: 0.7912 - val_loss: 0.6018 - val_binary_accuracy: 0.9280\n",
            "Epoch 12/60\n",
            "23428/23428 [==============================] - 0s 21us/step - loss: 0.6221 - binary_accuracy: 0.8163 - val_loss: 0.5826 - val_binary_accuracy: 0.9446\n",
            "Epoch 13/60\n",
            "23428/23428 [==============================] - 0s 21us/step - loss: 0.6012 - binary_accuracy: 0.8410 - val_loss: 0.5543 - val_binary_accuracy: 0.9480\n",
            "Epoch 14/60\n",
            "23428/23428 [==============================] - 1s 21us/step - loss: 0.5786 - binary_accuracy: 0.8612 - val_loss: 0.5237 - val_binary_accuracy: 0.9506\n",
            "Epoch 15/60\n",
            "23428/23428 [==============================] - 0s 21us/step - loss: 0.5606 - binary_accuracy: 0.8714 - val_loss: 0.5042 - val_binary_accuracy: 0.9501\n",
            "Epoch 16/60\n",
            "23428/23428 [==============================] - 0s 21us/step - loss: 0.5350 - binary_accuracy: 0.8850 - val_loss: 0.4853 - val_binary_accuracy: 0.9498\n",
            "Epoch 17/60\n",
            "23428/23428 [==============================] - 0s 21us/step - loss: 0.5226 - binary_accuracy: 0.8903 - val_loss: 0.4676 - val_binary_accuracy: 0.9496\n",
            "Epoch 18/60\n",
            "23428/23428 [==============================] - 1s 22us/step - loss: 0.5030 - binary_accuracy: 0.8951 - val_loss: 0.4379 - val_binary_accuracy: 0.9517\n",
            "Epoch 19/60\n",
            "23428/23428 [==============================] - 0s 21us/step - loss: 0.4902 - binary_accuracy: 0.9010 - val_loss: 0.4206 - val_binary_accuracy: 0.9514\n",
            "Epoch 20/60\n",
            "23428/23428 [==============================] - 1s 22us/step - loss: 0.4713 - binary_accuracy: 0.9087 - val_loss: 0.3984 - val_binary_accuracy: 0.9530\n",
            "Epoch 21/60\n",
            "23428/23428 [==============================] - 1s 21us/step - loss: 0.4656 - binary_accuracy: 0.9067 - val_loss: 0.3863 - val_binary_accuracy: 0.9526\n",
            "Epoch 22/60\n",
            "23428/23428 [==============================] - 0s 21us/step - loss: 0.4506 - binary_accuracy: 0.9114 - val_loss: 0.3694 - val_binary_accuracy: 0.9534\n",
            "Epoch 23/60\n",
            "23428/23428 [==============================] - 0s 21us/step - loss: 0.4296 - binary_accuracy: 0.9195 - val_loss: 0.3657 - val_binary_accuracy: 0.9511\n",
            "Epoch 24/60\n",
            "23428/23428 [==============================] - 0s 21us/step - loss: 0.4373 - binary_accuracy: 0.9134 - val_loss: 0.3606 - val_binary_accuracy: 0.9484\n",
            "Epoch 25/60\n",
            "23428/23428 [==============================] - 0s 21us/step - loss: 0.4176 - binary_accuracy: 0.9221 - val_loss: 0.3477 - val_binary_accuracy: 0.9501\n",
            "Epoch 26/60\n",
            "23428/23428 [==============================] - 0s 20us/step - loss: 0.4130 - binary_accuracy: 0.9215 - val_loss: 0.3341 - val_binary_accuracy: 0.9525\n",
            "Epoch 27/60\n",
            "23428/23428 [==============================] - 0s 21us/step - loss: 0.4058 - binary_accuracy: 0.9247 - val_loss: 0.3240 - val_binary_accuracy: 0.9523\n",
            "Epoch 28/60\n",
            "23428/23428 [==============================] - 0s 20us/step - loss: 0.4057 - binary_accuracy: 0.9256 - val_loss: 0.3141 - val_binary_accuracy: 0.9546\n",
            "Epoch 29/60\n",
            "23428/23428 [==============================] - 0s 20us/step - loss: 0.3831 - binary_accuracy: 0.9338 - val_loss: 0.3127 - val_binary_accuracy: 0.9516\n",
            "Epoch 30/60\n",
            "23428/23428 [==============================] - 0s 20us/step - loss: 0.3961 - binary_accuracy: 0.9261 - val_loss: 0.3093 - val_binary_accuracy: 0.9529\n",
            "Epoch 31/60\n",
            "23428/23428 [==============================] - 0s 21us/step - loss: 0.3813 - binary_accuracy: 0.9322 - val_loss: 0.3092 - val_binary_accuracy: 0.9501\n",
            "Epoch 32/60\n",
            "23428/23428 [==============================] - 1s 22us/step - loss: 0.3759 - binary_accuracy: 0.9337 - val_loss: 0.2853 - val_binary_accuracy: 0.9582\n",
            "Epoch 33/60\n",
            "23428/23428 [==============================] - 0s 20us/step - loss: 0.3772 - binary_accuracy: 0.9329 - val_loss: 0.2818 - val_binary_accuracy: 0.9600\n",
            "Epoch 34/60\n",
            "23428/23428 [==============================] - 0s 20us/step - loss: 0.3538 - binary_accuracy: 0.9396 - val_loss: 0.2811 - val_binary_accuracy: 0.9589\n",
            "Epoch 35/60\n",
            "23428/23428 [==============================] - 0s 20us/step - loss: 0.3777 - binary_accuracy: 0.9318 - val_loss: 0.2873 - val_binary_accuracy: 0.9557\n",
            "Epoch 36/60\n",
            "23428/23428 [==============================] - 0s 20us/step - loss: 0.3515 - binary_accuracy: 0.9437 - val_loss: 0.2721 - val_binary_accuracy: 0.9592\n",
            "Epoch 37/60\n",
            "23428/23428 [==============================] - 0s 20us/step - loss: 0.3610 - binary_accuracy: 0.9354 - val_loss: 0.2830 - val_binary_accuracy: 0.9542\n",
            "Epoch 38/60\n",
            "23428/23428 [==============================] - 0s 20us/step - loss: 0.3495 - binary_accuracy: 0.9401 - val_loss: 0.2949 - val_binary_accuracy: 0.9505\n",
            "Epoch 39/60\n",
            "23428/23428 [==============================] - 1s 21us/step - loss: 0.3546 - binary_accuracy: 0.9406 - val_loss: 0.2648 - val_binary_accuracy: 0.9605\n",
            "Epoch 40/60\n",
            "23428/23428 [==============================] - 0s 20us/step - loss: 0.3528 - binary_accuracy: 0.9372 - val_loss: 0.2808 - val_binary_accuracy: 0.9548\n",
            "Epoch 41/60\n",
            "23428/23428 [==============================] - 0s 21us/step - loss: 0.3356 - binary_accuracy: 0.9459 - val_loss: 0.2571 - val_binary_accuracy: 0.9617\n",
            "Epoch 42/60\n",
            "23428/23428 [==============================] - 0s 20us/step - loss: 0.3344 - binary_accuracy: 0.9464 - val_loss: 0.3067 - val_binary_accuracy: 0.9417\n",
            "Epoch 43/60\n",
            "23428/23428 [==============================] - 0s 20us/step - loss: 0.3544 - binary_accuracy: 0.9391 - val_loss: 0.2579 - val_binary_accuracy: 0.9610\n",
            "Epoch 44/60\n",
            "23428/23428 [==============================] - 0s 20us/step - loss: 0.3429 - binary_accuracy: 0.9422 - val_loss: 0.2638 - val_binary_accuracy: 0.9588\n",
            "Epoch 45/60\n",
            "23428/23428 [==============================] - 0s 21us/step - loss: 0.3348 - binary_accuracy: 0.9454 - val_loss: 0.2660 - val_binary_accuracy: 0.9574\n",
            "Epoch 46/60\n",
            "23428/23428 [==============================] - 0s 20us/step - loss: 0.3293 - binary_accuracy: 0.9475 - val_loss: 0.2582 - val_binary_accuracy: 0.9590\n",
            "Epoch 47/60\n",
            "23428/23428 [==============================] - 0s 20us/step - loss: 0.3335 - binary_accuracy: 0.9443 - val_loss: 0.2602 - val_binary_accuracy: 0.9586\n",
            "Epoch 48/60\n",
            "23428/23428 [==============================] - 0s 20us/step - loss: 0.3292 - binary_accuracy: 0.9465 - val_loss: 0.2521 - val_binary_accuracy: 0.9609\n",
            "Epoch 49/60\n",
            "23428/23428 [==============================] - 0s 20us/step - loss: 0.3231 - binary_accuracy: 0.9491 - val_loss: 0.2578 - val_binary_accuracy: 0.9584\n",
            "Epoch 50/60\n",
            "23428/23428 [==============================] - 0s 20us/step - loss: 0.3241 - binary_accuracy: 0.9487 - val_loss: 0.2585 - val_binary_accuracy: 0.9591\n",
            "Epoch 51/60\n",
            "23428/23428 [==============================] - 0s 21us/step - loss: 0.3307 - binary_accuracy: 0.9459 - val_loss: 0.2612 - val_binary_accuracy: 0.9574\n",
            "Epoch 52/60\n",
            "23428/23428 [==============================] - 0s 21us/step - loss: 0.3206 - binary_accuracy: 0.9487 - val_loss: 0.2454 - val_binary_accuracy: 0.9625\n",
            "Epoch 53/60\n",
            "23428/23428 [==============================] - 0s 20us/step - loss: 0.3237 - binary_accuracy: 0.9469 - val_loss: 0.2740 - val_binary_accuracy: 0.9534\n",
            "Epoch 54/60\n",
            "23428/23428 [==============================] - 1s 23us/step - loss: 0.3104 - binary_accuracy: 0.9524 - val_loss: 0.2349 - val_binary_accuracy: 0.9656\n",
            "Epoch 55/60\n",
            "23428/23428 [==============================] - 0s 20us/step - loss: 0.3208 - binary_accuracy: 0.9475 - val_loss: 0.2336 - val_binary_accuracy: 0.9672\n",
            "Epoch 56/60\n",
            "23428/23428 [==============================] - 0s 20us/step - loss: 0.3193 - binary_accuracy: 0.9469 - val_loss: 0.2811 - val_binary_accuracy: 0.9492\n",
            "Epoch 57/60\n",
            "23428/23428 [==============================] - 0s 20us/step - loss: 0.3136 - binary_accuracy: 0.9514 - val_loss: 0.2350 - val_binary_accuracy: 0.9650\n",
            "Epoch 58/60\n",
            "23428/23428 [==============================] - 0s 21us/step - loss: 0.3173 - binary_accuracy: 0.9474 - val_loss: 0.2368 - val_binary_accuracy: 0.9657\n",
            "Epoch 59/60\n",
            "23428/23428 [==============================] - 0s 20us/step - loss: 0.3019 - binary_accuracy: 0.9543 - val_loss: 0.2425 - val_binary_accuracy: 0.9622\n",
            "Epoch 60/60\n",
            "23428/23428 [==============================] - 0s 21us/step - loss: 0.3159 - binary_accuracy: 0.9503 - val_loss: 0.2274 - val_binary_accuracy: 0.9681\n",
            "23428/23428 [==============================] - 1s 42us/step\n",
            "8000/8000 [==============================] - 0s 41us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TyR4WPj1Sm0v",
        "colab_type": "text"
      },
      "source": [
        "Generamos un dataframe con las métricas por fold para luego seleccionar los mejores resultados:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ouY2E4k9Sm0w",
        "colab_type": "code",
        "outputId": "0df7ff19-fc2b-4b18-830e-1afc51223da2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        }
      },
      "source": [
        "import pandas as pd\n",
        "df = pd.DataFrame(global_history)\n",
        "df.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>fold</th>\n",
              "      <th>layers</th>\n",
              "      <th>optimizer</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>train_acc</th>\n",
              "      <th>val_loss</th>\n",
              "      <th>val_acc</th>\n",
              "      <th>history</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>[512, 10, 1]</td>\n",
              "      <td>&lt;keras.optimizers.SGD object at 0x7f535fc21320&gt;</td>\n",
              "      <td>1.011484</td>\n",
              "      <td>0.923980</td>\n",
              "      <td>1.008805</td>\n",
              "      <td>0.923250</td>\n",
              "      <td>&lt;keras.callbacks.callbacks.History object at 0...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>[512, 10, 1]</td>\n",
              "      <td>&lt;keras.optimizers.SGD object at 0x7f535fc21320&gt;</td>\n",
              "      <td>0.917846</td>\n",
              "      <td>0.925090</td>\n",
              "      <td>0.915105</td>\n",
              "      <td>0.924375</td>\n",
              "      <td>&lt;keras.callbacks.callbacks.History object at 0...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>[512, 10, 1]</td>\n",
              "      <td>&lt;keras.optimizers.SGD object at 0x7f535fc21320&gt;</td>\n",
              "      <td>0.920403</td>\n",
              "      <td>0.924364</td>\n",
              "      <td>0.917640</td>\n",
              "      <td>0.924875</td>\n",
              "      <td>&lt;keras.callbacks.callbacks.History object at 0...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>[512, 10, 1]</td>\n",
              "      <td>&lt;keras.optimizers.Adam object at 0x7f535fc21be0&gt;</td>\n",
              "      <td>0.364087</td>\n",
              "      <td>0.956761</td>\n",
              "      <td>0.364975</td>\n",
              "      <td>0.953875</td>\n",
              "      <td>&lt;keras.callbacks.callbacks.History object at 0...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>[512, 10, 1]</td>\n",
              "      <td>&lt;keras.optimizers.Adam object at 0x7f535fc21be0&gt;</td>\n",
              "      <td>0.328470</td>\n",
              "      <td>0.962865</td>\n",
              "      <td>0.330700</td>\n",
              "      <td>0.961000</td>\n",
              "      <td>&lt;keras.callbacks.callbacks.History object at 0...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   fold  ...                                            history\n",
              "0     0  ...  <keras.callbacks.callbacks.History object at 0...\n",
              "1     1  ...  <keras.callbacks.callbacks.History object at 0...\n",
              "2     2  ...  <keras.callbacks.callbacks.History object at 0...\n",
              "3     0  ...  <keras.callbacks.callbacks.History object at 0...\n",
              "4     1  ...  <keras.callbacks.callbacks.History object at 0...\n",
              "\n",
              "[5 rows x 8 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vhdl0xgzSm0z",
        "colab_type": "code",
        "outputId": "44ceda2a-254d-49c9-da25-563c24ac45ed",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 933
        }
      },
      "source": [
        "df.head(19)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>fold</th>\n",
              "      <th>layers</th>\n",
              "      <th>optimizer</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>train_acc</th>\n",
              "      <th>val_loss</th>\n",
              "      <th>val_acc</th>\n",
              "      <th>history</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>[512, 10, 1]</td>\n",
              "      <td>&lt;keras.optimizers.SGD object at 0x7f535fc21320&gt;</td>\n",
              "      <td>1.011484</td>\n",
              "      <td>0.923980</td>\n",
              "      <td>1.008805</td>\n",
              "      <td>0.923250</td>\n",
              "      <td>&lt;keras.callbacks.callbacks.History object at 0...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>[512, 10, 1]</td>\n",
              "      <td>&lt;keras.optimizers.SGD object at 0x7f535fc21320&gt;</td>\n",
              "      <td>0.917846</td>\n",
              "      <td>0.925090</td>\n",
              "      <td>0.915105</td>\n",
              "      <td>0.924375</td>\n",
              "      <td>&lt;keras.callbacks.callbacks.History object at 0...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>[512, 10, 1]</td>\n",
              "      <td>&lt;keras.optimizers.SGD object at 0x7f535fc21320&gt;</td>\n",
              "      <td>0.920403</td>\n",
              "      <td>0.924364</td>\n",
              "      <td>0.917640</td>\n",
              "      <td>0.924875</td>\n",
              "      <td>&lt;keras.callbacks.callbacks.History object at 0...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>[512, 10, 1]</td>\n",
              "      <td>&lt;keras.optimizers.Adam object at 0x7f535fc21be0&gt;</td>\n",
              "      <td>0.364087</td>\n",
              "      <td>0.956761</td>\n",
              "      <td>0.364975</td>\n",
              "      <td>0.953875</td>\n",
              "      <td>&lt;keras.callbacks.callbacks.History object at 0...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>[512, 10, 1]</td>\n",
              "      <td>&lt;keras.optimizers.Adam object at 0x7f535fc21be0&gt;</td>\n",
              "      <td>0.328470</td>\n",
              "      <td>0.962865</td>\n",
              "      <td>0.330700</td>\n",
              "      <td>0.961000</td>\n",
              "      <td>&lt;keras.callbacks.callbacks.History object at 0...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>2</td>\n",
              "      <td>[512, 10, 1]</td>\n",
              "      <td>&lt;keras.optimizers.Adam object at 0x7f535fc21be0&gt;</td>\n",
              "      <td>0.318716</td>\n",
              "      <td>0.964274</td>\n",
              "      <td>0.322141</td>\n",
              "      <td>0.961625</td>\n",
              "      <td>&lt;keras.callbacks.callbacks.History object at 0...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0</td>\n",
              "      <td>[512, 10, 1]</td>\n",
              "      <td>&lt;keras.optimizers.RMSprop object at 0x7f535fc2...</td>\n",
              "      <td>0.695128</td>\n",
              "      <td>0.918089</td>\n",
              "      <td>0.694048</td>\n",
              "      <td>0.918500</td>\n",
              "      <td>&lt;keras.callbacks.callbacks.History object at 0...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>1</td>\n",
              "      <td>[512, 10, 1]</td>\n",
              "      <td>&lt;keras.optimizers.RMSprop object at 0x7f535fc2...</td>\n",
              "      <td>0.729016</td>\n",
              "      <td>0.919498</td>\n",
              "      <td>0.728890</td>\n",
              "      <td>0.916750</td>\n",
              "      <td>&lt;keras.callbacks.callbacks.History object at 0...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>2</td>\n",
              "      <td>[512, 10, 1]</td>\n",
              "      <td>&lt;keras.optimizers.RMSprop object at 0x7f535fc2...</td>\n",
              "      <td>0.721967</td>\n",
              "      <td>0.909339</td>\n",
              "      <td>0.720383</td>\n",
              "      <td>0.907500</td>\n",
              "      <td>&lt;keras.callbacks.callbacks.History object at 0...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>0</td>\n",
              "      <td>[64, 32, 10, 1]</td>\n",
              "      <td>&lt;keras.optimizers.SGD object at 0x7f535fc21320&gt;</td>\n",
              "      <td>0.529459</td>\n",
              "      <td>0.944682</td>\n",
              "      <td>0.526551</td>\n",
              "      <td>0.947125</td>\n",
              "      <td>&lt;keras.callbacks.callbacks.History object at 0...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>1</td>\n",
              "      <td>[64, 32, 10, 1]</td>\n",
              "      <td>&lt;keras.optimizers.SGD object at 0x7f535fc21320&gt;</td>\n",
              "      <td>0.482973</td>\n",
              "      <td>0.948096</td>\n",
              "      <td>0.480278</td>\n",
              "      <td>0.950000</td>\n",
              "      <td>&lt;keras.callbacks.callbacks.History object at 0...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>2</td>\n",
              "      <td>[64, 32, 10, 1]</td>\n",
              "      <td>&lt;keras.optimizers.SGD object at 0x7f535fc21320&gt;</td>\n",
              "      <td>0.489154</td>\n",
              "      <td>0.947413</td>\n",
              "      <td>0.486529</td>\n",
              "      <td>0.949125</td>\n",
              "      <td>&lt;keras.callbacks.callbacks.History object at 0...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>0</td>\n",
              "      <td>[64, 32, 10, 1]</td>\n",
              "      <td>&lt;keras.optimizers.Adam object at 0x7f535fc21be0&gt;</td>\n",
              "      <td>0.188778</td>\n",
              "      <td>0.988006</td>\n",
              "      <td>0.224756</td>\n",
              "      <td>0.973500</td>\n",
              "      <td>&lt;keras.callbacks.callbacks.History object at 0...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>1</td>\n",
              "      <td>[64, 32, 10, 1]</td>\n",
              "      <td>&lt;keras.optimizers.Adam object at 0x7f535fc21be0&gt;</td>\n",
              "      <td>0.169433</td>\n",
              "      <td>0.987323</td>\n",
              "      <td>0.202752</td>\n",
              "      <td>0.974125</td>\n",
              "      <td>&lt;keras.callbacks.callbacks.History object at 0...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>2</td>\n",
              "      <td>[64, 32, 10, 1]</td>\n",
              "      <td>&lt;keras.optimizers.Adam object at 0x7f535fc21be0&gt;</td>\n",
              "      <td>0.167268</td>\n",
              "      <td>0.986256</td>\n",
              "      <td>0.197216</td>\n",
              "      <td>0.973125</td>\n",
              "      <td>&lt;keras.callbacks.callbacks.History object at 0...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>0</td>\n",
              "      <td>[64, 32, 10, 1]</td>\n",
              "      <td>&lt;keras.optimizers.RMSprop object at 0x7f535fc2...</td>\n",
              "      <td>0.234031</td>\n",
              "      <td>0.963932</td>\n",
              "      <td>0.242942</td>\n",
              "      <td>0.960750</td>\n",
              "      <td>&lt;keras.callbacks.callbacks.History object at 0...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>1</td>\n",
              "      <td>[64, 32, 10, 1]</td>\n",
              "      <td>&lt;keras.optimizers.RMSprop object at 0x7f535fc2...</td>\n",
              "      <td>0.234804</td>\n",
              "      <td>0.965725</td>\n",
              "      <td>0.243802</td>\n",
              "      <td>0.962750</td>\n",
              "      <td>&lt;keras.callbacks.callbacks.History object at 0...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>2</td>\n",
              "      <td>[64, 32, 10, 1]</td>\n",
              "      <td>&lt;keras.optimizers.RMSprop object at 0x7f535fc2...</td>\n",
              "      <td>0.216801</td>\n",
              "      <td>0.973621</td>\n",
              "      <td>0.227428</td>\n",
              "      <td>0.968125</td>\n",
              "      <td>&lt;keras.callbacks.callbacks.History object at 0...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    fold  ...                                            history\n",
              "0      0  ...  <keras.callbacks.callbacks.History object at 0...\n",
              "1      1  ...  <keras.callbacks.callbacks.History object at 0...\n",
              "2      2  ...  <keras.callbacks.callbacks.History object at 0...\n",
              "3      0  ...  <keras.callbacks.callbacks.History object at 0...\n",
              "4      1  ...  <keras.callbacks.callbacks.History object at 0...\n",
              "5      2  ...  <keras.callbacks.callbacks.History object at 0...\n",
              "6      0  ...  <keras.callbacks.callbacks.History object at 0...\n",
              "7      1  ...  <keras.callbacks.callbacks.History object at 0...\n",
              "8      2  ...  <keras.callbacks.callbacks.History object at 0...\n",
              "9      0  ...  <keras.callbacks.callbacks.History object at 0...\n",
              "10     1  ...  <keras.callbacks.callbacks.History object at 0...\n",
              "11     2  ...  <keras.callbacks.callbacks.History object at 0...\n",
              "12     0  ...  <keras.callbacks.callbacks.History object at 0...\n",
              "13     1  ...  <keras.callbacks.callbacks.History object at 0...\n",
              "14     2  ...  <keras.callbacks.callbacks.History object at 0...\n",
              "15     0  ...  <keras.callbacks.callbacks.History object at 0...\n",
              "16     1  ...  <keras.callbacks.callbacks.History object at 0...\n",
              "17     2  ...  <keras.callbacks.callbacks.History object at 0...\n",
              "\n",
              "[18 rows x 8 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BROYoEhVSm03",
        "colab_type": "text"
      },
      "source": [
        "Verificamos que el maximo accuracy en validacion se obtuvo en el indice 13, con optimizador Adam, y estructura de layers [64, 32, 10, 1]."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3P9g-U71Sm03",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "best_iteration = df.iloc[np.argmax(df['val_acc'])]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1DxhxtUJSm07",
        "colab_type": "code",
        "outputId": "7cb3bec0-1b56-4b1c-a08f-e222c204f7e7",
        "colab": {}
      },
      "source": [
        "best_iteration"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "fold                                                          1\n",
              "layers                                          [64, 32, 10, 1]\n",
              "optimizer     <keras.optimizers.Adam object at 0x00000180021...\n",
              "train_loss                                             0.468873\n",
              "train_acc                                              0.782098\n",
              "val_loss                                               0.461023\n",
              "val_acc                                                   0.792\n",
              "history       <keras.callbacks.callbacks.History object at 0...\n",
              "Name: 13, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QWSezGu9Sm0-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "layers = best_iteration[1]\n",
        "optimizer = best_iteration[2]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZJUMcfjHSm1E",
        "colab_type": "text"
      },
      "source": [
        "Construimos nuevamente el modelo a fines de plotear los scores en cada epoch:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T6CB1cJ9Sm1F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_model(input_shape, layers=layers, optimizer=optimizer):\n",
        "    # Instanciamos la clase del modelo secuencial\n",
        "    model = Sequential(name='Modelo de base')\n",
        "    \n",
        "    model.add(Dense(layers[0], activation='relu', input_shape=(X_train_partial_vec.shape[1],), kernel_regularizer=regularizers.l1(0.001)))\n",
        "    model.add(Dropout(0.5))\n",
        "\n",
        "    for l in layers[1:-1]:\n",
        "        model.add(Dense(units=l, activation='relu'))\n",
        "        model.add(Dropout(0.5))\n",
        "\n",
        "    # Agregamos la última capa \n",
        "    model.add(Dense(units=layers[-1], activation='sigmoid'))\n",
        "\n",
        "    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['binary_accuracy'])\n",
        "    # Retornamos el modelo compilado\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p4eXPjicSm1J",
        "colab_type": "text"
      },
      "source": [
        "En base a los parametros seleccionados, la arquitectura del modelo es la siguiente:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yZ8DGSdtSm1J",
        "colab_type": "code",
        "outputId": "d9691769-e92b-4b2a-fcb2-e6d8c7ee612b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        }
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"Modelo de base\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_18 (Dense)             (None, 64)                91072     \n",
            "_________________________________________________________________\n",
            "dropout_13 (Dropout)         (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_19 (Dense)             (None, 32)                2080      \n",
            "_________________________________________________________________\n",
            "dropout_14 (Dropout)         (None, 32)                0         \n",
            "_________________________________________________________________\n",
            "dense_20 (Dense)             (None, 10)                330       \n",
            "_________________________________________________________________\n",
            "dropout_15 (Dropout)         (None, 10)                0         \n",
            "_________________________________________________________________\n",
            "dense_21 (Dense)             (None, 1)                 11        \n",
            "=================================================================\n",
            "Total params: 93,493\n",
            "Trainable params: 93,493\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HuCE2iJcSm1N",
        "colab_type": "code",
        "outputId": "02914166-22f3-490f-d2f6-cb2e234d7f26",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model.load_weights('initial_weights.h5')\n",
        "history=model.fit(x=X_train_partial_vec,\n",
        "                  y=y_train_partial,\n",
        "                  batch_size=batch_size,\n",
        "                  epochs=epochs, \n",
        "                  validation_split=0.3,\n",
        "                  verbose=1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 16399 samples, validate on 7029 samples\n",
            "Epoch 1/60\n",
            "16399/16399 [==============================] - 0s 23us/step - loss: 3.4323 - binary_accuracy: 0.5084 - val_loss: 3.1785 - val_binary_accuracy: 0.5299\n",
            "Epoch 2/60\n",
            "16399/16399 [==============================] - 0s 23us/step - loss: 3.0295 - binary_accuracy: 0.5170 - val_loss: 2.7846 - val_binary_accuracy: 0.5301\n",
            "Epoch 3/60\n",
            "16399/16399 [==============================] - 0s 23us/step - loss: 2.6425 - binary_accuracy: 0.5230 - val_loss: 2.4095 - val_binary_accuracy: 0.5301\n",
            "Epoch 4/60\n",
            "16399/16399 [==============================] - 0s 21us/step - loss: 2.2763 - binary_accuracy: 0.5209 - val_loss: 2.0592 - val_binary_accuracy: 0.5301\n",
            "Epoch 5/60\n",
            "16399/16399 [==============================] - 0s 23us/step - loss: 1.9377 - binary_accuracy: 0.5207 - val_loss: 1.7398 - val_binary_accuracy: 0.5301\n",
            "Epoch 6/60\n",
            "16399/16399 [==============================] - 0s 22us/step - loss: 1.6326 - binary_accuracy: 0.5229 - val_loss: 1.4572 - val_binary_accuracy: 0.5301\n",
            "Epoch 7/60\n",
            "16399/16399 [==============================] - 0s 21us/step - loss: 1.3664 - binary_accuracy: 0.5253 - val_loss: 1.2152 - val_binary_accuracy: 0.5311\n",
            "Epoch 8/60\n",
            "16399/16399 [==============================] - 0s 22us/step - loss: 1.1408 - binary_accuracy: 0.5395 - val_loss: 1.0150 - val_binary_accuracy: 0.5560\n",
            "Epoch 9/60\n",
            "16399/16399 [==============================] - 0s 22us/step - loss: 0.9612 - binary_accuracy: 0.5829 - val_loss: 0.8609 - val_binary_accuracy: 0.5904\n",
            "Epoch 10/60\n",
            "16399/16399 [==============================] - 0s 23us/step - loss: 0.8298 - binary_accuracy: 0.6282 - val_loss: 0.7580 - val_binary_accuracy: 0.6263\n",
            "Epoch 11/60\n",
            "16399/16399 [==============================] - 0s 22us/step - loss: 0.7520 - binary_accuracy: 0.6790 - val_loss: 0.7017 - val_binary_accuracy: 0.6553\n",
            "Epoch 12/60\n",
            "16399/16399 [==============================] - 0s 21us/step - loss: 0.7053 - binary_accuracy: 0.7178 - val_loss: 0.6647 - val_binary_accuracy: 0.7125\n",
            "Epoch 13/60\n",
            "16399/16399 [==============================] - 0s 21us/step - loss: 0.6810 - binary_accuracy: 0.7452 - val_loss: 0.6430 - val_binary_accuracy: 0.7213\n",
            "Epoch 14/60\n",
            "16399/16399 [==============================] - 0s 21us/step - loss: 0.6618 - binary_accuracy: 0.7666 - val_loss: 0.6217 - val_binary_accuracy: 0.7608\n",
            "Epoch 15/60\n",
            "16399/16399 [==============================] - 0s 22us/step - loss: 0.6473 - binary_accuracy: 0.7924 - val_loss: 0.5986 - val_binary_accuracy: 0.8136\n",
            "Epoch 16/60\n",
            "16399/16399 [==============================] - 0s 22us/step - loss: 0.6320 - binary_accuracy: 0.8146 - val_loss: 0.5851 - val_binary_accuracy: 0.8088\n",
            "Epoch 17/60\n",
            "16399/16399 [==============================] - 0s 22us/step - loss: 0.6180 - binary_accuracy: 0.8265 - val_loss: 0.5685 - val_binary_accuracy: 0.8239\n",
            "Epoch 18/60\n",
            "16399/16399 [==============================] - 0s 22us/step - loss: 0.6038 - binary_accuracy: 0.8432 - val_loss: 0.5447 - val_binary_accuracy: 0.8728\n",
            "Epoch 19/60\n",
            "16399/16399 [==============================] - 0s 21us/step - loss: 0.5844 - binary_accuracy: 0.8547 - val_loss: 0.5261 - val_binary_accuracy: 0.8966\n",
            "Epoch 20/60\n",
            "16399/16399 [==============================] - 0s 22us/step - loss: 0.5709 - binary_accuracy: 0.8705 - val_loss: 0.5096 - val_binary_accuracy: 0.8969\n",
            "Epoch 21/60\n",
            "16399/16399 [==============================] - 0s 21us/step - loss: 0.5598 - binary_accuracy: 0.8746 - val_loss: 0.4945 - val_binary_accuracy: 0.8924\n",
            "Epoch 22/60\n",
            "16399/16399 [==============================] - 0s 21us/step - loss: 0.5465 - binary_accuracy: 0.8800 - val_loss: 0.4728 - val_binary_accuracy: 0.9154\n",
            "Epoch 23/60\n",
            "16399/16399 [==============================] - 0s 22us/step - loss: 0.5277 - binary_accuracy: 0.8880 - val_loss: 0.4610 - val_binary_accuracy: 0.9097\n",
            "Epoch 24/60\n",
            "16399/16399 [==============================] - 0s 21us/step - loss: 0.5183 - binary_accuracy: 0.8918 - val_loss: 0.4336 - val_binary_accuracy: 0.9420\n",
            "Epoch 25/60\n",
            "16399/16399 [==============================] - 0s 22us/step - loss: 0.4937 - binary_accuracy: 0.9060 - val_loss: 0.4359 - val_binary_accuracy: 0.9146\n",
            "Epoch 26/60\n",
            "16399/16399 [==============================] - 0s 22us/step - loss: 0.4958 - binary_accuracy: 0.8971 - val_loss: 0.4100 - val_binary_accuracy: 0.9397\n",
            "Epoch 27/60\n",
            "16399/16399 [==============================] - 0s 21us/step - loss: 0.4713 - binary_accuracy: 0.9112 - val_loss: 0.4054 - val_binary_accuracy: 0.9313\n",
            "Epoch 28/60\n",
            "16399/16399 [==============================] - 0s 21us/step - loss: 0.4687 - binary_accuracy: 0.9092 - val_loss: 0.3938 - val_binary_accuracy: 0.9357\n",
            "Epoch 29/60\n",
            "16399/16399 [==============================] - 0s 22us/step - loss: 0.4590 - binary_accuracy: 0.9088 - val_loss: 0.3693 - val_binary_accuracy: 0.9489\n",
            "Epoch 30/60\n",
            "16399/16399 [==============================] - 0s 21us/step - loss: 0.4381 - binary_accuracy: 0.9217 - val_loss: 0.3752 - val_binary_accuracy: 0.9374\n",
            "Epoch 31/60\n",
            "16399/16399 [==============================] - 0s 21us/step - loss: 0.4458 - binary_accuracy: 0.9130 - val_loss: 0.3609 - val_binary_accuracy: 0.9448\n",
            "Epoch 32/60\n",
            "16399/16399 [==============================] - 0s 22us/step - loss: 0.4341 - binary_accuracy: 0.9183 - val_loss: 0.3741 - val_binary_accuracy: 0.9328\n",
            "Epoch 33/60\n",
            "16399/16399 [==============================] - 0s 22us/step - loss: 0.4249 - binary_accuracy: 0.9208 - val_loss: 0.3310 - val_binary_accuracy: 0.9576\n",
            "Epoch 34/60\n",
            "16399/16399 [==============================] - 0s 21us/step - loss: 0.4112 - binary_accuracy: 0.9248 - val_loss: 0.3444 - val_binary_accuracy: 0.9422\n",
            "Epoch 35/60\n",
            "16399/16399 [==============================] - 0s 21us/step - loss: 0.4182 - binary_accuracy: 0.9198 - val_loss: 0.3313 - val_binary_accuracy: 0.9486\n",
            "Epoch 36/60\n",
            "16399/16399 [==============================] - 0s 22us/step - loss: 0.4047 - binary_accuracy: 0.9224 - val_loss: 0.3252 - val_binary_accuracy: 0.9515\n",
            "Epoch 37/60\n",
            "16399/16399 [==============================] - 0s 21us/step - loss: 0.4140 - binary_accuracy: 0.9225 - val_loss: 0.3244 - val_binary_accuracy: 0.9475\n",
            "Epoch 38/60\n",
            "16399/16399 [==============================] - 0s 22us/step - loss: 0.3907 - binary_accuracy: 0.9289 - val_loss: 0.3080 - val_binary_accuracy: 0.9558\n",
            "Epoch 39/60\n",
            "16399/16399 [==============================] - 0s 21us/step - loss: 0.4029 - binary_accuracy: 0.9226 - val_loss: 0.3172 - val_binary_accuracy: 0.9494\n",
            "Epoch 40/60\n",
            "16399/16399 [==============================] - 0s 22us/step - loss: 0.3857 - binary_accuracy: 0.9310 - val_loss: 0.3196 - val_binary_accuracy: 0.9431\n",
            "Epoch 41/60\n",
            "16399/16399 [==============================] - 0s 23us/step - loss: 0.3929 - binary_accuracy: 0.9261 - val_loss: 0.3005 - val_binary_accuracy: 0.9555\n",
            "Epoch 42/60\n",
            "16399/16399 [==============================] - 0s 22us/step - loss: 0.3728 - binary_accuracy: 0.9366 - val_loss: 0.2972 - val_binary_accuracy: 0.9535\n",
            "Epoch 43/60\n",
            "16399/16399 [==============================] - 0s 24us/step - loss: 0.3669 - binary_accuracy: 0.9343 - val_loss: 0.3110 - val_binary_accuracy: 0.9458\n",
            "Epoch 44/60\n",
            "16399/16399 [==============================] - 0s 23us/step - loss: 0.3924 - binary_accuracy: 0.9251 - val_loss: 0.3023 - val_binary_accuracy: 0.9528\n",
            "Epoch 45/60\n",
            "16399/16399 [==============================] - 0s 22us/step - loss: 0.3652 - binary_accuracy: 0.9375 - val_loss: 0.2871 - val_binary_accuracy: 0.9583\n",
            "Epoch 46/60\n",
            "16399/16399 [==============================] - 0s 23us/step - loss: 0.3796 - binary_accuracy: 0.9302 - val_loss: 0.3035 - val_binary_accuracy: 0.9494\n",
            "Epoch 47/60\n",
            "16399/16399 [==============================] - 0s 23us/step - loss: 0.3638 - binary_accuracy: 0.9395 - val_loss: 0.2857 - val_binary_accuracy: 0.9565\n",
            "Epoch 48/60\n",
            "16399/16399 [==============================] - 0s 23us/step - loss: 0.3619 - binary_accuracy: 0.9380 - val_loss: 0.3021 - val_binary_accuracy: 0.9484\n",
            "Epoch 49/60\n",
            "16399/16399 [==============================] - 0s 23us/step - loss: 0.3567 - binary_accuracy: 0.9430 - val_loss: 0.2804 - val_binary_accuracy: 0.9586\n",
            "Epoch 50/60\n",
            "16399/16399 [==============================] - 0s 22us/step - loss: 0.3508 - binary_accuracy: 0.9406 - val_loss: 0.3222 - val_binary_accuracy: 0.9383\n",
            "Epoch 51/60\n",
            "16399/16399 [==============================] - 0s 22us/step - loss: 0.3667 - binary_accuracy: 0.9366 - val_loss: 0.2684 - val_binary_accuracy: 0.9627\n",
            "Epoch 52/60\n",
            "16399/16399 [==============================] - 0s 22us/step - loss: 0.3354 - binary_accuracy: 0.9474 - val_loss: 0.2755 - val_binary_accuracy: 0.9572\n",
            "Epoch 53/60\n",
            "16399/16399 [==============================] - 0s 22us/step - loss: 0.3713 - binary_accuracy: 0.9342 - val_loss: 0.2665 - val_binary_accuracy: 0.9617\n",
            "Epoch 54/60\n",
            "16399/16399 [==============================] - 0s 23us/step - loss: 0.3379 - binary_accuracy: 0.9450 - val_loss: 0.2785 - val_binary_accuracy: 0.9575\n",
            "Epoch 55/60\n",
            "16399/16399 [==============================] - 0s 22us/step - loss: 0.3563 - binary_accuracy: 0.9374 - val_loss: 0.2891 - val_binary_accuracy: 0.9538\n",
            "Epoch 56/60\n",
            "16399/16399 [==============================] - 0s 22us/step - loss: 0.3424 - binary_accuracy: 0.9449 - val_loss: 0.2698 - val_binary_accuracy: 0.9596\n",
            "Epoch 57/60\n",
            "16399/16399 [==============================] - 0s 22us/step - loss: 0.3284 - binary_accuracy: 0.9507 - val_loss: 0.2650 - val_binary_accuracy: 0.9612\n",
            "Epoch 58/60\n",
            "16399/16399 [==============================] - 0s 22us/step - loss: 0.3694 - binary_accuracy: 0.9314 - val_loss: 0.2851 - val_binary_accuracy: 0.9576\n",
            "Epoch 59/60\n",
            "16399/16399 [==============================] - 0s 21us/step - loss: 0.3269 - binary_accuracy: 0.9501 - val_loss: 0.2605 - val_binary_accuracy: 0.9616\n",
            "Epoch 60/60\n",
            "16399/16399 [==============================] - 0s 22us/step - loss: 0.3332 - binary_accuracy: 0.9470 - val_loss: 0.2904 - val_binary_accuracy: 0.9498\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U3o9L5d7Sm1Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def plot_history(name, history, legend, plot_val=True):\n",
        "    fig, ax = plt.subplots(1,2,figsize=(14,6))\n",
        "    fig.suptitle(name)\n",
        "    \n",
        "    if not isinstance(history, list):\n",
        "        history = [history]\n",
        "        \n",
        "    for h in history:\n",
        "        acc = h.history['binary_accuracy']\n",
        "        loss = h.history['loss']\n",
        "        if plot_val:\n",
        "            val_loss = h.history['val_loss']\n",
        "            val_acc = h.history['val_binary_accuracy']\n",
        "        epochs = range(1, len(acc) + 1)\n",
        "\n",
        "        ax[0].set_title('Loss')\n",
        "        ax[0].set_xticks(ticks=epochs)\n",
        "        lb, ub = ax[0].get_xlim( )\n",
        "        ax[0].set_xticks( np.linspace(lb, ub, 6 ) )\n",
        "        ax[0].set_ylim([0, 1])\n",
        "        ax[0].set_yticks( np.linspace(0, 1, 6 ) )\n",
        "        ax[0].set_ylabel('Loss')\n",
        "        \n",
        "        ax[0].plot(epochs, loss)\n",
        "        if plot_val:\n",
        "            ax[0].plot(epochs, val_loss)\n",
        "            \n",
        "        ax[1].set_title('Accuracy')\n",
        "        ax[1].set_xticks(ticks=list(epochs))\n",
        "        ax[1].set_xlabel('Epochs')\n",
        "        lb, ub = ax[1].get_xlim( )\n",
        "        ax[1].set_xticks( np.linspace(lb, ub, 6 ) )\n",
        "        ax[1].set_ylim([0, 1])\n",
        "        ax[1].set_yticks( np.linspace(0, 1, 6 ) )\n",
        "        ax[1].set_ylabel('Accuracy')\n",
        "        ax[1].plot(epochs, acc)\n",
        "        \n",
        "        if plot_val:\n",
        "            ax[1].plot(epochs, val_acc)\n",
        "        \n",
        "    ax[0].legend([l+' loss' for l in legend])\n",
        "    ax[1].legend([l+' accuracy' for l in legend])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h-vIP2XdSm1U",
        "colab_type": "code",
        "outputId": "6536f173-0f72-4068-d71b-03b4f219438a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 431
        }
      },
      "source": [
        "plot_history('Scores Modelo', history, ['train', 'val'])\n",
        "plt.savefig('Scores Red.jpg', dpi=300)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1008x432 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rhLv6eFQSm1Y",
        "colab_type": "text"
      },
      "source": [
        "En base a las predicciones ploteamos el accuracy obtenido:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bzqNvIPkSm1Y",
        "colab_type": "code",
        "outputId": "74e12bab-1d27-4cb4-eb65-ebd4f40f6ba0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "y_pred =model.predict_classes(X_test_vec)\n",
        "\n",
        "print(\"Accuracy sobre Red: {}\".format(accuracy_score(y_test, y_pred.round())))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy sobre Red: 0.9503340757238308\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6eNChZzTSm1c",
        "colab_type": "code",
        "outputId": "cd2bdf13-ae2e-41d1-85fe-d5662674193a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 618
        }
      },
      "source": [
        "from sklearn.utils.multiclass import unique_labels\n",
        "from sklearn.metrics import confusion_matrix\n",
        "plt.figure(figsize=(10, 10))\n",
        "classes = unique_labels(y_test, y_pred)\n",
        "plt.title(\"confusion matrix Red\")\n",
        "\n",
        "mat = confusion_matrix(y_test, y_pred)\n",
        "ax=sns.heatmap(mat, square=True, annot=True, fmt='d', cbar=False)\n",
        "sns.set(font_scale=2) \n",
        "\n",
        "plt.xlabel('Etiquetas predichas')\n",
        "plt.ylabel('Etiquetas verdaderas')\n",
        "ax.set_xticklabels(classes,rotation=90)\n",
        "ax.set_yticklabels(classes,rotation=0);\n",
        "\n",
        "plt.savefig('Matriz Confusion Red.jpg', dpi=300)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x720 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1hBexxOxAziq",
        "colab_type": "text"
      },
      "source": [
        "# **7. CONCLUSIONES**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3tn3Riqz_GEK",
        "colab_type": "text"
      },
      "source": [
        "Los modelos Benchmark alcanzaron una muy alta performance con poco esfuerzo. Lejos de alegrarnos por el resultado, además de realizar una nueva revisión sobre los datos, se desdobló el análisis sobre dos de los campos del dataset : Título y Cuerpo de la Noticia\n",
        "\n",
        "Sobre esta decisión, el manejo de los datos sólo de Title genera una pobre disponibilidad de features que atenta directemente sobre el accuracy del modelo. No recomendamos ese camino de trabajo\n",
        "\n",
        "Para el análisis de texto de la noticia: En el proceso recurrente se detectaron las anomalías relacionadas con filtración de información del target dentro de los features: \n",
        "* Concentración de fake News en 2 de las 7 secciones\n",
        "* Noticias verdaderas con información sobre la agencia de noticias de origen.\n",
        "\n",
        "Redes: el análisis del dataset y del manejo de los datos, pudimos ver que el algoritmo cuenta con un 94% de accuracy final  en Train  (habiendo hecho los splits de validación, test y crossvalidation) y un 95% de accuracy en Test y se comporta de manera estable en los primeros 60 epochs. \n",
        "\n",
        "La fortaleza de predicción no radica en la complejidad del algoritmo ya que había tenido performances similares en arquitecturas más simples con menor demanda computacional.\n",
        "\n",
        "Encuentra dificultades con los falsos positivos, en nuestro caso, califica como falsas noticias que son en esencia verdaderas.  El Modelo Naive Bayes, por el contrario, tenía debilidad en los falsos negativos.\n",
        "\n",
        "Los árboles de decisión eran muy similares ambos tipos de errores. \n",
        "\n",
        "Nuestra recomendación, por las características del problema, es mantener especial énfasis en el indicador de precisión además del accuracy, por lo que recomendamos la Red Neuronal Optimizada.\n"
      ]
    }
  ]
}