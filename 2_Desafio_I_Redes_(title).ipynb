{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "2 - Desafio I - Redes (title).ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alcoleasnoelia/Proyecto1-IA-DigitalHouse-Grupo4/blob/master/2_Desafio_I_Redes_(title).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "J7aABcEU3fjj"
      },
      "source": [
        "(Continuación de la notebook anterior)\n",
        "Notebook 3\n",
        "    \n",
        "6.  Modelado Avanzado Neural Networks (LSTMs)\n",
        "    *   Red Secuencial Inicial\n",
        "    *   Red Secuencial con regularizaciómn \n",
        "    *   Modelado de Red con Cross Validation\n",
        "    *   Análisis final de performance\n",
        "7.  Conclusiones\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "VzsMDcdC3fjr",
        "colab": {},
        "outputId": "fbaaa6f5-91a4-46e4-a0b7-2bb8e43fc116"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import pickle\n",
        "import re\n",
        "\n",
        "#Librerias para generación de features\n",
        "import string\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "#Librerias para construcción de red\n",
        "from keras import losses\n",
        "from keras import metrics\n",
        "from keras import optimizers\n",
        "from keras import models, layers\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.callbacks import EarlyStopping\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import Embedding\n",
        "from keras import regularizers"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "E7MD-Qvy3fkB",
        "colab": {}
      },
      "source": [
        "def correr_stemming(text):\n",
        "    # Stemming\n",
        "    text = text.split()\n",
        "    stemmer = SnowballStemmer('english')\n",
        "    stemmed_words = [stemmer.stem(word) for word in text]\n",
        "    text = \" \".join(stemmed_words)\n",
        "    return(text)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "-i3Tilcfb3Ub"
      },
      "source": [
        "Cargamos el dataset a partir del pickle generado por la notebook de limpieza:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "oEL723aFb3Uc",
        "colab": {}
      },
      "source": [
        "news = pd.read_pickle(\"news_.p\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "LS9zw5BM5PI_",
        "outputId": "3b0cbf2e-1fd2-44a9-b0db-c672ed31557b",
        "colab": {}
      },
      "source": [
        "news.head(5)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>title</th>\n",
              "      <th>text</th>\n",
              "      <th>subject</th>\n",
              "      <th>date</th>\n",
              "      <th>text_st</th>\n",
              "      <th>fakenews_</th>\n",
              "      <th>title_clean</th>\n",
              "      <th>text_clean</th>\n",
              "      <th>text_polarity</th>\n",
              "      <th>text_subjectivity</th>\n",
              "      <th>title_polarity</th>\n",
              "      <th>title_subjectivity</th>\n",
              "      <th>text_clean_polarity</th>\n",
              "      <th>text_clean_subjectivity</th>\n",
              "      <th>title_clean_polarity</th>\n",
              "      <th>title_clean_subjectivity</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>As U.S. budget fight looms, Republicans flip t...</td>\n",
              "      <td>WASHINGTON (Reuters) - The head of a conservat...</td>\n",
              "      <td>politicsNews</td>\n",
              "      <td>December 31, 2017</td>\n",
              "      <td>washington (reuters) - the head of a conserv r...</td>\n",
              "      <td>0</td>\n",
              "      <td>us budget fight looms republicans flip fiscal ...</td>\n",
              "      <td>head conservative republican faction us congre...</td>\n",
              "      <td>0.037083</td>\n",
              "      <td>0.410250</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.036979</td>\n",
              "      <td>0.403438</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>U.S. military to accept transgender recruits o...</td>\n",
              "      <td>WASHINGTON (Reuters) - Transgender people will...</td>\n",
              "      <td>politicsNews</td>\n",
              "      <td>December 29, 2017</td>\n",
              "      <td>washington (reuters) - transgend peopl will be...</td>\n",
              "      <td>0</td>\n",
              "      <td>us military accept transgender recruits monday...</td>\n",
              "      <td>transgender people allowed first time enlist u...</td>\n",
              "      <td>0.055880</td>\n",
              "      <td>0.298557</td>\n",
              "      <td>-0.10</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.113095</td>\n",
              "      <td>0.296168</td>\n",
              "      <td>-0.1</td>\n",
              "      <td>0.10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>Senior U.S. Republican senator: 'Let Mr. Muell...</td>\n",
              "      <td>WASHINGTON (Reuters) - The special counsel inv...</td>\n",
              "      <td>politicsNews</td>\n",
              "      <td>December 31, 2017</td>\n",
              "      <td>washington (reuters) - the special counsel inv...</td>\n",
              "      <td>0</td>\n",
              "      <td>senior us republican senator let mr mueller job</td>\n",
              "      <td>special counsel investigation links russia pre...</td>\n",
              "      <td>0.115930</td>\n",
              "      <td>0.316798</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.129766</td>\n",
              "      <td>0.311252</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>FBI Russia probe helped by Australian diplomat...</td>\n",
              "      <td>WASHINGTON (Reuters) - Trump campaign adviser ...</td>\n",
              "      <td>politicsNews</td>\n",
              "      <td>December 30, 2017</td>\n",
              "      <td>washington (reuters) - trump campaign advis ge...</td>\n",
              "      <td>0</td>\n",
              "      <td>fbi russia probe helped australian diplomat ti...</td>\n",
              "      <td>trump campaign adviser george papadopoulos tol...</td>\n",
              "      <td>0.035968</td>\n",
              "      <td>0.306569</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.030305</td>\n",
              "      <td>0.276323</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>Trump wants Postal Service to charge 'much mor...</td>\n",
              "      <td>SEATTLE/WASHINGTON (Reuters) - President Donal...</td>\n",
              "      <td>politicsNews</td>\n",
              "      <td>December 29, 2017</td>\n",
              "      <td>seattle/washington (reuters) - presid donald t...</td>\n",
              "      <td>0</td>\n",
              "      <td>trump wants postal service charge much amazon ...</td>\n",
              "      <td>president donald trump called us postal servic...</td>\n",
              "      <td>0.030093</td>\n",
              "      <td>0.399891</td>\n",
              "      <td>0.35</td>\n",
              "      <td>0.3</td>\n",
              "      <td>0.042130</td>\n",
              "      <td>0.379259</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.15</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                               title  \\\n",
              "0  As U.S. budget fight looms, Republicans flip t...   \n",
              "1  U.S. military to accept transgender recruits o...   \n",
              "2  Senior U.S. Republican senator: 'Let Mr. Muell...   \n",
              "3  FBI Russia probe helped by Australian diplomat...   \n",
              "4  Trump wants Postal Service to charge 'much mor...   \n",
              "\n",
              "                                                text       subject  \\\n",
              "0  WASHINGTON (Reuters) - The head of a conservat...  politicsNews   \n",
              "1  WASHINGTON (Reuters) - Transgender people will...  politicsNews   \n",
              "2  WASHINGTON (Reuters) - The special counsel inv...  politicsNews   \n",
              "3  WASHINGTON (Reuters) - Trump campaign adviser ...  politicsNews   \n",
              "4  SEATTLE/WASHINGTON (Reuters) - President Donal...  politicsNews   \n",
              "\n",
              "                 date                                            text_st  \\\n",
              "0  December 31, 2017   washington (reuters) - the head of a conserv r...   \n",
              "1  December 29, 2017   washington (reuters) - transgend peopl will be...   \n",
              "2  December 31, 2017   washington (reuters) - the special counsel inv...   \n",
              "3  December 30, 2017   washington (reuters) - trump campaign advis ge...   \n",
              "4  December 29, 2017   seattle/washington (reuters) - presid donald t...   \n",
              "\n",
              "   fakenews_                                        title_clean  \\\n",
              "0          0  us budget fight looms republicans flip fiscal ...   \n",
              "1          0  us military accept transgender recruits monday...   \n",
              "2          0    senior us republican senator let mr mueller job   \n",
              "3          0  fbi russia probe helped australian diplomat ti...   \n",
              "4          0  trump wants postal service charge much amazon ...   \n",
              "\n",
              "                                          text_clean  text_polarity  \\\n",
              "0  head conservative republican faction us congre...       0.037083   \n",
              "1  transgender people allowed first time enlist u...       0.055880   \n",
              "2  special counsel investigation links russia pre...       0.115930   \n",
              "3  trump campaign adviser george papadopoulos tol...       0.035968   \n",
              "4  president donald trump called us postal servic...       0.030093   \n",
              "\n",
              "   text_subjectivity  title_polarity  title_subjectivity  text_clean_polarity  \\\n",
              "0           0.410250            0.00                 0.0             0.036979   \n",
              "1           0.298557           -0.10                 0.1             0.113095   \n",
              "2           0.316798            0.00                 0.0             0.129766   \n",
              "3           0.306569            0.00                 0.0             0.030305   \n",
              "4           0.399891            0.35                 0.3             0.042130   \n",
              "\n",
              "   text_clean_subjectivity  title_clean_polarity  title_clean_subjectivity  \n",
              "0                 0.403438                   0.0                      0.00  \n",
              "1                 0.296168                  -0.1                      0.10  \n",
              "2                 0.311252                   0.0                      0.00  \n",
              "3                 0.276323                   0.0                      0.00  \n",
              "4                 0.379259                   0.2                      0.15  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ectm8dRY3fk1"
      },
      "source": [
        "Dividimos el dataset en train y test con shuffle y estratificación:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "F03PUTvf3fk3",
        "outputId": "23a4502e-ad49-424a-bed1-752cdac5216b",
        "colab": {}
      },
      "source": [
        "data_train, data_test = train_test_split(news,test_size=0.30,shuffle=True,random_state=10, stratify=news[\"fakenews_\"])\n",
        "\n",
        "data_train = data_train.copy()\n",
        "data_test = data_test.copy() \n",
        "print(\"data train shape: {}\".format(data_train.shape))\n",
        "print(\"data test shape: {}\".format(data_test.shape))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "data train shape: (31428, 16)\n",
            "data test shape: (13470, 16)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "0n9AJ0-q3flA",
        "colab": {}
      },
      "source": [
        "X_train=data_train[\"title_clean\"].apply(correr_stemming)\n",
        "X_test=data_test[\"title_clean\"].apply(correr_stemming)\n",
        "y_train=data_train[\"fakenews_\"].copy()\n",
        "y_test=data_test[\"fakenews_\"].copy()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2mEyharQYhVq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train_partial=X_train[8000:]\n",
        "X_val=X_train[:8000]\n",
        "y_val=y_train[:8000]\n",
        "y_train_partial=y_train[8000:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GxrSaBDlYhWE",
        "colab_type": "code",
        "colab": {},
        "outputId": "1cd0e83c-be89-4010-c786-76edea5dc271"
      },
      "source": [
        "X_train_partial.shape, X_val.shape, X_test.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((23428,), (8000,), (13470,))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "6SnleUXtb3Ug"
      },
      "source": [
        "# **4.   Feature Extraction**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oGq6IsrdYhWV",
        "colab_type": "text"
      },
      "source": [
        "A partir de una vectorizacion por TFIDF obtenemos 10.000 features principales:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bFZYPxQyYhWY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tfidf_vec= TfidfVectorizer(stop_words='english', max_df=0.8, min_df=0.02)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zXm7U_yKYhWq",
        "colab_type": "code",
        "colab": {},
        "outputId": "0de7f36a-a2aa-4ebd-f41d-795205d04647"
      },
      "source": [
        "X_train_partial_vec=tfidf_vec.fit_transform(X_train_partial).copy()\n",
        "X_val_vec=tfidf_vec.transform(X_val).copy()\n",
        "X_test_vec=tfidf_vec.transform(X_test).copy()\n",
        "X_train_partial_vec.shape, X_val_vec.shape, X_test_vec.shape, y_test.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((23428, 31), (8000, 31), (13470, 31), (13470,))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1FGNzXriYhXI",
        "colab_type": "code",
        "colab": {},
        "outputId": "2f9a77fa-d435-43e9-e970-611c4c0a18ab"
      },
      "source": [
        "(X_train_partial_vec.shape[1],)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(31,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "t42KtQGf3flv"
      },
      "source": [
        "# **6.   Modelado Avanzado Neural Networks (LSTMs)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uhgEdwn3YhXT",
        "colab_type": "text"
      },
      "source": [
        "Definimos una función para la construcción del modelo de capas densas:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "p5hS8v623fm_",
        "colab": {}
      },
      "source": [
        "def build_model(input_shape, layers=[15,10,1], optimizer='rmsprop'):\n",
        "    # Instanciamos la clase del modelo secuencial\n",
        "    model = Sequential(name='Modelo de base')\n",
        "    \n",
        "    #Configuramos la primera capa para que tome las features, con regularizacion l1\n",
        "    model.add(Dense(layers[0], activation='relu', input_shape=(X_train_partial_vec.shape[1],), kernel_regularizer=regularizers.l1(0.001)))\n",
        "    model.add(Dropout(0.5))\n",
        "\n",
        "    for l in layers[1:-1]:\n",
        "        model.add(Dense(units=l, activation='relu'))\n",
        "        model.add(Dropout(0.5))\n",
        "\n",
        "    # Agregamos la última capa \n",
        "    model.add(Dense(units=layers[-1], activation='sigmoid'))\n",
        "\n",
        "    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['binary_accuracy'])\n",
        "    # Retornamos el modelo compilado\n",
        "    return model\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XWgF-0IJYhai",
        "colab_type": "text"
      },
      "source": [
        "Realizamos una busqueda por iteraciones para encontrar los mejores hiperparametros "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ucFB_eEr3fnB",
        "colab": {}
      },
      "source": [
        "from keras import optimizers"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZL2WyMxdYhao",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Definimos la \"grilla\" de parámetros que vamos a explorar\n",
        "layers = [[512, 10, 1], [64, 32,10, 1]]\n",
        "optimizers = [ optimizers.SGD(momentum=0.9, nesterov=True), optimizers.Adam(), optimizers.RMSprop()]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JEBisJwfYhas",
        "colab_type": "code",
        "colab": {},
        "outputId": "355debb2-7e08-4673-f87a-27f00e0e93f0"
      },
      "source": [
        "import itertools\n",
        "combinaciones = list(itertools.product(layers, optimizers))\n",
        "combinaciones"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[([512, 10, 1], <keras.optimizers.SGD at 0x180021b5cc8>),\n",
              " ([512, 10, 1], <keras.optimizers.Adam at 0x180021b5b88>),\n",
              " ([512, 10, 1], <keras.optimizers.RMSprop at 0x180021b5c48>),\n",
              " ([64, 32, 10, 1], <keras.optimizers.SGD at 0x180021b5cc8>),\n",
              " ([64, 32, 10, 1], <keras.optimizers.Adam at 0x180021b5b88>),\n",
              " ([64, 32, 10, 1], <keras.optimizers.RMSprop at 0x180021b5c48>)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CFodGvaBYhbC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Definamos algunas variables\n",
        "n_splits = 3\n",
        "batch_size = 3000\n",
        "epochs = 60\n",
        "verbose = 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MqXd46jlYhbI",
        "colab_type": "text"
      },
      "source": [
        "Creamos una lista vacía para ir guardando los entrenamientos y luego ejecutamos la función de cross validation para la selección del modelo con mejor arquitectura y optimizadores según su accuracy:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X4KrIgdxYhbJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "global_history = []"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VYWDLpSOYhbP",
        "colab_type": "code",
        "colab": {},
        "outputId": "ecc412c3-5dc3-42f9-9a53-ac1ad1720675"
      },
      "source": [
        "# Importamos KFold para hacer cross-validation\n",
        "from sklearn.model_selection import KFold\n",
        "\n",
        "# Instanciamos el objeto KFold\n",
        "kfold = KFold(n_splits=n_splits, shuffle=False)\n",
        "\n",
        "# Recorremos las combinaciones y generamos distintos modelos a ensayar\n",
        "for (layers, optimizer) in combinaciones:\n",
        "    print('\\n\\nEnsayando modelo con estructura {} y optimizador {}'.format(layers, optimizer))\n",
        "    \n",
        "    # Construimos el modelo\n",
        "    \n",
        "    model = build_model(input_shape=X_train_partial_vec.shape, layers=layers, optimizer=optimizer)\n",
        "    \n",
        "    # Guardamos los pesos iniciales para usarlos en cada fold\n",
        "    model.save_weights('initial_weights.h5')\n",
        "    \n",
        "    # Generamos los sets de train y val para ensayar el modelo\n",
        "    for fold, (train_idx, val_idx) in enumerate(kfold.split(X_train_partial_vec)):\n",
        "        \n",
        "        # Reiniciamos los pesos del modelo\n",
        "        model.load_weights('initial_weights.h5')\n",
        "        \n",
        "        print(X_train_partial_vec.shape, X_val_vec.shape, X_test_vec.shape, y_train_partial.shape, y_val.shape, y_test.shape)       \n",
        "        \n",
        "        \n",
        "        # Lo entrenamos con el split de x_train e y_train correspondiente\n",
        "        history = model.fit(x=X_train_partial_vec,\n",
        "                            y=y_train_partial,\n",
        "                            batch_size=batch_size,\n",
        "                            epochs=epochs,\n",
        "                            validation_data=(X_val_vec, y_val),\n",
        "                            verbose=verbose\n",
        "                           )\n",
        "        \n",
        "        \n",
        "        \n",
        "        # Evaluamos en train y en val (estos mismos valores los podemos sacar de history)\n",
        "        train_loss, train_acc = model.evaluate(X_train_partial_vec, y_train_partial)\n",
        "        val_loss, val_acc = model.evaluate(X_val_vec, y_val)\n",
        "        \n",
        "        # Agregamos esta corrida a la historia global\n",
        "        global_history.append({'fold':fold, \n",
        "                               'layers':layers,\n",
        "                               'optimizer':optimizer,\n",
        "                               'train_loss':train_loss,\n",
        "                               'train_acc':train_acc,\n",
        "                               'val_loss':val_loss,\n",
        "                               'val_acc':val_acc,\n",
        "                               'history':history\n",
        "                              })"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "Ensayando modelo con estructura [512, 10, 1] y optimizador <keras.optimizers.SGD object at 0x00000180021B5CC8>\n",
            "(23428, 31) (8000, 31) (13470, 31) (23428,) (8000,) (13470,)\n",
            "Train on 23428 samples, validate on 8000 samples\n",
            "Epoch 1/60\n",
            "23428/23428 [==============================] - 1s 60us/step - loss: 1.5164 - binary_accuracy: 0.5321 - val_loss: 1.5113 - val_binary_accuracy: 0.5191\n",
            "Epoch 2/60\n",
            "23428/23428 [==============================] - 0s 11us/step - loss: 1.5072 - binary_accuracy: 0.5266 - val_loss: 1.4982 - val_binary_accuracy: 0.5191\n",
            "Epoch 3/60\n",
            "23428/23428 [==============================] - 0s 9us/step - loss: 1.4924 - binary_accuracy: 0.5307 - val_loss: 1.4823 - val_binary_accuracy: 0.5191\n",
            "Epoch 4/60\n",
            "23428/23428 [==============================] - 0s 10us/step - loss: 1.4771 - binary_accuracy: 0.5464 - val_loss: 1.4650 - val_binary_accuracy: 0.5191\n",
            "Epoch 5/60\n",
            "23428/23428 [==============================] - 0s 11us/step - loss: 1.4598 - binary_accuracy: 0.5659 - val_loss: 1.4469 - val_binary_accuracy: 0.6022\n",
            "Epoch 6/60\n",
            "23428/23428 [==============================] - 0s 12us/step - loss: 1.4434 - binary_accuracy: 0.5816 - val_loss: 1.4284 - val_binary_accuracy: 0.6291\n",
            "Epoch 7/60\n",
            "23428/23428 [==============================] - 0s 11us/step - loss: 1.4246 - binary_accuracy: 0.5967 - val_loss: 1.4096 - val_binary_accuracy: 0.6324\n",
            "Epoch 8/60\n",
            "23428/23428 [==============================] - 0s 10us/step - loss: 1.4062 - binary_accuracy: 0.6107 - val_loss: 1.3907 - val_binary_accuracy: 0.6425\n",
            "Epoch 9/60\n",
            "23428/23428 [==============================] - 0s 10us/step - loss: 1.3890 - binary_accuracy: 0.6232 - val_loss: 1.3720 - val_binary_accuracy: 0.6543\n",
            "Epoch 10/60\n",
            "23428/23428 [==============================] - 0s 9us/step - loss: 1.3720 - binary_accuracy: 0.6373 - val_loss: 1.3533 - val_binary_accuracy: 0.6752\n",
            "Epoch 11/60\n",
            "23428/23428 [==============================] - 0s 9us/step - loss: 1.3533 - binary_accuracy: 0.6537 - val_loss: 1.3348 - val_binary_accuracy: 0.6846\n",
            "Epoch 12/60\n",
            "23428/23428 [==============================] - 0s 10us/step - loss: 1.3351 - binary_accuracy: 0.6651 - val_loss: 1.3164 - val_binary_accuracy: 0.6889\n",
            "Epoch 13/60\n",
            "23428/23428 [==============================] - 0s 10us/step - loss: 1.3189 - binary_accuracy: 0.6720 - val_loss: 1.2981 - val_binary_accuracy: 0.6973\n",
            "Epoch 14/60\n",
            "23428/23428 [==============================] - 0s 9us/step - loss: 1.3015 - binary_accuracy: 0.6765 - val_loss: 1.2800 - val_binary_accuracy: 0.7220\n",
            "Epoch 15/60\n",
            "23428/23428 [==============================] - 0s 9us/step - loss: 1.2844 - binary_accuracy: 0.6869 - val_loss: 1.2621 - val_binary_accuracy: 0.7310\n",
            "Epoch 16/60\n",
            "23428/23428 [==============================] - 0s 9us/step - loss: 1.2673 - binary_accuracy: 0.6969 - val_loss: 1.2445 - val_binary_accuracy: 0.7435\n",
            "Epoch 17/60\n",
            "23428/23428 [==============================] - 0s 9us/step - loss: 1.2512 - binary_accuracy: 0.7033 - val_loss: 1.2271 - val_binary_accuracy: 0.7435\n",
            "Epoch 18/60\n",
            "23428/23428 [==============================] - 0s 9us/step - loss: 1.2347 - binary_accuracy: 0.7100 - val_loss: 1.2099 - val_binary_accuracy: 0.7454\n",
            "Epoch 19/60\n",
            "23428/23428 [==============================] - 0s 11us/step - loss: 1.2184 - binary_accuracy: 0.7155 - val_loss: 1.1930 - val_binary_accuracy: 0.7485\n",
            "Epoch 20/60\n",
            "23428/23428 [==============================] - 0s 10us/step - loss: 1.2031 - binary_accuracy: 0.7187 - val_loss: 1.1764 - val_binary_accuracy: 0.7499\n",
            "Epoch 21/60\n",
            "23428/23428 [==============================] - 0s 9us/step - loss: 1.1859 - binary_accuracy: 0.7240 - val_loss: 1.1600 - val_binary_accuracy: 0.7596\n",
            "Epoch 22/60\n",
            "23428/23428 [==============================] - 0s 10us/step - loss: 1.1718 - binary_accuracy: 0.7248 - val_loss: 1.1440 - val_binary_accuracy: 0.7599\n",
            "Epoch 23/60\n",
            "23428/23428 [==============================] - 0s 11us/step - loss: 1.1567 - binary_accuracy: 0.7298 - val_loss: 1.1282 - val_binary_accuracy: 0.7699\n",
            "Epoch 24/60\n",
            "23428/23428 [==============================] - 0s 11us/step - loss: 1.1407 - binary_accuracy: 0.7314 - val_loss: 1.1129 - val_binary_accuracy: 0.7745\n",
            "Epoch 25/60\n",
            "23428/23428 [==============================] - 0s 11us/step - loss: 1.1269 - binary_accuracy: 0.7347 - val_loss: 1.0980 - val_binary_accuracy: 0.7765\n",
            "Epoch 26/60\n",
            "23428/23428 [==============================] - 0s 10us/step - loss: 1.1138 - binary_accuracy: 0.7342 - val_loss: 1.0833 - val_binary_accuracy: 0.7801\n",
            "Epoch 27/60\n",
            "23428/23428 [==============================] - 0s 11us/step - loss: 1.0999 - binary_accuracy: 0.7384 - val_loss: 1.0692 - val_binary_accuracy: 0.7804\n",
            "Epoch 28/60\n",
            "23428/23428 [==============================] - 0s 11us/step - loss: 1.0852 - binary_accuracy: 0.7418 - val_loss: 1.0550 - val_binary_accuracy: 0.7818\n",
            "Epoch 29/60\n",
            "23428/23428 [==============================] - 0s 10us/step - loss: 1.0729 - binary_accuracy: 0.7374 - val_loss: 1.0413 - val_binary_accuracy: 0.7815\n",
            "Epoch 30/60\n",
            "23428/23428 [==============================] - 0s 10us/step - loss: 1.0627 - binary_accuracy: 0.7381 - val_loss: 1.0284 - val_binary_accuracy: 0.7822\n",
            "Epoch 31/60\n",
            "23428/23428 [==============================] - 0s 10us/step - loss: 1.0487 - binary_accuracy: 0.7424 - val_loss: 1.0154 - val_binary_accuracy: 0.7820\n",
            "Epoch 32/60\n",
            "23428/23428 [==============================] - 0s 10us/step - loss: 1.0363 - binary_accuracy: 0.7425 - val_loss: 1.0026 - val_binary_accuracy: 0.7861\n",
            "Epoch 33/60\n",
            "23428/23428 [==============================] - 0s 10us/step - loss: 1.0233 - binary_accuracy: 0.7456 - val_loss: 0.9903 - val_binary_accuracy: 0.7860\n",
            "Epoch 34/60\n",
            "23428/23428 [==============================] - 0s 11us/step - loss: 1.0109 - binary_accuracy: 0.7469 - val_loss: 0.9781 - val_binary_accuracy: 0.7862\n",
            "Epoch 35/60\n",
            "23428/23428 [==============================] - 0s 11us/step - loss: 1.0007 - binary_accuracy: 0.7466 - val_loss: 0.9664 - val_binary_accuracy: 0.7859\n",
            "Epoch 36/60\n",
            "23428/23428 [==============================] - 0s 11us/step - loss: 0.9885 - binary_accuracy: 0.7484 - val_loss: 0.9550 - val_binary_accuracy: 0.7872\n",
            "Epoch 37/60\n",
            "23428/23428 [==============================] - 0s 10us/step - loss: 0.9776 - binary_accuracy: 0.7483 - val_loss: 0.9437 - val_binary_accuracy: 0.7856\n",
            "Epoch 38/60\n",
            "23428/23428 [==============================] - 0s 11us/step - loss: 0.9661 - binary_accuracy: 0.7504 - val_loss: 0.9328 - val_binary_accuracy: 0.7870\n",
            "Epoch 39/60\n",
            "23428/23428 [==============================] - 0s 11us/step - loss: 0.9569 - binary_accuracy: 0.7495 - val_loss: 0.9218 - val_binary_accuracy: 0.7861\n",
            "Epoch 40/60\n",
            "23428/23428 [==============================] - 0s 10us/step - loss: 0.9446 - binary_accuracy: 0.7486 - val_loss: 0.9112 - val_binary_accuracy: 0.7864\n",
            "Epoch 41/60\n",
            "23428/23428 [==============================] - 0s 10us/step - loss: 0.9354 - binary_accuracy: 0.7527 - val_loss: 0.9009 - val_binary_accuracy: 0.7876\n",
            "Epoch 42/60\n",
            "23428/23428 [==============================] - 0s 11us/step - loss: 0.9252 - binary_accuracy: 0.7499 - val_loss: 0.8907 - val_binary_accuracy: 0.7879\n",
            "Epoch 43/60\n",
            "23428/23428 [==============================] - 0s 11us/step - loss: 0.9161 - binary_accuracy: 0.7487 - val_loss: 0.8808 - val_binary_accuracy: 0.7880\n",
            "Epoch 44/60\n",
            "23428/23428 [==============================] - 0s 11us/step - loss: 0.9072 - binary_accuracy: 0.7496 - val_loss: 0.8712 - val_binary_accuracy: 0.7879\n",
            "Epoch 45/60\n",
            "23428/23428 [==============================] - 0s 10us/step - loss: 0.8957 - binary_accuracy: 0.7524 - val_loss: 0.8613 - val_binary_accuracy: 0.7869\n",
            "Epoch 46/60\n",
            "23428/23428 [==============================] - 0s 11us/step - loss: 0.8850 - binary_accuracy: 0.7555 - val_loss: 0.8519 - val_binary_accuracy: 0.7868\n",
            "Epoch 47/60\n",
            "23428/23428 [==============================] - 0s 10us/step - loss: 0.8766 - binary_accuracy: 0.7543 - val_loss: 0.8426 - val_binary_accuracy: 0.7879\n",
            "Epoch 48/60\n",
            "23428/23428 [==============================] - 0s 11us/step - loss: 0.8702 - binary_accuracy: 0.7529 - val_loss: 0.8340 - val_binary_accuracy: 0.7876\n",
            "Epoch 49/60\n",
            "23428/23428 [==============================] - 0s 11us/step - loss: 0.8605 - binary_accuracy: 0.7559 - val_loss: 0.8250 - val_binary_accuracy: 0.7875\n",
            "Epoch 50/60\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "23428/23428 [==============================] - 0s 11us/step - loss: 0.8511 - binary_accuracy: 0.7547 - val_loss: 0.8161 - val_binary_accuracy: 0.7865\n",
            "Epoch 51/60\n",
            "23428/23428 [==============================] - 0s 11us/step - loss: 0.8419 - binary_accuracy: 0.7535 - val_loss: 0.8078 - val_binary_accuracy: 0.7876\n",
            "Epoch 52/60\n",
            "23428/23428 [==============================] - 0s 11us/step - loss: 0.8347 - binary_accuracy: 0.7532 - val_loss: 0.7995 - val_binary_accuracy: 0.7878\n",
            "Epoch 53/60\n",
            "23428/23428 [==============================] - 0s 11us/step - loss: 0.8267 - binary_accuracy: 0.7536 - val_loss: 0.7911 - val_binary_accuracy: 0.7878\n",
            "Epoch 54/60\n",
            "23428/23428 [==============================] - 0s 8us/step - loss: 0.8168 - binary_accuracy: 0.7561 - val_loss: 0.7831 - val_binary_accuracy: 0.7878\n",
            "Epoch 55/60\n",
            "23428/23428 [==============================] - 0s 9us/step - loss: 0.8115 - binary_accuracy: 0.7541 - val_loss: 0.7752 - val_binary_accuracy: 0.7883\n",
            "Epoch 56/60\n",
            "23428/23428 [==============================] - 0s 9us/step - loss: 0.8005 - binary_accuracy: 0.7567 - val_loss: 0.7673 - val_binary_accuracy: 0.7884\n",
            "Epoch 57/60\n",
            "23428/23428 [==============================] - 0s 10us/step - loss: 0.7922 - binary_accuracy: 0.7584 - val_loss: 0.7593 - val_binary_accuracy: 0.7883\n",
            "Epoch 58/60\n",
            "23428/23428 [==============================] - 0s 11us/step - loss: 0.7874 - binary_accuracy: 0.7542 - val_loss: 0.7521 - val_binary_accuracy: 0.7885\n",
            "Epoch 59/60\n",
            "23428/23428 [==============================] - 0s 10us/step - loss: 0.7781 - binary_accuracy: 0.7573 - val_loss: 0.7446 - val_binary_accuracy: 0.7885\n",
            "Epoch 60/60\n",
            "23428/23428 [==============================] - 0s 11us/step - loss: 0.7715 - binary_accuracy: 0.7586 - val_loss: 0.7370 - val_binary_accuracy: 0.7881\n",
            "23428/23428 [==============================] - 1s 63us/step\n",
            "8000/8000 [==============================] - 0s 60us/step\n",
            "(23428, 31) (8000, 31) (13470, 31) (23428,) (8000,) (13470,)\n",
            "Train on 23428 samples, validate on 8000 samples\n",
            "Epoch 1/60\n",
            "23428/23428 [==============================] - 0s 12us/step - loss: 1.5143 - binary_accuracy: 0.5719 - val_loss: 1.5072 - val_binary_accuracy: 0.6022\n",
            "Epoch 2/60\n",
            "23428/23428 [==============================] - 0s 10us/step - loss: 1.5016 - binary_accuracy: 0.5788 - val_loss: 1.4922 - val_binary_accuracy: 0.6022\n",
            "Epoch 3/60\n",
            "23428/23428 [==============================] - 0s 9us/step - loss: 1.4862 - binary_accuracy: 0.5862 - val_loss: 1.4754 - val_binary_accuracy: 0.6022\n",
            "Epoch 4/60\n",
            "23428/23428 [==============================] - 0s 9us/step - loss: 1.4697 - binary_accuracy: 0.5771 - val_loss: 1.4577 - val_binary_accuracy: 0.6022\n",
            "Epoch 5/60\n",
            "23428/23428 [==============================] - 0s 10us/step - loss: 1.4527 - binary_accuracy: 0.5801 - val_loss: 1.4393 - val_binary_accuracy: 0.6261\n",
            "Epoch 6/60\n",
            "23428/23428 [==============================] - 0s 10us/step - loss: 1.4349 - binary_accuracy: 0.5898 - val_loss: 1.4204 - val_binary_accuracy: 0.6295\n",
            "Epoch 7/60\n",
            "23428/23428 [==============================] - 0s 10us/step - loss: 1.4170 - binary_accuracy: 0.6031 - val_loss: 1.4014 - val_binary_accuracy: 0.6330\n",
            "Epoch 8/60\n",
            "23428/23428 [==============================] - 0s 10us/step - loss: 1.3996 - binary_accuracy: 0.6179 - val_loss: 1.3825 - val_binary_accuracy: 0.6464\n",
            "Epoch 9/60\n",
            "23428/23428 [==============================] - 0s 10us/step - loss: 1.3807 - binary_accuracy: 0.6295 - val_loss: 1.3636 - val_binary_accuracy: 0.6658\n",
            "Epoch 10/60\n",
            "23428/23428 [==============================] - 0s 10us/step - loss: 1.3633 - binary_accuracy: 0.6415 - val_loss: 1.3448 - val_binary_accuracy: 0.6760\n",
            "Epoch 11/60\n",
            "23428/23428 [==============================] - 0s 11us/step - loss: 1.3447 - binary_accuracy: 0.6575 - val_loss: 1.3262 - val_binary_accuracy: 0.6873\n",
            "Epoch 12/60\n",
            "23428/23428 [==============================] - 0s 9us/step - loss: 1.3283 - binary_accuracy: 0.6678 - val_loss: 1.3077 - val_binary_accuracy: 0.6921\n",
            "Epoch 13/60\n",
            "23428/23428 [==============================] - 0s 8us/step - loss: 1.3106 - binary_accuracy: 0.6771 - val_loss: 1.2895 - val_binary_accuracy: 0.6995\n",
            "Epoch 14/60\n",
            "23428/23428 [==============================] - 0s 9us/step - loss: 1.2918 - binary_accuracy: 0.6880 - val_loss: 1.2714 - val_binary_accuracy: 0.7291\n",
            "Epoch 15/60\n",
            "23428/23428 [==============================] - 0s 10us/step - loss: 1.2765 - binary_accuracy: 0.6937 - val_loss: 1.2534 - val_binary_accuracy: 0.7345\n",
            "Epoch 16/60\n",
            "23428/23428 [==============================] - 0s 9us/step - loss: 1.2589 - binary_accuracy: 0.6989 - val_loss: 1.2357 - val_binary_accuracy: 0.7444\n",
            "Epoch 17/60\n",
            "23428/23428 [==============================] - 0s 9us/step - loss: 1.2427 - binary_accuracy: 0.7039 - val_loss: 1.2183 - val_binary_accuracy: 0.7444\n",
            "Epoch 18/60\n",
            "23428/23428 [==============================] - 0s 9us/step - loss: 1.2264 - binary_accuracy: 0.7120 - val_loss: 1.2012 - val_binary_accuracy: 0.7490\n",
            "Epoch 19/60\n",
            "23428/23428 [==============================] - 0s 9us/step - loss: 1.2107 - binary_accuracy: 0.7170 - val_loss: 1.1843 - val_binary_accuracy: 0.7492\n",
            "Epoch 20/60\n",
            "23428/23428 [==============================] - 0s 9us/step - loss: 1.1937 - binary_accuracy: 0.7218 - val_loss: 1.1677 - val_binary_accuracy: 0.7511\n",
            "Epoch 21/60\n",
            "23428/23428 [==============================] - 0s 9us/step - loss: 1.1788 - binary_accuracy: 0.7234 - val_loss: 1.1514 - val_binary_accuracy: 0.7596\n",
            "Epoch 22/60\n",
            "23428/23428 [==============================] - 0s 9us/step - loss: 1.1626 - binary_accuracy: 0.7279 - val_loss: 1.1355 - val_binary_accuracy: 0.7653\n",
            "Epoch 23/60\n",
            "23428/23428 [==============================] - 0s 8us/step - loss: 1.1477 - binary_accuracy: 0.7297 - val_loss: 1.1198 - val_binary_accuracy: 0.7732\n",
            "Epoch 24/60\n",
            "23428/23428 [==============================] - 0s 9us/step - loss: 1.1344 - binary_accuracy: 0.7339 - val_loss: 1.1045 - val_binary_accuracy: 0.7747\n",
            "Epoch 25/60\n",
            "23428/23428 [==============================] - 0s 9us/step - loss: 1.1185 - binary_accuracy: 0.7356 - val_loss: 1.0895 - val_binary_accuracy: 0.7781\n",
            "Epoch 26/60\n",
            "23428/23428 [==============================] - 0s 9us/step - loss: 1.1068 - binary_accuracy: 0.7362 - val_loss: 1.0752 - val_binary_accuracy: 0.7786\n",
            "Epoch 27/60\n",
            "23428/23428 [==============================] - 0s 9us/step - loss: 1.0909 - binary_accuracy: 0.7409 - val_loss: 1.0608 - val_binary_accuracy: 0.7790\n",
            "Epoch 28/60\n",
            "23428/23428 [==============================] - 0s 9us/step - loss: 1.0793 - binary_accuracy: 0.7358 - val_loss: 1.0469 - val_binary_accuracy: 0.7814\n",
            "Epoch 29/60\n",
            "23428/23428 [==============================] - 0s 9us/step - loss: 1.0638 - binary_accuracy: 0.7410 - val_loss: 1.0332 - val_binary_accuracy: 0.7806\n",
            "Epoch 30/60\n",
            "23428/23428 [==============================] - 0s 9us/step - loss: 1.0539 - binary_accuracy: 0.7429 - val_loss: 1.0200 - val_binary_accuracy: 0.7814\n",
            "Epoch 31/60\n",
            "23428/23428 [==============================] - 0s 9us/step - loss: 1.0390 - binary_accuracy: 0.7421 - val_loss: 1.0071 - val_binary_accuracy: 0.7815\n",
            "Epoch 32/60\n",
            "23428/23428 [==============================] - 0s 9us/step - loss: 1.0259 - binary_accuracy: 0.7456 - val_loss: 0.9946 - val_binary_accuracy: 0.7826\n",
            "Epoch 33/60\n",
            "23428/23428 [==============================] - 0s 9us/step - loss: 1.0152 - binary_accuracy: 0.7473 - val_loss: 0.9823 - val_binary_accuracy: 0.7876\n",
            "Epoch 34/60\n",
            "23428/23428 [==============================] - 0s 9us/step - loss: 1.0011 - binary_accuracy: 0.7495 - val_loss: 0.9702 - val_binary_accuracy: 0.7861\n",
            "Epoch 35/60\n",
            "23428/23428 [==============================] - 0s 9us/step - loss: 0.9914 - binary_accuracy: 0.7452 - val_loss: 0.9585 - val_binary_accuracy: 0.7874\n",
            "Epoch 36/60\n",
            "23428/23428 [==============================] - 0s 10us/step - loss: 0.9814 - binary_accuracy: 0.7468 - val_loss: 0.9469 - val_binary_accuracy: 0.7874\n",
            "Epoch 37/60\n",
            "23428/23428 [==============================] - 0s 10us/step - loss: 0.9690 - binary_accuracy: 0.7491 - val_loss: 0.9355 - val_binary_accuracy: 0.7866\n",
            "Epoch 38/60\n",
            "23428/23428 [==============================] - 0s 9us/step - loss: 0.9580 - binary_accuracy: 0.7494 - val_loss: 0.9247 - val_binary_accuracy: 0.7872\n",
            "Epoch 39/60\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "23428/23428 [==============================] - 0s 10us/step - loss: 0.9486 - binary_accuracy: 0.7494 - val_loss: 0.9140 - val_binary_accuracy: 0.7874\n",
            "Epoch 40/60\n",
            "23428/23428 [==============================] - 0s 10us/step - loss: 0.9377 - binary_accuracy: 0.7494 - val_loss: 0.9034 - val_binary_accuracy: 0.7875\n",
            "Epoch 41/60\n",
            "23428/23428 [==============================] - ETA: 0s - loss: 0.9283 - binary_accuracy: 0.752 - 0s 9us/step - loss: 0.9268 - binary_accuracy: 0.7524 - val_loss: 0.8930 - val_binary_accuracy: 0.7876\n",
            "Epoch 42/60\n",
            "23428/23428 [==============================] - 0s 9us/step - loss: 0.9174 - binary_accuracy: 0.7496 - val_loss: 0.8829 - val_binary_accuracy: 0.7875\n",
            "Epoch 43/60\n",
            "23428/23428 [==============================] - 0s 9us/step - loss: 0.9070 - binary_accuracy: 0.7531 - val_loss: 0.8730 - val_binary_accuracy: 0.7875\n",
            "Epoch 44/60\n",
            "23428/23428 [==============================] - 0s 9us/step - loss: 0.8972 - binary_accuracy: 0.7528 - val_loss: 0.8633 - val_binary_accuracy: 0.7878\n",
            "Epoch 45/60\n",
            "23428/23428 [==============================] - 0s 10us/step - loss: 0.8886 - binary_accuracy: 0.7526 - val_loss: 0.8537 - val_binary_accuracy: 0.7879\n",
            "Epoch 46/60\n",
            "23428/23428 [==============================] - 0s 9us/step - loss: 0.8797 - binary_accuracy: 0.7523 - val_loss: 0.8445 - val_binary_accuracy: 0.7878\n",
            "Epoch 47/60\n",
            "23428/23428 [==============================] - 0s 10us/step - loss: 0.8701 - binary_accuracy: 0.7526 - val_loss: 0.8353 - val_binary_accuracy: 0.7878\n",
            "Epoch 48/60\n",
            "23428/23428 [==============================] - 0s 10us/step - loss: 0.8601 - binary_accuracy: 0.7549 - val_loss: 0.8262 - val_binary_accuracy: 0.7879\n",
            "Epoch 49/60\n",
            "23428/23428 [==============================] - 0s 10us/step - loss: 0.8512 - binary_accuracy: 0.7523 - val_loss: 0.8172 - val_binary_accuracy: 0.7865\n",
            "Epoch 50/60\n",
            "23428/23428 [==============================] - 0s 9us/step - loss: 0.8413 - binary_accuracy: 0.7537 - val_loss: 0.8086 - val_binary_accuracy: 0.7878\n",
            "Epoch 51/60\n",
            "23428/23428 [==============================] - 0s 9us/step - loss: 0.8349 - binary_accuracy: 0.7541 - val_loss: 0.8002 - val_binary_accuracy: 0.7876\n",
            "Epoch 52/60\n",
            "23428/23428 [==============================] - 0s 9us/step - loss: 0.8255 - binary_accuracy: 0.7553 - val_loss: 0.7916 - val_binary_accuracy: 0.7866\n",
            "Epoch 53/60\n",
            "23428/23428 [==============================] - 0s 10us/step - loss: 0.8181 - binary_accuracy: 0.7545 - val_loss: 0.7836 - val_binary_accuracy: 0.7876\n",
            "Epoch 54/60\n",
            "23428/23428 [==============================] - 0s 9us/step - loss: 0.8098 - binary_accuracy: 0.7570 - val_loss: 0.7754 - val_binary_accuracy: 0.7876\n",
            "Epoch 55/60\n",
            "23428/23428 [==============================] - 0s 9us/step - loss: 0.8025 - binary_accuracy: 0.7560 - val_loss: 0.7675 - val_binary_accuracy: 0.7878\n",
            "Epoch 56/60\n",
            "23428/23428 [==============================] - 0s 9us/step - loss: 0.7929 - binary_accuracy: 0.7565 - val_loss: 0.7594 - val_binary_accuracy: 0.7868\n",
            "Epoch 57/60\n",
            "23428/23428 [==============================] - 0s 10us/step - loss: 0.7857 - binary_accuracy: 0.7558 - val_loss: 0.7519 - val_binary_accuracy: 0.7880\n",
            "Epoch 58/60\n",
            "23428/23428 [==============================] - 0s 10us/step - loss: 0.7783 - binary_accuracy: 0.7574 - val_loss: 0.7446 - val_binary_accuracy: 0.7884\n",
            "Epoch 59/60\n",
            "23428/23428 [==============================] - 0s 10us/step - loss: 0.7705 - binary_accuracy: 0.7555 - val_loss: 0.7371 - val_binary_accuracy: 0.7884\n",
            "Epoch 60/60\n",
            "23428/23428 [==============================] - 0s 10us/step - loss: 0.7630 - binary_accuracy: 0.7584 - val_loss: 0.7296 - val_binary_accuracy: 0.7866\n",
            "23428/23428 [==============================] - 1s 63us/step\n",
            "8000/8000 [==============================] - 1s 71us/step\n",
            "(23428, 31) (8000, 31) (13470, 31) (23428,) (8000,) (13470,)\n",
            "Train on 23428 samples, validate on 8000 samples\n",
            "Epoch 1/60\n",
            "23428/23428 [==============================] - 0s 11us/step - loss: 1.5143 - binary_accuracy: 0.5551 - val_loss: 1.5072 - val_binary_accuracy: 0.6022\n",
            "Epoch 2/60\n",
            "23428/23428 [==============================] - 0s 10us/step - loss: 1.5016 - binary_accuracy: 0.5602 - val_loss: 1.4923 - val_binary_accuracy: 0.6022\n",
            "Epoch 3/60\n",
            "23428/23428 [==============================] - 0s 10us/step - loss: 1.4864 - binary_accuracy: 0.5645 - val_loss: 1.4756 - val_binary_accuracy: 0.6022\n",
            "Epoch 4/60\n",
            "23428/23428 [==============================] - 0s 8us/step - loss: 1.4704 - binary_accuracy: 0.5668 - val_loss: 1.4580 - val_binary_accuracy: 0.6022\n",
            "Epoch 5/60\n",
            "23428/23428 [==============================] - 0s 9us/step - loss: 1.4526 - binary_accuracy: 0.5823 - val_loss: 1.4398 - val_binary_accuracy: 0.6261\n",
            "Epoch 6/60\n",
            "23428/23428 [==============================] - 0s 10us/step - loss: 1.4351 - binary_accuracy: 0.5885 - val_loss: 1.4211 - val_binary_accuracy: 0.6301\n",
            "Epoch 7/60\n",
            "23428/23428 [==============================] - 0s 10us/step - loss: 1.4182 - binary_accuracy: 0.6028 - val_loss: 1.4022 - val_binary_accuracy: 0.6332\n",
            "Epoch 8/60\n",
            "23428/23428 [==============================] - 0s 10us/step - loss: 1.4000 - binary_accuracy: 0.6188 - val_loss: 1.3834 - val_binary_accuracy: 0.6457\n",
            "Epoch 9/60\n",
            "23428/23428 [==============================] - 0s 9us/step - loss: 1.3817 - binary_accuracy: 0.6270 - val_loss: 1.3646 - val_binary_accuracy: 0.6675\n",
            "Epoch 10/60\n",
            "23428/23428 [==============================] - 0s 9us/step - loss: 1.3629 - binary_accuracy: 0.6437 - val_loss: 1.3459 - val_binary_accuracy: 0.6735\n",
            "Epoch 11/60\n",
            "23428/23428 [==============================] - 0s 9us/step - loss: 1.3463 - binary_accuracy: 0.6520 - val_loss: 1.3273 - val_binary_accuracy: 0.6860\n",
            "Epoch 12/60\n",
            "23428/23428 [==============================] - 0s 10us/step - loss: 1.3288 - binary_accuracy: 0.6612 - val_loss: 1.3089 - val_binary_accuracy: 0.6921\n",
            "Epoch 13/60\n",
            "23428/23428 [==============================] - 0s 9us/step - loss: 1.3106 - binary_accuracy: 0.6807 - val_loss: 1.2905 - val_binary_accuracy: 0.7021\n",
            "Epoch 14/60\n",
            "23428/23428 [==============================] - 0s 9us/step - loss: 1.2938 - binary_accuracy: 0.6837 - val_loss: 1.2724 - val_binary_accuracy: 0.7244\n",
            "Epoch 15/60\n",
            "23428/23428 [==============================] - 0s 9us/step - loss: 1.2763 - binary_accuracy: 0.6940 - val_loss: 1.2545 - val_binary_accuracy: 0.7349\n",
            "Epoch 16/60\n",
            "23428/23428 [==============================] - 0s 10us/step - loss: 1.2599 - binary_accuracy: 0.6996 - val_loss: 1.2368 - val_binary_accuracy: 0.7425\n",
            "Epoch 17/60\n",
            "23428/23428 [==============================] - 0s 9us/step - loss: 1.2444 - binary_accuracy: 0.7070 - val_loss: 1.2194 - val_binary_accuracy: 0.7435\n",
            "Epoch 18/60\n",
            "23428/23428 [==============================] - 0s 9us/step - loss: 1.2274 - binary_accuracy: 0.7106 - val_loss: 1.2022 - val_binary_accuracy: 0.7475\n",
            "Epoch 19/60\n",
            "23428/23428 [==============================] - 0s 10us/step - loss: 1.2112 - binary_accuracy: 0.7158 - val_loss: 1.1853 - val_binary_accuracy: 0.7487\n",
            "Epoch 20/60\n",
            "23428/23428 [==============================] - 0s 9us/step - loss: 1.1956 - binary_accuracy: 0.7209 - val_loss: 1.1687 - val_binary_accuracy: 0.7501\n",
            "Epoch 21/60\n",
            "23428/23428 [==============================] - 0s 9us/step - loss: 1.1789 - binary_accuracy: 0.7282 - val_loss: 1.1525 - val_binary_accuracy: 0.7611\n",
            "Epoch 22/60\n",
            "23428/23428 [==============================] - 0s 9us/step - loss: 1.1644 - binary_accuracy: 0.7314 - val_loss: 1.1365 - val_binary_accuracy: 0.7705\n",
            "Epoch 23/60\n",
            "23428/23428 [==============================] - 0s 9us/step - loss: 1.1507 - binary_accuracy: 0.7256 - val_loss: 1.1209 - val_binary_accuracy: 0.7701\n",
            "Epoch 24/60\n",
            "23428/23428 [==============================] - 0s 9us/step - loss: 1.1337 - binary_accuracy: 0.7338 - val_loss: 1.1055 - val_binary_accuracy: 0.7749\n",
            "Epoch 25/60\n",
            "23428/23428 [==============================] - 0s 9us/step - loss: 1.1204 - binary_accuracy: 0.7335 - val_loss: 1.0905 - val_binary_accuracy: 0.7789\n",
            "Epoch 26/60\n",
            "23428/23428 [==============================] - 0s 9us/step - loss: 1.1068 - binary_accuracy: 0.7370 - val_loss: 1.0761 - val_binary_accuracy: 0.7799\n",
            "Epoch 27/60\n",
            "23428/23428 [==============================] - 0s 9us/step - loss: 1.0932 - binary_accuracy: 0.7359 - val_loss: 1.0619 - val_binary_accuracy: 0.7810\n",
            "Epoch 28/60\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "23428/23428 [==============================] - 0s 10us/step - loss: 1.0783 - binary_accuracy: 0.7400 - val_loss: 1.0479 - val_binary_accuracy: 0.7815\n",
            "Epoch 29/60\n",
            "23428/23428 [==============================] - 0s 10us/step - loss: 1.0654 - binary_accuracy: 0.7405 - val_loss: 1.0344 - val_binary_accuracy: 0.7826\n",
            "Epoch 30/60\n",
            "23428/23428 [==============================] - 0s 10us/step - loss: 1.0533 - binary_accuracy: 0.7430 - val_loss: 1.0210 - val_binary_accuracy: 0.7825\n",
            "Epoch 31/60\n",
            "23428/23428 [==============================] - 0s 9us/step - loss: 1.0416 - binary_accuracy: 0.7409 - val_loss: 1.0082 - val_binary_accuracy: 0.7857\n",
            "Epoch 32/60\n",
            "23428/23428 [==============================] - 0s 10us/step - loss: 1.0273 - binary_accuracy: 0.7431 - val_loss: 0.9956 - val_binary_accuracy: 0.7855\n",
            "Epoch 33/60\n",
            "23428/23428 [==============================] - 0s 9us/step - loss: 1.0148 - binary_accuracy: 0.7468 - val_loss: 0.9831 - val_binary_accuracy: 0.7855\n",
            "Epoch 34/60\n",
            "23428/23428 [==============================] - 0s 10us/step - loss: 1.0043 - binary_accuracy: 0.7445 - val_loss: 0.9710 - val_binary_accuracy: 0.7857\n",
            "Epoch 35/60\n",
            "23428/23428 [==============================] - 0s 10us/step - loss: 0.9912 - binary_accuracy: 0.7483 - val_loss: 0.9592 - val_binary_accuracy: 0.7859\n",
            "Epoch 36/60\n",
            "23428/23428 [==============================] - 0s 10us/step - loss: 0.9787 - binary_accuracy: 0.7515 - val_loss: 0.9477 - val_binary_accuracy: 0.7869\n",
            "Epoch 37/60\n",
            "23428/23428 [==============================] - 0s 10us/step - loss: 0.9692 - binary_accuracy: 0.7501 - val_loss: 0.9363 - val_binary_accuracy: 0.7861\n",
            "Epoch 38/60\n",
            "23428/23428 [==============================] - 0s 11us/step - loss: 0.9586 - binary_accuracy: 0.7483 - val_loss: 0.9252 - val_binary_accuracy: 0.7865\n",
            "Epoch 39/60\n",
            "23428/23428 [==============================] - 0s 10us/step - loss: 0.9493 - binary_accuracy: 0.7476 - val_loss: 0.9143 - val_binary_accuracy: 0.7866\n",
            "Epoch 40/60\n",
            "23428/23428 [==============================] - 0s 11us/step - loss: 0.9382 - binary_accuracy: 0.7520 - val_loss: 0.9040 - val_binary_accuracy: 0.7875\n",
            "Epoch 41/60\n",
            "23428/23428 [==============================] - 0s 10us/step - loss: 0.9280 - binary_accuracy: 0.7511 - val_loss: 0.8936 - val_binary_accuracy: 0.7875\n",
            "Epoch 42/60\n",
            "23428/23428 [==============================] - 0s 10us/step - loss: 0.9173 - binary_accuracy: 0.7531 - val_loss: 0.8835 - val_binary_accuracy: 0.7876\n",
            "Epoch 43/60\n",
            "23428/23428 [==============================] - 0s 10us/step - loss: 0.9077 - binary_accuracy: 0.7532 - val_loss: 0.8735 - val_binary_accuracy: 0.7866\n",
            "Epoch 44/60\n",
            "23428/23428 [==============================] - 0s 10us/step - loss: 0.8987 - binary_accuracy: 0.7526 - val_loss: 0.8639 - val_binary_accuracy: 0.7880\n",
            "Epoch 45/60\n",
            "23428/23428 [==============================] - 0s 10us/step - loss: 0.8895 - binary_accuracy: 0.7508 - val_loss: 0.8545 - val_binary_accuracy: 0.7878\n",
            "Epoch 46/60\n",
            "23428/23428 [==============================] - 0s 11us/step - loss: 0.8799 - binary_accuracy: 0.7536 - val_loss: 0.8452 - val_binary_accuracy: 0.7878\n",
            "Epoch 47/60\n",
            "23428/23428 [==============================] - 0s 10us/step - loss: 0.8694 - binary_accuracy: 0.7562 - val_loss: 0.8356 - val_binary_accuracy: 0.7866\n",
            "Epoch 48/60\n",
            "23428/23428 [==============================] - 0s 11us/step - loss: 0.8610 - binary_accuracy: 0.7548 - val_loss: 0.8266 - val_binary_accuracy: 0.7866\n",
            "Epoch 49/60\n",
            "23428/23428 [==============================] - 0s 10us/step - loss: 0.8508 - binary_accuracy: 0.7551 - val_loss: 0.8179 - val_binary_accuracy: 0.7875\n",
            "Epoch 50/60\n",
            "23428/23428 [==============================] - 0s 10us/step - loss: 0.8421 - binary_accuracy: 0.7548 - val_loss: 0.8090 - val_binary_accuracy: 0.7865\n",
            "Epoch 51/60\n",
            "23428/23428 [==============================] - 0s 10us/step - loss: 0.8350 - binary_accuracy: 0.7524 - val_loss: 0.8006 - val_binary_accuracy: 0.7876\n",
            "Epoch 52/60\n",
            "23428/23428 [==============================] - 0s 10us/step - loss: 0.8256 - binary_accuracy: 0.7547 - val_loss: 0.7921 - val_binary_accuracy: 0.7879\n",
            "Epoch 53/60\n",
            "23428/23428 [==============================] - 0s 11us/step - loss: 0.8194 - binary_accuracy: 0.7538 - val_loss: 0.7839 - val_binary_accuracy: 0.7879\n",
            "Epoch 54/60\n",
            "23428/23428 [==============================] - 0s 10us/step - loss: 0.8093 - binary_accuracy: 0.7546 - val_loss: 0.7756 - val_binary_accuracy: 0.7866\n",
            "Epoch 55/60\n",
            "23428/23428 [==============================] - 0s 10us/step - loss: 0.8020 - binary_accuracy: 0.7564 - val_loss: 0.7676 - val_binary_accuracy: 0.7880\n",
            "Epoch 56/60\n",
            "23428/23428 [==============================] - 0s 10us/step - loss: 0.7944 - binary_accuracy: 0.7570 - val_loss: 0.7598 - val_binary_accuracy: 0.7883\n",
            "Epoch 57/60\n",
            "23428/23428 [==============================] - 0s 10us/step - loss: 0.7883 - binary_accuracy: 0.7574 - val_loss: 0.7524 - val_binary_accuracy: 0.7885\n",
            "Epoch 58/60\n",
            "23428/23428 [==============================] - 0s 10us/step - loss: 0.7787 - binary_accuracy: 0.7573 - val_loss: 0.7445 - val_binary_accuracy: 0.7868\n",
            "Epoch 59/60\n",
            "23428/23428 [==============================] - 0s 10us/step - loss: 0.7702 - binary_accuracy: 0.7582 - val_loss: 0.7372 - val_binary_accuracy: 0.7884\n",
            "Epoch 60/60\n",
            "23428/23428 [==============================] - 0s 9us/step - loss: 0.7645 - binary_accuracy: 0.7576 - val_loss: 0.7299 - val_binary_accuracy: 0.7884\n",
            "23428/23428 [==============================] - 1s 63us/step\n",
            "8000/8000 [==============================] - 1s 76us/step\n",
            "\n",
            "\n",
            "Ensayando modelo con estructura [512, 10, 1] y optimizador <keras.optimizers.Adam object at 0x00000180021B5B88>\n",
            "(23428, 31) (8000, 31) (13470, 31) (23428,) (8000,) (13470,)\n",
            "Train on 23428 samples, validate on 8000 samples\n",
            "Epoch 1/60\n",
            "23428/23428 [==============================] - 1s 64us/step - loss: 1.4700 - binary_accuracy: 0.5213 - val_loss: 1.3925 - val_binary_accuracy: 0.5669\n",
            "Epoch 2/60\n",
            "23428/23428 [==============================] - 0s 11us/step - loss: 1.3389 - binary_accuracy: 0.5920 - val_loss: 1.2640 - val_binary_accuracy: 0.6026\n",
            "Epoch 3/60\n",
            "23428/23428 [==============================] - 0s 10us/step - loss: 1.2148 - binary_accuracy: 0.6326 - val_loss: 1.1410 - val_binary_accuracy: 0.7234\n",
            "Epoch 4/60\n",
            "23428/23428 [==============================] - 0s 10us/step - loss: 1.0986 - binary_accuracy: 0.6669 - val_loss: 1.0261 - val_binary_accuracy: 0.7517\n",
            "Epoch 5/60\n",
            "23428/23428 [==============================] - 0s 9us/step - loss: 0.9916 - binary_accuracy: 0.6940 - val_loss: 0.9204 - val_binary_accuracy: 0.7609\n",
            "Epoch 6/60\n",
            "23428/23428 [==============================] - 0s 10us/step - loss: 0.8946 - binary_accuracy: 0.7142 - val_loss: 0.8257 - val_binary_accuracy: 0.7747\n",
            "Epoch 7/60\n",
            "23428/23428 [==============================] - 0s 10us/step - loss: 0.8111 - binary_accuracy: 0.7188 - val_loss: 0.7459 - val_binary_accuracy: 0.7786\n",
            "Epoch 8/60\n",
            "23428/23428 [==============================] - 0s 9us/step - loss: 0.7412 - binary_accuracy: 0.7324 - val_loss: 0.6811 - val_binary_accuracy: 0.7782\n",
            "Epoch 9/60\n",
            "23428/23428 [==============================] - 0s 8us/step - loss: 0.6874 - binary_accuracy: 0.7312 - val_loss: 0.6297 - val_binary_accuracy: 0.7782\n",
            "Epoch 10/60\n",
            "23428/23428 [==============================] - 0s 10us/step - loss: 0.6417 - binary_accuracy: 0.7369 - val_loss: 0.5899 - val_binary_accuracy: 0.7780\n",
            "Epoch 11/60\n",
            "23428/23428 [==============================] - 0s 10us/step - loss: 0.6084 - binary_accuracy: 0.7396 - val_loss: 0.5617 - val_binary_accuracy: 0.7793\n",
            "Epoch 12/60\n",
            "23428/23428 [==============================] - 0s 9us/step - loss: 0.5857 - binary_accuracy: 0.7370 - val_loss: 0.5440 - val_binary_accuracy: 0.7779\n",
            "Epoch 13/60\n",
            "23428/23428 [==============================] - 0s 9us/step - loss: 0.5705 - binary_accuracy: 0.7419 - val_loss: 0.5345 - val_binary_accuracy: 0.7830\n",
            "Epoch 14/60\n",
            "23428/23428 [==============================] - 0s 9us/step - loss: 0.5622 - binary_accuracy: 0.7436 - val_loss: 0.5280 - val_binary_accuracy: 0.7788\n",
            "Epoch 15/60\n",
            "23428/23428 [==============================] - 0s 9us/step - loss: 0.5563 - binary_accuracy: 0.7445 - val_loss: 0.5225 - val_binary_accuracy: 0.7781\n",
            "Epoch 16/60\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "23428/23428 [==============================] - 0s 10us/step - loss: 0.5535 - binary_accuracy: 0.7436 - val_loss: 0.5180 - val_binary_accuracy: 0.7797\n",
            "Epoch 17/60\n",
            "23428/23428 [==============================] - 0s 10us/step - loss: 0.5469 - binary_accuracy: 0.7489 - val_loss: 0.5149 - val_binary_accuracy: 0.7806\n",
            "Epoch 18/60\n",
            "23428/23428 [==============================] - 0s 10us/step - loss: 0.5447 - binary_accuracy: 0.7460 - val_loss: 0.5122 - val_binary_accuracy: 0.7809\n",
            "Epoch 19/60\n",
            "23428/23428 [==============================] - 0s 11us/step - loss: 0.5418 - binary_accuracy: 0.7512 - val_loss: 0.5090 - val_binary_accuracy: 0.7801\n",
            "Epoch 20/60\n",
            "23428/23428 [==============================] - 0s 10us/step - loss: 0.5424 - binary_accuracy: 0.7436 - val_loss: 0.5063 - val_binary_accuracy: 0.7799\n",
            "Epoch 21/60\n",
            "23428/23428 [==============================] - 0s 10us/step - loss: 0.5374 - binary_accuracy: 0.7506 - val_loss: 0.5043 - val_binary_accuracy: 0.7837\n",
            "Epoch 22/60\n",
            "23428/23428 [==============================] - 0s 10us/step - loss: 0.5359 - binary_accuracy: 0.7516 - val_loss: 0.5025 - val_binary_accuracy: 0.7854\n",
            "Epoch 23/60\n",
            "23428/23428 [==============================] - 0s 11us/step - loss: 0.5326 - binary_accuracy: 0.7506 - val_loss: 0.5000 - val_binary_accuracy: 0.7806\n",
            "Epoch 24/60\n",
            "23428/23428 [==============================] - 0s 11us/step - loss: 0.5313 - binary_accuracy: 0.7524 - val_loss: 0.4976 - val_binary_accuracy: 0.7841\n",
            "Epoch 25/60\n",
            "23428/23428 [==============================] - 0s 10us/step - loss: 0.5296 - binary_accuracy: 0.7517 - val_loss: 0.4952 - val_binary_accuracy: 0.7857\n",
            "Epoch 26/60\n",
            "23428/23428 [==============================] - 0s 10us/step - loss: 0.5271 - binary_accuracy: 0.7558 - val_loss: 0.4941 - val_binary_accuracy: 0.7879\n",
            "Epoch 27/60\n",
            "23428/23428 [==============================] - 0s 10us/step - loss: 0.5270 - binary_accuracy: 0.7545 - val_loss: 0.4929 - val_binary_accuracy: 0.7875\n",
            "Epoch 28/60\n",
            "23428/23428 [==============================] - 0s 10us/step - loss: 0.5259 - binary_accuracy: 0.7545 - val_loss: 0.4914 - val_binary_accuracy: 0.7864\n",
            "Epoch 29/60\n",
            "23428/23428 [==============================] - 0s 10us/step - loss: 0.5234 - binary_accuracy: 0.7555 - val_loss: 0.4894 - val_binary_accuracy: 0.7870\n",
            "Epoch 30/60\n",
            "23428/23428 [==============================] - 0s 9us/step - loss: 0.5205 - binary_accuracy: 0.7561 - val_loss: 0.4874 - val_binary_accuracy: 0.7874\n",
            "Epoch 31/60\n",
            "23428/23428 [==============================] - 0s 11us/step - loss: 0.5207 - binary_accuracy: 0.7559 - val_loss: 0.4865 - val_binary_accuracy: 0.7859\n",
            "Epoch 32/60\n",
            "23428/23428 [==============================] - 0s 10us/step - loss: 0.5174 - binary_accuracy: 0.7579 - val_loss: 0.4854 - val_binary_accuracy: 0.7876\n",
            "Epoch 33/60\n",
            "23428/23428 [==============================] - 0s 10us/step - loss: 0.5166 - binary_accuracy: 0.7552 - val_loss: 0.4839 - val_binary_accuracy: 0.7890\n",
            "Epoch 34/60\n",
            "23428/23428 [==============================] - 0s 10us/step - loss: 0.5174 - binary_accuracy: 0.7571 - val_loss: 0.4828 - val_binary_accuracy: 0.7897\n",
            "Epoch 35/60\n",
            "23428/23428 [==============================] - 0s 9us/step - loss: 0.5145 - binary_accuracy: 0.7586 - val_loss: 0.4821 - val_binary_accuracy: 0.7915\n",
            "Epoch 36/60\n",
            "23428/23428 [==============================] - 0s 10us/step - loss: 0.5128 - binary_accuracy: 0.7571 - val_loss: 0.4810 - val_binary_accuracy: 0.7876\n",
            "Epoch 37/60\n",
            "23428/23428 [==============================] - 0s 9us/step - loss: 0.5111 - binary_accuracy: 0.7585 - val_loss: 0.4796 - val_binary_accuracy: 0.7876\n",
            "Epoch 38/60\n",
            "23428/23428 [==============================] - 0s 10us/step - loss: 0.5105 - binary_accuracy: 0.7602 - val_loss: 0.4778 - val_binary_accuracy: 0.7896\n",
            "Epoch 39/60\n",
            "23428/23428 [==============================] - 0s 9us/step - loss: 0.5098 - binary_accuracy: 0.7593 - val_loss: 0.4778 - val_binary_accuracy: 0.7890\n",
            "Epoch 40/60\n",
            "23428/23428 [==============================] - 0s 9us/step - loss: 0.5103 - binary_accuracy: 0.7604 - val_loss: 0.4767 - val_binary_accuracy: 0.7899\n",
            "Epoch 41/60\n",
            "23428/23428 [==============================] - 0s 9us/step - loss: 0.5069 - binary_accuracy: 0.7638 - val_loss: 0.4753 - val_binary_accuracy: 0.7903\n",
            "Epoch 42/60\n",
            "23428/23428 [==============================] - 0s 10us/step - loss: 0.5083 - binary_accuracy: 0.7599 - val_loss: 0.4746 - val_binary_accuracy: 0.7901\n",
            "Epoch 43/60\n",
            "23428/23428 [==============================] - 0s 9us/step - loss: 0.5071 - binary_accuracy: 0.7615 - val_loss: 0.4741 - val_binary_accuracy: 0.7903\n",
            "Epoch 44/60\n",
            "23428/23428 [==============================] - 0s 9us/step - loss: 0.5056 - binary_accuracy: 0.7597 - val_loss: 0.4734 - val_binary_accuracy: 0.7897\n",
            "Epoch 45/60\n",
            "23428/23428 [==============================] - 0s 9us/step - loss: 0.5047 - binary_accuracy: 0.7622 - val_loss: 0.4726 - val_binary_accuracy: 0.7901\n",
            "Epoch 46/60\n",
            "23428/23428 [==============================] - 0s 9us/step - loss: 0.5060 - binary_accuracy: 0.7633 - val_loss: 0.4718 - val_binary_accuracy: 0.7906\n",
            "Epoch 47/60\n",
            "23428/23428 [==============================] - 0s 10us/step - loss: 0.5045 - binary_accuracy: 0.7617 - val_loss: 0.4716 - val_binary_accuracy: 0.7899\n",
            "Epoch 48/60\n",
            "23428/23428 [==============================] - 0s 9us/step - loss: 0.5037 - binary_accuracy: 0.7617 - val_loss: 0.4697 - val_binary_accuracy: 0.7900\n",
            "Epoch 49/60\n",
            "23428/23428 [==============================] - 0s 9us/step - loss: 0.5032 - binary_accuracy: 0.7634 - val_loss: 0.4692 - val_binary_accuracy: 0.7885\n",
            "Epoch 50/60\n",
            "23428/23428 [==============================] - 0s 10us/step - loss: 0.5029 - binary_accuracy: 0.7623 - val_loss: 0.4687 - val_binary_accuracy: 0.7900\n",
            "Epoch 51/60\n",
            "23428/23428 [==============================] - 0s 9us/step - loss: 0.5019 - binary_accuracy: 0.7624 - val_loss: 0.4681 - val_binary_accuracy: 0.7903\n",
            "Epoch 52/60\n",
            "23428/23428 [==============================] - 0s 10us/step - loss: 0.5004 - binary_accuracy: 0.7655 - val_loss: 0.4677 - val_binary_accuracy: 0.7879\n",
            "Epoch 53/60\n",
            "23428/23428 [==============================] - 0s 10us/step - loss: 0.5017 - binary_accuracy: 0.7620 - val_loss: 0.4673 - val_binary_accuracy: 0.7881\n",
            "Epoch 54/60\n",
            "23428/23428 [==============================] - 0s 9us/step - loss: 0.4982 - binary_accuracy: 0.7644 - val_loss: 0.4666 - val_binary_accuracy: 0.7890\n",
            "Epoch 55/60\n",
            "23428/23428 [==============================] - 0s 9us/step - loss: 0.4987 - binary_accuracy: 0.7638 - val_loss: 0.4656 - val_binary_accuracy: 0.7900\n",
            "Epoch 56/60\n",
            "23428/23428 [==============================] - 0s 9us/step - loss: 0.4998 - binary_accuracy: 0.7605 - val_loss: 0.4650 - val_binary_accuracy: 0.7895\n",
            "Epoch 57/60\n",
            "23428/23428 [==============================] - 0s 8us/step - loss: 0.4987 - binary_accuracy: 0.7605 - val_loss: 0.4646 - val_binary_accuracy: 0.7906\n",
            "Epoch 58/60\n",
            "23428/23428 [==============================] - 0s 8us/step - loss: 0.4979 - binary_accuracy: 0.7637 - val_loss: 0.4646 - val_binary_accuracy: 0.7890\n",
            "Epoch 59/60\n",
            "23428/23428 [==============================] - 0s 8us/step - loss: 0.4981 - binary_accuracy: 0.7653 - val_loss: 0.4641 - val_binary_accuracy: 0.7894\n",
            "Epoch 60/60\n",
            "23428/23428 [==============================] - 0s 8us/step - loss: 0.4968 - binary_accuracy: 0.7631 - val_loss: 0.4637 - val_binary_accuracy: 0.7904\n",
            "23428/23428 [==============================] - 1s 48us/step\n",
            "8000/8000 [==============================] - 0s 60us/step\n",
            "(23428, 31) (8000, 31) (13470, 31) (23428,) (8000,) (13470,)\n",
            "Train on 23428 samples, validate on 8000 samples\n",
            "Epoch 1/60\n",
            "23428/23428 [==============================] - 0s 12us/step - loss: 1.5099 - binary_accuracy: 0.5527 - val_loss: 1.4697 - val_binary_accuracy: 0.6593\n",
            "Epoch 2/60\n",
            "23428/23428 [==============================] - 0s 8us/step - loss: 1.4280 - binary_accuracy: 0.6181 - val_loss: 1.3603 - val_binary_accuracy: 0.6936\n",
            "Epoch 3/60\n",
            "23428/23428 [==============================] - 0s 8us/step - loss: 1.3154 - binary_accuracy: 0.6496 - val_loss: 1.2397 - val_binary_accuracy: 0.7470\n",
            "Epoch 4/60\n",
            "23428/23428 [==============================] - 0s 8us/step - loss: 1.1998 - binary_accuracy: 0.6909 - val_loss: 1.1180 - val_binary_accuracy: 0.7582\n",
            "Epoch 5/60\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "23428/23428 [==============================] - 0s 10us/step - loss: 1.0867 - binary_accuracy: 0.7133 - val_loss: 0.9985 - val_binary_accuracy: 0.7720\n",
            "Epoch 6/60\n",
            "23428/23428 [==============================] - 0s 11us/step - loss: 0.9795 - binary_accuracy: 0.7283 - val_loss: 0.8922 - val_binary_accuracy: 0.7800\n",
            "Epoch 7/60\n",
            "23428/23428 [==============================] - 0s 10us/step - loss: 0.8863 - binary_accuracy: 0.7373 - val_loss: 0.8043 - val_binary_accuracy: 0.7839\n",
            "Epoch 8/60\n",
            "23428/23428 [==============================] - 0s 9us/step - loss: 0.8061 - binary_accuracy: 0.7448 - val_loss: 0.7337 - val_binary_accuracy: 0.7836\n",
            "Epoch 9/60\n",
            "23428/23428 [==============================] - 0s 9us/step - loss: 0.7456 - binary_accuracy: 0.7456 - val_loss: 0.6763 - val_binary_accuracy: 0.7871\n",
            "Epoch 10/60\n",
            "23428/23428 [==============================] - 0s 9us/step - loss: 0.6893 - binary_accuracy: 0.7522 - val_loss: 0.6298 - val_binary_accuracy: 0.7797\n",
            "Epoch 11/60\n",
            "23428/23428 [==============================] - 0s 9us/step - loss: 0.6493 - binary_accuracy: 0.7503 - val_loss: 0.5938 - val_binary_accuracy: 0.7864\n",
            "Epoch 12/60\n",
            "23428/23428 [==============================] - 0s 10us/step - loss: 0.6154 - binary_accuracy: 0.7510 - val_loss: 0.5664 - val_binary_accuracy: 0.7843\n",
            "Epoch 13/60\n",
            "23428/23428 [==============================] - 0s 10us/step - loss: 0.5902 - binary_accuracy: 0.7529 - val_loss: 0.5460 - val_binary_accuracy: 0.7854\n",
            "Epoch 14/60\n",
            "23428/23428 [==============================] - 0s 10us/step - loss: 0.5731 - binary_accuracy: 0.7526 - val_loss: 0.5325 - val_binary_accuracy: 0.7853\n",
            "Epoch 15/60\n",
            "23428/23428 [==============================] - 0s 10us/step - loss: 0.5623 - binary_accuracy: 0.7518 - val_loss: 0.5248 - val_binary_accuracy: 0.7886\n",
            "Epoch 16/60\n",
            "23428/23428 [==============================] - 0s 10us/step - loss: 0.5539 - binary_accuracy: 0.7510 - val_loss: 0.5172 - val_binary_accuracy: 0.7881\n",
            "Epoch 17/60\n",
            "23428/23428 [==============================] - 0s 10us/step - loss: 0.5468 - binary_accuracy: 0.7530 - val_loss: 0.5123 - val_binary_accuracy: 0.7797\n",
            "Epoch 18/60\n",
            "23428/23428 [==============================] - 0s 10us/step - loss: 0.5462 - binary_accuracy: 0.7480 - val_loss: 0.5087 - val_binary_accuracy: 0.7793\n",
            "Epoch 19/60\n",
            "23428/23428 [==============================] - 0s 9us/step - loss: 0.5381 - binary_accuracy: 0.7529 - val_loss: 0.5041 - val_binary_accuracy: 0.7829\n",
            "Epoch 20/60\n",
            "23428/23428 [==============================] - 0s 9us/step - loss: 0.5334 - binary_accuracy: 0.7536 - val_loss: 0.5003 - val_binary_accuracy: 0.7856\n",
            "Epoch 21/60\n",
            "23428/23428 [==============================] - 0s 9us/step - loss: 0.5322 - binary_accuracy: 0.7527 - val_loss: 0.4981 - val_binary_accuracy: 0.7871\n",
            "Epoch 22/60\n",
            "23428/23428 [==============================] - 0s 9us/step - loss: 0.5279 - binary_accuracy: 0.7537 - val_loss: 0.4953 - val_binary_accuracy: 0.7880\n",
            "Epoch 23/60\n",
            "23428/23428 [==============================] - 0s 10us/step - loss: 0.5274 - binary_accuracy: 0.7566 - val_loss: 0.4930 - val_binary_accuracy: 0.7889\n",
            "Epoch 24/60\n",
            "23428/23428 [==============================] - 0s 9us/step - loss: 0.5252 - binary_accuracy: 0.7571 - val_loss: 0.4916 - val_binary_accuracy: 0.7896\n",
            "Epoch 25/60\n",
            "23428/23428 [==============================] - 0s 9us/step - loss: 0.5228 - binary_accuracy: 0.7550 - val_loss: 0.4888 - val_binary_accuracy: 0.7889\n",
            "Epoch 26/60\n",
            "23428/23428 [==============================] - 0s 10us/step - loss: 0.5221 - binary_accuracy: 0.7549 - val_loss: 0.4868 - val_binary_accuracy: 0.7899\n",
            "Epoch 27/60\n",
            "23428/23428 [==============================] - 0s 10us/step - loss: 0.5205 - binary_accuracy: 0.7582 - val_loss: 0.4864 - val_binary_accuracy: 0.7896\n",
            "Epoch 28/60\n",
            "23428/23428 [==============================] - 0s 10us/step - loss: 0.5173 - binary_accuracy: 0.7584 - val_loss: 0.4838 - val_binary_accuracy: 0.7886\n",
            "Epoch 29/60\n",
            "23428/23428 [==============================] - 0s 10us/step - loss: 0.5136 - binary_accuracy: 0.7581 - val_loss: 0.4814 - val_binary_accuracy: 0.7887\n",
            "Epoch 30/60\n",
            "23428/23428 [==============================] - 0s 9us/step - loss: 0.5128 - binary_accuracy: 0.7597 - val_loss: 0.4807 - val_binary_accuracy: 0.7910\n",
            "Epoch 31/60\n",
            "23428/23428 [==============================] - 0s 9us/step - loss: 0.5141 - binary_accuracy: 0.7593 - val_loss: 0.4792 - val_binary_accuracy: 0.7891\n",
            "Epoch 32/60\n",
            "23428/23428 [==============================] - 0s 10us/step - loss: 0.5106 - binary_accuracy: 0.7628 - val_loss: 0.4786 - val_binary_accuracy: 0.7894\n",
            "Epoch 33/60\n",
            "23428/23428 [==============================] - 0s 9us/step - loss: 0.5109 - binary_accuracy: 0.7580 - val_loss: 0.4772 - val_binary_accuracy: 0.7900\n",
            "Epoch 34/60\n",
            "23428/23428 [==============================] - 0s 9us/step - loss: 0.5070 - binary_accuracy: 0.7617 - val_loss: 0.4758 - val_binary_accuracy: 0.7884\n",
            "Epoch 35/60\n",
            "23428/23428 [==============================] - 0s 9us/step - loss: 0.5066 - binary_accuracy: 0.7599 - val_loss: 0.4740 - val_binary_accuracy: 0.7886\n",
            "Epoch 36/60\n",
            "23428/23428 [==============================] - 0s 9us/step - loss: 0.5073 - binary_accuracy: 0.7605 - val_loss: 0.4724 - val_binary_accuracy: 0.7906\n",
            "Epoch 37/60\n",
            "23428/23428 [==============================] - 0s 9us/step - loss: 0.5054 - binary_accuracy: 0.7599 - val_loss: 0.4723 - val_binary_accuracy: 0.7889\n",
            "Epoch 38/60\n",
            "23428/23428 [==============================] - 0s 9us/step - loss: 0.5020 - binary_accuracy: 0.7631 - val_loss: 0.4706 - val_binary_accuracy: 0.7899\n",
            "Epoch 39/60\n",
            "23428/23428 [==============================] - 0s 9us/step - loss: 0.5012 - binary_accuracy: 0.7635 - val_loss: 0.4694 - val_binary_accuracy: 0.7891\n",
            "Epoch 40/60\n",
            "23428/23428 [==============================] - 0s 9us/step - loss: 0.5018 - binary_accuracy: 0.7640 - val_loss: 0.4689 - val_binary_accuracy: 0.7897\n",
            "Epoch 41/60\n",
            "23428/23428 [==============================] - 0s 10us/step - loss: 0.5019 - binary_accuracy: 0.7629 - val_loss: 0.4686 - val_binary_accuracy: 0.7901\n",
            "Epoch 42/60\n",
            "23428/23428 [==============================] - 0s 9us/step - loss: 0.5015 - binary_accuracy: 0.7654 - val_loss: 0.4670 - val_binary_accuracy: 0.7891\n",
            "Epoch 43/60\n",
            "23428/23428 [==============================] - 0s 9us/step - loss: 0.4975 - binary_accuracy: 0.7629 - val_loss: 0.4672 - val_binary_accuracy: 0.7895\n",
            "Epoch 44/60\n",
            "23428/23428 [==============================] - 0s 9us/step - loss: 0.4990 - binary_accuracy: 0.7614 - val_loss: 0.4657 - val_binary_accuracy: 0.7896\n",
            "Epoch 45/60\n",
            "23428/23428 [==============================] - 0s 10us/step - loss: 0.4984 - binary_accuracy: 0.7639 - val_loss: 0.4650 - val_binary_accuracy: 0.7911\n",
            "Epoch 46/60\n",
            "23428/23428 [==============================] - 0s 10us/step - loss: 0.4999 - binary_accuracy: 0.7614 - val_loss: 0.4648 - val_binary_accuracy: 0.7879\n",
            "Epoch 47/60\n",
            "23428/23428 [==============================] - 0s 9us/step - loss: 0.4960 - binary_accuracy: 0.7660 - val_loss: 0.4633 - val_binary_accuracy: 0.7897\n",
            "Epoch 48/60\n",
            "23428/23428 [==============================] - 0s 9us/step - loss: 0.4948 - binary_accuracy: 0.7662 - val_loss: 0.4630 - val_binary_accuracy: 0.7887\n",
            "Epoch 49/60\n",
            "23428/23428 [==============================] - 0s 9us/step - loss: 0.4953 - binary_accuracy: 0.7629 - val_loss: 0.4631 - val_binary_accuracy: 0.7899\n",
            "Epoch 50/60\n",
            "23428/23428 [==============================] - 0s 9us/step - loss: 0.4967 - binary_accuracy: 0.7646 - val_loss: 0.4625 - val_binary_accuracy: 0.7899\n",
            "Epoch 51/60\n",
            "23428/23428 [==============================] - 0s 10us/step - loss: 0.4949 - binary_accuracy: 0.7647 - val_loss: 0.4611 - val_binary_accuracy: 0.7906\n",
            "Epoch 52/60\n",
            "23428/23428 [==============================] - 0s 9us/step - loss: 0.4965 - binary_accuracy: 0.7646 - val_loss: 0.4616 - val_binary_accuracy: 0.7884\n",
            "Epoch 53/60\n",
            "23428/23428 [==============================] - 0s 10us/step - loss: 0.4944 - binary_accuracy: 0.7631 - val_loss: 0.4612 - val_binary_accuracy: 0.7904\n",
            "Epoch 54/60\n",
            "23428/23428 [==============================] - 0s 9us/step - loss: 0.4914 - binary_accuracy: 0.7669 - val_loss: 0.4596 - val_binary_accuracy: 0.7903\n",
            "Epoch 55/60\n",
            "23428/23428 [==============================] - 0s 9us/step - loss: 0.4918 - binary_accuracy: 0.7671 - val_loss: 0.4598 - val_binary_accuracy: 0.7880\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 56/60\n",
            "23428/23428 [==============================] - 0s 10us/step - loss: 0.4936 - binary_accuracy: 0.7647 - val_loss: 0.4602 - val_binary_accuracy: 0.7875\n",
            "Epoch 57/60\n",
            "23428/23428 [==============================] - 0s 10us/step - loss: 0.4911 - binary_accuracy: 0.7651 - val_loss: 0.4586 - val_binary_accuracy: 0.7893\n",
            "Epoch 58/60\n",
            "23428/23428 [==============================] - 0s 9us/step - loss: 0.4906 - binary_accuracy: 0.7652 - val_loss: 0.4591 - val_binary_accuracy: 0.7887\n",
            "Epoch 59/60\n",
            "23428/23428 [==============================] - 0s 9us/step - loss: 0.4914 - binary_accuracy: 0.7635 - val_loss: 0.4579 - val_binary_accuracy: 0.7901\n",
            "Epoch 60/60\n",
            "23428/23428 [==============================] - 0s 9us/step - loss: 0.4916 - binary_accuracy: 0.7649 - val_loss: 0.4578 - val_binary_accuracy: 0.7895\n",
            "23428/23428 [==============================] - 1s 60us/step\n",
            "8000/8000 [==============================] - 0s 47us/step\n",
            "(23428, 31) (8000, 31) (13470, 31) (23428,) (8000,) (13470,)\n",
            "Train on 23428 samples, validate on 8000 samples\n",
            "Epoch 1/60\n",
            "23428/23428 [==============================] - 0s 11us/step - loss: 1.5111 - binary_accuracy: 0.5102 - val_loss: 1.4728 - val_binary_accuracy: 0.5667\n",
            "Epoch 2/60\n",
            "23428/23428 [==============================] - 0s 10us/step - loss: 1.4325 - binary_accuracy: 0.6048 - val_loss: 1.3666 - val_binary_accuracy: 0.7032\n",
            "Epoch 3/60\n",
            "23428/23428 [==============================] - 0s 10us/step - loss: 1.3238 - binary_accuracy: 0.6348 - val_loss: 1.2493 - val_binary_accuracy: 0.7409\n",
            "Epoch 4/60\n",
            "23428/23428 [==============================] - 0s 10us/step - loss: 1.2097 - binary_accuracy: 0.6864 - val_loss: 1.1292 - val_binary_accuracy: 0.7569\n",
            "Epoch 5/60\n",
            "23428/23428 [==============================] - 0s 10us/step - loss: 1.0979 - binary_accuracy: 0.7089 - val_loss: 1.0124 - val_binary_accuracy: 0.7651\n",
            "Epoch 6/60\n",
            "23428/23428 [==============================] - 0s 10us/step - loss: 0.9943 - binary_accuracy: 0.7260 - val_loss: 0.9090 - val_binary_accuracy: 0.7745\n",
            "Epoch 7/60\n",
            "23428/23428 [==============================] - 0s 10us/step - loss: 0.9017 - binary_accuracy: 0.7370 - val_loss: 0.8214 - val_binary_accuracy: 0.7818\n",
            "Epoch 8/60\n",
            "23428/23428 [==============================] - 0s 8us/step - loss: 0.8249 - binary_accuracy: 0.7422 - val_loss: 0.7508 - val_binary_accuracy: 0.7845\n",
            "Epoch 9/60\n",
            "23428/23428 [==============================] - 0s 9us/step - loss: 0.7595 - binary_accuracy: 0.7465 - val_loss: 0.6937 - val_binary_accuracy: 0.7856\n",
            "Epoch 10/60\n",
            "23428/23428 [==============================] - 0s 9us/step - loss: 0.7091 - binary_accuracy: 0.7483 - val_loss: 0.6471 - val_binary_accuracy: 0.7857\n",
            "Epoch 11/60\n",
            "23428/23428 [==============================] - 0s 9us/step - loss: 0.6636 - binary_accuracy: 0.7507 - val_loss: 0.6094 - val_binary_accuracy: 0.7860\n",
            "Epoch 12/60\n",
            "23428/23428 [==============================] - 0s 9us/step - loss: 0.6302 - binary_accuracy: 0.7503 - val_loss: 0.5798 - val_binary_accuracy: 0.7859\n",
            "Epoch 13/60\n",
            "23428/23428 [==============================] - 0s 9us/step - loss: 0.5994 - binary_accuracy: 0.7554 - val_loss: 0.5561 - val_binary_accuracy: 0.7881\n",
            "Epoch 14/60\n",
            "23428/23428 [==============================] - 0s 9us/step - loss: 0.5803 - binary_accuracy: 0.7537 - val_loss: 0.5403 - val_binary_accuracy: 0.7856\n",
            "Epoch 15/60\n",
            "23428/23428 [==============================] - 0s 9us/step - loss: 0.5666 - binary_accuracy: 0.7550 - val_loss: 0.5290 - val_binary_accuracy: 0.7871\n",
            "Epoch 16/60\n",
            "23428/23428 [==============================] - 0s 10us/step - loss: 0.5588 - binary_accuracy: 0.7526 - val_loss: 0.5214 - val_binary_accuracy: 0.7857\n",
            "Epoch 17/60\n",
            "23428/23428 [==============================] - 0s 9us/step - loss: 0.5510 - binary_accuracy: 0.7531 - val_loss: 0.5149 - val_binary_accuracy: 0.7866\n",
            "Epoch 18/60\n",
            "23428/23428 [==============================] - 0s 9us/step - loss: 0.5460 - binary_accuracy: 0.7513 - val_loss: 0.5110 - val_binary_accuracy: 0.7854\n",
            "Epoch 19/60\n",
            "23428/23428 [==============================] - 0s 9us/step - loss: 0.5402 - binary_accuracy: 0.7534 - val_loss: 0.5063 - val_binary_accuracy: 0.7887\n",
            "Epoch 20/60\n",
            "23428/23428 [==============================] - 0s 10us/step - loss: 0.5378 - binary_accuracy: 0.7499 - val_loss: 0.5027 - val_binary_accuracy: 0.7870\n",
            "Epoch 21/60\n",
            "23428/23428 [==============================] - 0s 9us/step - loss: 0.5368 - binary_accuracy: 0.7540 - val_loss: 0.5011 - val_binary_accuracy: 0.7865\n",
            "Epoch 22/60\n",
            "23428/23428 [==============================] - 0s 9us/step - loss: 0.5298 - binary_accuracy: 0.7569 - val_loss: 0.4982 - val_binary_accuracy: 0.7861\n",
            "Epoch 23/60\n",
            "23428/23428 [==============================] - 0s 9us/step - loss: 0.5258 - binary_accuracy: 0.7588 - val_loss: 0.4955 - val_binary_accuracy: 0.7856\n",
            "Epoch 24/60\n",
            "23428/23428 [==============================] - 0s 9us/step - loss: 0.5274 - binary_accuracy: 0.7544 - val_loss: 0.4940 - val_binary_accuracy: 0.7849\n",
            "Epoch 25/60\n",
            "23428/23428 [==============================] - 0s 9us/step - loss: 0.5240 - binary_accuracy: 0.7567 - val_loss: 0.4915 - val_binary_accuracy: 0.7868\n",
            "Epoch 26/60\n",
            "23428/23428 [==============================] - 0s 9us/step - loss: 0.5234 - binary_accuracy: 0.7580 - val_loss: 0.4896 - val_binary_accuracy: 0.7879\n",
            "Epoch 27/60\n",
            "23428/23428 [==============================] - 0s 9us/step - loss: 0.5211 - binary_accuracy: 0.7585 - val_loss: 0.4883 - val_binary_accuracy: 0.7884\n",
            "Epoch 28/60\n",
            "23428/23428 [==============================] - 0s 9us/step - loss: 0.5207 - binary_accuracy: 0.7579 - val_loss: 0.4872 - val_binary_accuracy: 0.7875\n",
            "Epoch 29/60\n",
            "23428/23428 [==============================] - 0s 9us/step - loss: 0.5186 - binary_accuracy: 0.7583 - val_loss: 0.4857 - val_binary_accuracy: 0.7895\n",
            "Epoch 30/60\n",
            "23428/23428 [==============================] - 0s 8us/step - loss: 0.5163 - binary_accuracy: 0.7588 - val_loss: 0.4846 - val_binary_accuracy: 0.7887\n",
            "Epoch 31/60\n",
            "23428/23428 [==============================] - 0s 9us/step - loss: 0.5149 - binary_accuracy: 0.7594 - val_loss: 0.4826 - val_binary_accuracy: 0.7897\n",
            "Epoch 32/60\n",
            "23428/23428 [==============================] - 0s 9us/step - loss: 0.5149 - binary_accuracy: 0.7576 - val_loss: 0.4813 - val_binary_accuracy: 0.7881\n",
            "Epoch 33/60\n",
            "23428/23428 [==============================] - 0s 9us/step - loss: 0.5136 - binary_accuracy: 0.7612 - val_loss: 0.4815 - val_binary_accuracy: 0.7872\n",
            "Epoch 34/60\n",
            "23428/23428 [==============================] - 0s 9us/step - loss: 0.5106 - binary_accuracy: 0.7616 - val_loss: 0.4796 - val_binary_accuracy: 0.7890\n",
            "Epoch 35/60\n",
            "23428/23428 [==============================] - 0s 9us/step - loss: 0.5103 - binary_accuracy: 0.7595 - val_loss: 0.4786 - val_binary_accuracy: 0.7889\n",
            "Epoch 36/60\n",
            "23428/23428 [==============================] - 0s 9us/step - loss: 0.5092 - binary_accuracy: 0.7594 - val_loss: 0.4772 - val_binary_accuracy: 0.7899\n",
            "Epoch 37/60\n",
            "23428/23428 [==============================] - 0s 9us/step - loss: 0.5075 - binary_accuracy: 0.7615 - val_loss: 0.4760 - val_binary_accuracy: 0.7900\n",
            "Epoch 38/60\n",
            "23428/23428 [==============================] - 0s 9us/step - loss: 0.5083 - binary_accuracy: 0.7611 - val_loss: 0.4753 - val_binary_accuracy: 0.7886\n",
            "Epoch 39/60\n",
            "23428/23428 [==============================] - 0s 8us/step - loss: 0.5077 - binary_accuracy: 0.7585 - val_loss: 0.4749 - val_binary_accuracy: 0.7890\n",
            "Epoch 40/60\n",
            "23428/23428 [==============================] - 0s 9us/step - loss: 0.5069 - binary_accuracy: 0.7610 - val_loss: 0.4733 - val_binary_accuracy: 0.7893\n",
            "Epoch 41/60\n",
            "23428/23428 [==============================] - 0s 9us/step - loss: 0.5038 - binary_accuracy: 0.7602 - val_loss: 0.4719 - val_binary_accuracy: 0.7886\n",
            "Epoch 42/60\n",
            "23428/23428 [==============================] - 0s 9us/step - loss: 0.5037 - binary_accuracy: 0.7639 - val_loss: 0.4713 - val_binary_accuracy: 0.7887\n",
            "Epoch 43/60\n",
            "23428/23428 [==============================] - 0s 9us/step - loss: 0.5023 - binary_accuracy: 0.7614 - val_loss: 0.4711 - val_binary_accuracy: 0.7875\n",
            "Epoch 44/60\n",
            "23428/23428 [==============================] - 0s 9us/step - loss: 0.5049 - binary_accuracy: 0.7637 - val_loss: 0.4712 - val_binary_accuracy: 0.7883\n",
            "Epoch 45/60\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "23428/23428 [==============================] - 0s 10us/step - loss: 0.5014 - binary_accuracy: 0.7626 - val_loss: 0.4701 - val_binary_accuracy: 0.7885\n",
            "Epoch 46/60\n",
            "23428/23428 [==============================] - 0s 9us/step - loss: 0.5008 - binary_accuracy: 0.7633 - val_loss: 0.4683 - val_binary_accuracy: 0.7896\n",
            "Epoch 47/60\n",
            "23428/23428 [==============================] - 0s 10us/step - loss: 0.5014 - binary_accuracy: 0.7619 - val_loss: 0.4684 - val_binary_accuracy: 0.7876\n",
            "Epoch 48/60\n",
            "23428/23428 [==============================] - 0s 9us/step - loss: 0.4995 - binary_accuracy: 0.7628 - val_loss: 0.4682 - val_binary_accuracy: 0.7886\n",
            "Epoch 49/60\n",
            "23428/23428 [==============================] - 0s 10us/step - loss: 0.4994 - binary_accuracy: 0.7622 - val_loss: 0.4665 - val_binary_accuracy: 0.7894\n",
            "Epoch 50/60\n",
            "23428/23428 [==============================] - 0s 10us/step - loss: 0.5016 - binary_accuracy: 0.7617 - val_loss: 0.4669 - val_binary_accuracy: 0.7886\n",
            "Epoch 51/60\n",
            "23428/23428 [==============================] - 0s 10us/step - loss: 0.4984 - binary_accuracy: 0.7646 - val_loss: 0.4658 - val_binary_accuracy: 0.7897\n",
            "Epoch 52/60\n",
            "23428/23428 [==============================] - 0s 8us/step - loss: 0.4991 - binary_accuracy: 0.7633 - val_loss: 0.4649 - val_binary_accuracy: 0.7906\n",
            "Epoch 53/60\n",
            "23428/23428 [==============================] - 0s 9us/step - loss: 0.4973 - binary_accuracy: 0.7625 - val_loss: 0.4644 - val_binary_accuracy: 0.7897\n",
            "Epoch 54/60\n",
            "23428/23428 [==============================] - 0s 9us/step - loss: 0.4952 - binary_accuracy: 0.7619 - val_loss: 0.4639 - val_binary_accuracy: 0.7904\n",
            "Epoch 55/60\n",
            "23428/23428 [==============================] - 0s 9us/step - loss: 0.4968 - binary_accuracy: 0.7644 - val_loss: 0.4639 - val_binary_accuracy: 0.7874\n",
            "Epoch 56/60\n",
            "23428/23428 [==============================] - 0s 8us/step - loss: 0.4955 - binary_accuracy: 0.7628 - val_loss: 0.4634 - val_binary_accuracy: 0.7885\n",
            "Epoch 57/60\n",
            "23428/23428 [==============================] - 0s 10us/step - loss: 0.4936 - binary_accuracy: 0.7657 - val_loss: 0.4620 - val_binary_accuracy: 0.7894\n",
            "Epoch 58/60\n",
            "23428/23428 [==============================] - 0s 8us/step - loss: 0.4964 - binary_accuracy: 0.7629 - val_loss: 0.4616 - val_binary_accuracy: 0.7889\n",
            "Epoch 59/60\n",
            "23428/23428 [==============================] - 0s 10us/step - loss: 0.4956 - binary_accuracy: 0.7644 - val_loss: 0.4622 - val_binary_accuracy: 0.7884\n",
            "Epoch 60/60\n",
            "23428/23428 [==============================] - 0s 9us/step - loss: 0.4939 - binary_accuracy: 0.7636 - val_loss: 0.4613 - val_binary_accuracy: 0.7883\n",
            "23428/23428 [==============================] - 2s 69us/step\n",
            "8000/8000 [==============================] - 1s 71us/step\n",
            "\n",
            "\n",
            "Ensayando modelo con estructura [512, 10, 1] y optimizador <keras.optimizers.RMSprop object at 0x00000180021B5C48>\n",
            "(23428, 31) (8000, 31) (13470, 31) (23428,) (8000,) (13470,)\n",
            "Train on 23428 samples, validate on 8000 samples\n",
            "Epoch 1/60\n",
            "23428/23428 [==============================] - 1s 53us/step - loss: 1.3957 - binary_accuracy: 0.5677 - val_loss: 1.2712 - val_binary_accuracy: 0.6926\n",
            "Epoch 2/60\n",
            "23428/23428 [==============================] - 0s 11us/step - loss: 1.2110 - binary_accuracy: 0.6598 - val_loss: 1.1221 - val_binary_accuracy: 0.7632\n",
            "Epoch 3/60\n",
            "23428/23428 [==============================] - 0s 10us/step - loss: 1.0788 - binary_accuracy: 0.6979 - val_loss: 1.0008 - val_binary_accuracy: 0.7691\n",
            "Epoch 4/60\n",
            "23428/23428 [==============================] - 0s 9us/step - loss: 0.9703 - binary_accuracy: 0.7084 - val_loss: 0.8977 - val_binary_accuracy: 0.7735\n",
            "Epoch 5/60\n",
            "23428/23428 [==============================] - 0s 10us/step - loss: 0.8820 - binary_accuracy: 0.7179 - val_loss: 0.8120 - val_binary_accuracy: 0.7740\n",
            "Epoch 6/60\n",
            "23428/23428 [==============================] - 0s 10us/step - loss: 0.8024 - binary_accuracy: 0.7303 - val_loss: 0.7396 - val_binary_accuracy: 0.7818\n",
            "Epoch 7/60\n",
            "23428/23428 [==============================] - 0s 11us/step - loss: 0.7406 - binary_accuracy: 0.7314 - val_loss: 0.6817 - val_binary_accuracy: 0.7810\n",
            "Epoch 8/60\n",
            "23428/23428 [==============================] - 0s 10us/step - loss: 0.6886 - binary_accuracy: 0.7311 - val_loss: 0.6339 - val_binary_accuracy: 0.7764\n",
            "Epoch 9/60\n",
            "23428/23428 [==============================] - 0s 11us/step - loss: 0.6481 - binary_accuracy: 0.7352 - val_loss: 0.5962 - val_binary_accuracy: 0.7779\n",
            "Epoch 10/60\n",
            "23428/23428 [==============================] - 0s 9us/step - loss: 0.6134 - binary_accuracy: 0.7399 - val_loss: 0.5720 - val_binary_accuracy: 0.7697\n",
            "Epoch 11/60\n",
            "23428/23428 [==============================] - 0s 11us/step - loss: 0.5922 - binary_accuracy: 0.7428 - val_loss: 0.5518 - val_binary_accuracy: 0.7757\n",
            "Epoch 12/60\n",
            "23428/23428 [==============================] - 0s 10us/step - loss: 0.5801 - binary_accuracy: 0.7357 - val_loss: 0.5424 - val_binary_accuracy: 0.7790\n",
            "Epoch 13/60\n",
            "23428/23428 [==============================] - 0s 9us/step - loss: 0.5728 - binary_accuracy: 0.7426 - val_loss: 0.5358 - val_binary_accuracy: 0.7780\n",
            "Epoch 14/60\n",
            "23428/23428 [==============================] - 0s 10us/step - loss: 0.5658 - binary_accuracy: 0.7459 - val_loss: 0.5314 - val_binary_accuracy: 0.7803\n",
            "Epoch 15/60\n",
            "23428/23428 [==============================] - 0s 10us/step - loss: 0.5636 - binary_accuracy: 0.7447 - val_loss: 0.5278 - val_binary_accuracy: 0.7770\n",
            "Epoch 16/60\n",
            "23428/23428 [==============================] - 0s 9us/step - loss: 0.5567 - binary_accuracy: 0.7499 - val_loss: 0.5225 - val_binary_accuracy: 0.7784\n",
            "Epoch 17/60\n",
            "23428/23428 [==============================] - 0s 11us/step - loss: 0.5557 - binary_accuracy: 0.7484 - val_loss: 0.5216 - val_binary_accuracy: 0.7800\n",
            "Epoch 18/60\n",
            "23428/23428 [==============================] - 0s 9us/step - loss: 0.5531 - binary_accuracy: 0.7517 - val_loss: 0.5164 - val_binary_accuracy: 0.7821\n",
            "Epoch 19/60\n",
            "23428/23428 [==============================] - 0s 9us/step - loss: 0.5509 - binary_accuracy: 0.7492 - val_loss: 0.5140 - val_binary_accuracy: 0.7859\n",
            "Epoch 20/60\n",
            "23428/23428 [==============================] - 0s 10us/step - loss: 0.5492 - binary_accuracy: 0.7482 - val_loss: 0.5136 - val_binary_accuracy: 0.7781\n",
            "Epoch 21/60\n",
            "23428/23428 [==============================] - 0s 11us/step - loss: 0.5439 - binary_accuracy: 0.7526 - val_loss: 0.5087 - val_binary_accuracy: 0.7829\n",
            "Epoch 22/60\n",
            "23428/23428 [==============================] - 0s 11us/step - loss: 0.5423 - binary_accuracy: 0.7531 - val_loss: 0.5092 - val_binary_accuracy: 0.7793\n",
            "Epoch 23/60\n",
            "23428/23428 [==============================] - 0s 11us/step - loss: 0.5403 - binary_accuracy: 0.7548 - val_loss: 0.5064 - val_binary_accuracy: 0.7776\n",
            "Epoch 24/60\n",
            "23428/23428 [==============================] - 0s 11us/step - loss: 0.5390 - binary_accuracy: 0.7524 - val_loss: 0.5046 - val_binary_accuracy: 0.7786\n",
            "Epoch 25/60\n",
            "23428/23428 [==============================] - 0s 12us/step - loss: 0.5365 - binary_accuracy: 0.7558 - val_loss: 0.5000 - val_binary_accuracy: 0.7851\n",
            "Epoch 26/60\n",
            "23428/23428 [==============================] - 0s 12us/step - loss: 0.5326 - binary_accuracy: 0.7568 - val_loss: 0.4988 - val_binary_accuracy: 0.7815\n",
            "Epoch 27/60\n",
            "23428/23428 [==============================] - 0s 12us/step - loss: 0.5325 - binary_accuracy: 0.7561 - val_loss: 0.4980 - val_binary_accuracy: 0.7790\n",
            "Epoch 28/60\n",
            "23428/23428 [==============================] - 0s 11us/step - loss: 0.5307 - binary_accuracy: 0.7544 - val_loss: 0.4977 - val_binary_accuracy: 0.7805\n",
            "Epoch 29/60\n",
            "23428/23428 [==============================] - 0s 10us/step - loss: 0.5297 - binary_accuracy: 0.7556 - val_loss: 0.4932 - val_binary_accuracy: 0.7884\n",
            "Epoch 30/60\n",
            "23428/23428 [==============================] - 0s 12us/step - loss: 0.5257 - binary_accuracy: 0.7585 - val_loss: 0.4911 - val_binary_accuracy: 0.7870\n",
            "Epoch 31/60\n",
            "23428/23428 [==============================] - 0s 12us/step - loss: 0.5257 - binary_accuracy: 0.7578 - val_loss: 0.4899 - val_binary_accuracy: 0.7871\n",
            "Epoch 32/60\n",
            "23428/23428 [==============================] - 0s 10us/step - loss: 0.5246 - binary_accuracy: 0.7568 - val_loss: 0.4895 - val_binary_accuracy: 0.7872\n",
            "Epoch 33/60\n",
            "23428/23428 [==============================] - 0s 9us/step - loss: 0.5235 - binary_accuracy: 0.7590 - val_loss: 0.4900 - val_binary_accuracy: 0.7820\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 34/60\n",
            "23428/23428 [==============================] - 0s 9us/step - loss: 0.5212 - binary_accuracy: 0.7585 - val_loss: 0.4890 - val_binary_accuracy: 0.7818\n",
            "Epoch 35/60\n",
            "23428/23428 [==============================] - 0s 8us/step - loss: 0.5188 - binary_accuracy: 0.7615 - val_loss: 0.4857 - val_binary_accuracy: 0.7887\n",
            "Epoch 36/60\n",
            "23428/23428 [==============================] - 0s 9us/step - loss: 0.5193 - binary_accuracy: 0.7593 - val_loss: 0.4869 - val_binary_accuracy: 0.7843\n",
            "Epoch 37/60\n",
            "23428/23428 [==============================] - 0s 11us/step - loss: 0.5198 - binary_accuracy: 0.7605 - val_loss: 0.4836 - val_binary_accuracy: 0.7891\n",
            "Epoch 38/60\n",
            "23428/23428 [==============================] - 0s 10us/step - loss: 0.5170 - binary_accuracy: 0.7621 - val_loss: 0.4841 - val_binary_accuracy: 0.7855\n",
            "Epoch 39/60\n",
            "23428/23428 [==============================] - 0s 10us/step - loss: 0.5173 - binary_accuracy: 0.7607 - val_loss: 0.4849 - val_binary_accuracy: 0.7849\n",
            "Epoch 40/60\n",
            "23428/23428 [==============================] - 0s 11us/step - loss: 0.5160 - binary_accuracy: 0.7608 - val_loss: 0.4867 - val_binary_accuracy: 0.7795\n",
            "Epoch 41/60\n",
            "23428/23428 [==============================] - 0s 10us/step - loss: 0.5146 - binary_accuracy: 0.7644 - val_loss: 0.4834 - val_binary_accuracy: 0.7807\n",
            "Epoch 42/60\n",
            "23428/23428 [==============================] - 0s 10us/step - loss: 0.5130 - binary_accuracy: 0.7628 - val_loss: 0.4789 - val_binary_accuracy: 0.7825\n",
            "Epoch 43/60\n",
            "23428/23428 [==============================] - 0s 10us/step - loss: 0.5132 - binary_accuracy: 0.7592 - val_loss: 0.4773 - val_binary_accuracy: 0.7919\n",
            "Epoch 44/60\n",
            "23428/23428 [==============================] - 0s 9us/step - loss: 0.5127 - binary_accuracy: 0.7611 - val_loss: 0.4781 - val_binary_accuracy: 0.7874\n",
            "Epoch 45/60\n",
            "23428/23428 [==============================] - 0s 9us/step - loss: 0.5108 - binary_accuracy: 0.7616 - val_loss: 0.4789 - val_binary_accuracy: 0.7832\n",
            "Epoch 46/60\n",
            "23428/23428 [==============================] - 0s 9us/step - loss: 0.5113 - binary_accuracy: 0.7619 - val_loss: 0.4791 - val_binary_accuracy: 0.7824\n",
            "Epoch 47/60\n",
            "23428/23428 [==============================] - 0s 10us/step - loss: 0.5120 - binary_accuracy: 0.7601 - val_loss: 0.4784 - val_binary_accuracy: 0.7820\n",
            "Epoch 48/60\n",
            "23428/23428 [==============================] - 0s 9us/step - loss: 0.5109 - binary_accuracy: 0.7596 - val_loss: 0.4747 - val_binary_accuracy: 0.7879\n",
            "Epoch 49/60\n",
            "23428/23428 [==============================] - 0s 10us/step - loss: 0.5104 - binary_accuracy: 0.7580 - val_loss: 0.4755 - val_binary_accuracy: 0.7875\n",
            "Epoch 50/60\n",
            "23428/23428 [==============================] - 0s 10us/step - loss: 0.5061 - binary_accuracy: 0.7603 - val_loss: 0.4735 - val_binary_accuracy: 0.7885\n",
            "Epoch 51/60\n",
            "23428/23428 [==============================] - 0s 9us/step - loss: 0.5090 - binary_accuracy: 0.7604 - val_loss: 0.4734 - val_binary_accuracy: 0.7886\n",
            "Epoch 52/60\n",
            "23428/23428 [==============================] - 0s 10us/step - loss: 0.5073 - binary_accuracy: 0.7607 - val_loss: 0.4743 - val_binary_accuracy: 0.7800\n",
            "Epoch 53/60\n",
            "23428/23428 [==============================] - 0s 10us/step - loss: 0.5073 - binary_accuracy: 0.7581 - val_loss: 0.4715 - val_binary_accuracy: 0.7910\n",
            "Epoch 54/60\n",
            "23428/23428 [==============================] - 0s 9us/step - loss: 0.5047 - binary_accuracy: 0.7622 - val_loss: 0.4708 - val_binary_accuracy: 0.7895\n",
            "Epoch 55/60\n",
            "23428/23428 [==============================] - 0s 10us/step - loss: 0.5037 - binary_accuracy: 0.7611 - val_loss: 0.4717 - val_binary_accuracy: 0.7864\n",
            "Epoch 56/60\n",
            "23428/23428 [==============================] - 0s 10us/step - loss: 0.5015 - binary_accuracy: 0.7629 - val_loss: 0.4697 - val_binary_accuracy: 0.7896\n",
            "Epoch 57/60\n",
            "23428/23428 [==============================] - 0s 10us/step - loss: 0.5020 - binary_accuracy: 0.7628 - val_loss: 0.4695 - val_binary_accuracy: 0.7893\n",
            "Epoch 58/60\n",
            "23428/23428 [==============================] - 0s 10us/step - loss: 0.5034 - binary_accuracy: 0.7614 - val_loss: 0.4687 - val_binary_accuracy: 0.7889\n",
            "Epoch 59/60\n",
            "23428/23428 [==============================] - 0s 10us/step - loss: 0.5036 - binary_accuracy: 0.7615 - val_loss: 0.4692 - val_binary_accuracy: 0.7786\n",
            "Epoch 60/60\n",
            "23428/23428 [==============================] - 0s 10us/step - loss: 0.5000 - binary_accuracy: 0.7641 - val_loss: 0.4686 - val_binary_accuracy: 0.7875\n",
            "23428/23428 [==============================] - 1s 57us/step\n",
            "8000/8000 [==============================] - 0s 47us/step\n",
            "(23428, 31) (8000, 31) (13470, 31) (23428,) (8000,) (13470,)\n",
            "Train on 23428 samples, validate on 8000 samples\n",
            "Epoch 1/60\n",
            "23428/23428 [==============================] - 0s 11us/step - loss: 1.4612 - binary_accuracy: 0.5569 - val_loss: 1.3820 - val_binary_accuracy: 0.5849\n",
            "Epoch 2/60\n",
            "23428/23428 [==============================] - 0s 9us/step - loss: 1.3300 - binary_accuracy: 0.6368 - val_loss: 1.2508 - val_binary_accuracy: 0.7506\n",
            "Epoch 3/60\n",
            "23428/23428 [==============================] - 0s 10us/step - loss: 1.2067 - binary_accuracy: 0.6915 - val_loss: 1.1279 - val_binary_accuracy: 0.7670\n",
            "Epoch 4/60\n",
            "23428/23428 [==============================] - 0s 8us/step - loss: 1.0936 - binary_accuracy: 0.7069 - val_loss: 1.0160 - val_binary_accuracy: 0.7714\n",
            "Epoch 5/60\n",
            "23428/23428 [==============================] - 0s 8us/step - loss: 0.9900 - binary_accuracy: 0.7162 - val_loss: 0.9150 - val_binary_accuracy: 0.7790\n",
            "Epoch 6/60\n",
            "23428/23428 [==============================] - 0s 9us/step - loss: 0.8986 - binary_accuracy: 0.7257 - val_loss: 0.8272 - val_binary_accuracy: 0.7806\n",
            "Epoch 7/60\n",
            "23428/23428 [==============================] - 0s 9us/step - loss: 0.8200 - binary_accuracy: 0.7294 - val_loss: 0.7537 - val_binary_accuracy: 0.7828\n",
            "Epoch 8/60\n",
            "23428/23428 [==============================] - 0s 9us/step - loss: 0.7548 - binary_accuracy: 0.7337 - val_loss: 0.6928 - val_binary_accuracy: 0.7810\n",
            "Epoch 9/60\n",
            "23428/23428 [==============================] - 0s 10us/step - loss: 0.7002 - binary_accuracy: 0.7393 - val_loss: 0.6430 - val_binary_accuracy: 0.7825\n",
            "Epoch 10/60\n",
            "23428/23428 [==============================] - 0s 9us/step - loss: 0.6573 - binary_accuracy: 0.7363 - val_loss: 0.6043 - val_binary_accuracy: 0.7826\n",
            "Epoch 11/60\n",
            "23428/23428 [==============================] - 0s 9us/step - loss: 0.6234 - binary_accuracy: 0.7375 - val_loss: 0.5740 - val_binary_accuracy: 0.7785\n",
            "Epoch 12/60\n",
            "23428/23428 [==============================] - 0s 10us/step - loss: 0.5961 - binary_accuracy: 0.7416 - val_loss: 0.5555 - val_binary_accuracy: 0.7784\n",
            "Epoch 13/60\n",
            "23428/23428 [==============================] - 0s 9us/step - loss: 0.5808 - binary_accuracy: 0.7442 - val_loss: 0.5437 - val_binary_accuracy: 0.7739\n",
            "Epoch 14/60\n",
            "23428/23428 [==============================] - 0s 10us/step - loss: 0.5699 - binary_accuracy: 0.7455 - val_loss: 0.5342 - val_binary_accuracy: 0.7788\n",
            "Epoch 15/60\n",
            "23428/23428 [==============================] - 0s 10us/step - loss: 0.5653 - binary_accuracy: 0.7453 - val_loss: 0.5324 - val_binary_accuracy: 0.7771\n",
            "Epoch 16/60\n",
            "23428/23428 [==============================] - 0s 10us/step - loss: 0.5605 - binary_accuracy: 0.7465 - val_loss: 0.5261 - val_binary_accuracy: 0.7821\n",
            "Epoch 17/60\n",
            "23428/23428 [==============================] - 0s 10us/step - loss: 0.5566 - binary_accuracy: 0.7497 - val_loss: 0.5265 - val_binary_accuracy: 0.7803\n",
            "Epoch 18/60\n",
            "23428/23428 [==============================] - 0s 9us/step - loss: 0.5524 - binary_accuracy: 0.7524 - val_loss: 0.5187 - val_binary_accuracy: 0.7753\n",
            "Epoch 19/60\n",
            "23428/23428 [==============================] - 0s 9us/step - loss: 0.5526 - binary_accuracy: 0.7514 - val_loss: 0.5158 - val_binary_accuracy: 0.7859\n",
            "Epoch 20/60\n",
            "23428/23428 [==============================] - 0s 9us/step - loss: 0.5482 - binary_accuracy: 0.7513 - val_loss: 0.5140 - val_binary_accuracy: 0.7818\n",
            "Epoch 21/60\n",
            "23428/23428 [==============================] - 0s 9us/step - loss: 0.5462 - binary_accuracy: 0.7535 - val_loss: 0.5105 - val_binary_accuracy: 0.7794\n",
            "Epoch 22/60\n",
            "23428/23428 [==============================] - 0s 9us/step - loss: 0.5431 - binary_accuracy: 0.7547 - val_loss: 0.5082 - val_binary_accuracy: 0.7814\n",
            "Epoch 23/60\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "23428/23428 [==============================] - 0s 10us/step - loss: 0.5432 - binary_accuracy: 0.7513 - val_loss: 0.5058 - val_binary_accuracy: 0.7872\n",
            "Epoch 24/60\n",
            "23428/23428 [==============================] - 0s 10us/step - loss: 0.5382 - binary_accuracy: 0.7551 - val_loss: 0.5029 - val_binary_accuracy: 0.7821\n",
            "Epoch 25/60\n",
            "23428/23428 [==============================] - 0s 11us/step - loss: 0.5363 - binary_accuracy: 0.7527 - val_loss: 0.5070 - val_binary_accuracy: 0.7794\n",
            "Epoch 26/60\n",
            "23428/23428 [==============================] - 0s 10us/step - loss: 0.5339 - binary_accuracy: 0.7558 - val_loss: 0.5003 - val_binary_accuracy: 0.7889\n",
            "Epoch 27/60\n",
            "23428/23428 [==============================] - 0s 10us/step - loss: 0.5332 - binary_accuracy: 0.7544 - val_loss: 0.4981 - val_binary_accuracy: 0.7870\n",
            "Epoch 28/60\n",
            "23428/23428 [==============================] - 0s 10us/step - loss: 0.5307 - binary_accuracy: 0.7579 - val_loss: 0.4962 - val_binary_accuracy: 0.7820\n",
            "Epoch 29/60\n",
            "23428/23428 [==============================] - 0s 10us/step - loss: 0.5307 - binary_accuracy: 0.7567 - val_loss: 0.4947 - val_binary_accuracy: 0.7868\n",
            "Epoch 30/60\n",
            "23428/23428 [==============================] - 0s 10us/step - loss: 0.5280 - binary_accuracy: 0.7568 - val_loss: 0.4971 - val_binary_accuracy: 0.7765\n",
            "Epoch 31/60\n",
            "23428/23428 [==============================] - 0s 10us/step - loss: 0.5286 - binary_accuracy: 0.7580 - val_loss: 0.4928 - val_binary_accuracy: 0.7865\n",
            "Epoch 32/60\n",
            "23428/23428 [==============================] - 0s 10us/step - loss: 0.5278 - binary_accuracy: 0.7541 - val_loss: 0.4909 - val_binary_accuracy: 0.7881\n",
            "Epoch 33/60\n",
            "23428/23428 [==============================] - 0s 10us/step - loss: 0.5269 - binary_accuracy: 0.7593 - val_loss: 0.4925 - val_binary_accuracy: 0.7800\n",
            "Epoch 34/60\n",
            "23428/23428 [==============================] - 0s 10us/step - loss: 0.5223 - binary_accuracy: 0.7597 - val_loss: 0.4869 - val_binary_accuracy: 0.7889\n",
            "Epoch 35/60\n",
            "23428/23428 [==============================] - 0s 11us/step - loss: 0.5233 - binary_accuracy: 0.7578 - val_loss: 0.4876 - val_binary_accuracy: 0.7884\n",
            "Epoch 36/60\n",
            "23428/23428 [==============================] - 0s 9us/step - loss: 0.5193 - binary_accuracy: 0.7589 - val_loss: 0.4864 - val_binary_accuracy: 0.7814\n",
            "Epoch 37/60\n",
            "23428/23428 [==============================] - 0s 10us/step - loss: 0.5214 - binary_accuracy: 0.7590 - val_loss: 0.4861 - val_binary_accuracy: 0.7843\n",
            "Epoch 38/60\n",
            "23428/23428 [==============================] - 0s 10us/step - loss: 0.5190 - binary_accuracy: 0.7607 - val_loss: 0.4868 - val_binary_accuracy: 0.7800\n",
            "Epoch 39/60\n",
            "23428/23428 [==============================] - 0s 10us/step - loss: 0.5186 - binary_accuracy: 0.7582 - val_loss: 0.4869 - val_binary_accuracy: 0.7815\n",
            "Epoch 40/60\n",
            "23428/23428 [==============================] - 0s 10us/step - loss: 0.5190 - binary_accuracy: 0.7602 - val_loss: 0.4898 - val_binary_accuracy: 0.7770\n",
            "Epoch 41/60\n",
            "23428/23428 [==============================] - 0s 10us/step - loss: 0.5177 - binary_accuracy: 0.7581 - val_loss: 0.4816 - val_binary_accuracy: 0.7876\n",
            "Epoch 42/60\n",
            "23428/23428 [==============================] - 0s 10us/step - loss: 0.5146 - binary_accuracy: 0.7611 - val_loss: 0.4818 - val_binary_accuracy: 0.7866\n",
            "Epoch 43/60\n",
            "23428/23428 [==============================] - 0s 10us/step - loss: 0.5142 - binary_accuracy: 0.7620 - val_loss: 0.4803 - val_binary_accuracy: 0.7857\n",
            "Epoch 44/60\n",
            "23428/23428 [==============================] - 0s 10us/step - loss: 0.5136 - binary_accuracy: 0.7618 - val_loss: 0.4787 - val_binary_accuracy: 0.7880\n",
            "Epoch 45/60\n",
            "23428/23428 [==============================] - 0s 9us/step - loss: 0.5118 - binary_accuracy: 0.7590 - val_loss: 0.4773 - val_binary_accuracy: 0.7883\n",
            "Epoch 46/60\n",
            "23428/23428 [==============================] - 0s 10us/step - loss: 0.5122 - binary_accuracy: 0.7584 - val_loss: 0.4790 - val_binary_accuracy: 0.7834\n",
            "Epoch 47/60\n",
            "23428/23428 [==============================] - 0s 10us/step - loss: 0.5111 - binary_accuracy: 0.7579 - val_loss: 0.4766 - val_binary_accuracy: 0.7879\n",
            "Epoch 48/60\n",
            "23428/23428 [==============================] - 0s 10us/step - loss: 0.5104 - binary_accuracy: 0.7592 - val_loss: 0.4763 - val_binary_accuracy: 0.7890\n",
            "Epoch 49/60\n",
            "23428/23428 [==============================] - 0s 9us/step - loss: 0.5084 - binary_accuracy: 0.7588 - val_loss: 0.4765 - val_binary_accuracy: 0.7866\n",
            "Epoch 50/60\n",
            "23428/23428 [==============================] - 0s 9us/step - loss: 0.5096 - binary_accuracy: 0.7611 - val_loss: 0.4767 - val_binary_accuracy: 0.7834\n",
            "Epoch 51/60\n",
            "23428/23428 [==============================] - 0s 10us/step - loss: 0.5091 - binary_accuracy: 0.7582 - val_loss: 0.4771 - val_binary_accuracy: 0.7791\n",
            "Epoch 52/60\n",
            "23428/23428 [==============================] - 0s 9us/step - loss: 0.5057 - binary_accuracy: 0.7608 - val_loss: 0.4717 - val_binary_accuracy: 0.7903\n",
            "Epoch 53/60\n",
            "23428/23428 [==============================] - 0s 10us/step - loss: 0.5050 - binary_accuracy: 0.7620 - val_loss: 0.4721 - val_binary_accuracy: 0.7883\n",
            "Epoch 54/60\n",
            "23428/23428 [==============================] - 0s 9us/step - loss: 0.5037 - binary_accuracy: 0.7627 - val_loss: 0.4762 - val_binary_accuracy: 0.7790\n",
            "Epoch 55/60\n",
            "23428/23428 [==============================] - 0s 9us/step - loss: 0.5043 - binary_accuracy: 0.7613 - val_loss: 0.4721 - val_binary_accuracy: 0.7891\n",
            "Epoch 56/60\n",
            "23428/23428 [==============================] - 0s 9us/step - loss: 0.5037 - binary_accuracy: 0.7643 - val_loss: 0.4716 - val_binary_accuracy: 0.7878\n",
            "Epoch 57/60\n",
            "23428/23428 [==============================] - 0s 9us/step - loss: 0.5043 - binary_accuracy: 0.7631 - val_loss: 0.4736 - val_binary_accuracy: 0.7831\n",
            "Epoch 58/60\n",
            "23428/23428 [==============================] - 0s 9us/step - loss: 0.5049 - binary_accuracy: 0.7620 - val_loss: 0.4702 - val_binary_accuracy: 0.7870\n",
            "Epoch 59/60\n",
            "23428/23428 [==============================] - 0s 9us/step - loss: 0.5040 - binary_accuracy: 0.7631 - val_loss: 0.4715 - val_binary_accuracy: 0.7846\n",
            "Epoch 60/60\n",
            "23428/23428 [==============================] - 0s 10us/step - loss: 0.5026 - binary_accuracy: 0.7637 - val_loss: 0.4697 - val_binary_accuracy: 0.7872\n",
            "23428/23428 [==============================] - 1s 44us/step\n",
            "8000/8000 [==============================] - 0s 47us/step\n",
            "(23428, 31) (8000, 31) (13470, 31) (23428,) (8000,) (13470,)\n",
            "Train on 23428 samples, validate on 8000 samples\n",
            "Epoch 1/60\n",
            "23428/23428 [==============================] - 0s 12us/step - loss: 1.4621 - binary_accuracy: 0.5668 - val_loss: 1.3834 - val_binary_accuracy: 0.6580\n",
            "Epoch 2/60\n",
            "23428/23428 [==============================] - 0s 10us/step - loss: 1.3327 - binary_accuracy: 0.6409 - val_loss: 1.2538 - val_binary_accuracy: 0.7476\n",
            "Epoch 3/60\n",
            "23428/23428 [==============================] - 0s 9us/step - loss: 1.2096 - binary_accuracy: 0.6839 - val_loss: 1.1317 - val_binary_accuracy: 0.7655\n",
            "Epoch 4/60\n",
            "23428/23428 [==============================] - 0s 9us/step - loss: 1.0956 - binary_accuracy: 0.7051 - val_loss: 1.0191 - val_binary_accuracy: 0.7685\n",
            "Epoch 5/60\n",
            "23428/23428 [==============================] - 0s 10us/step - loss: 0.9907 - binary_accuracy: 0.7145 - val_loss: 0.9171 - val_binary_accuracy: 0.7782\n",
            "Epoch 6/60\n",
            "23428/23428 [==============================] - 0s 10us/step - loss: 0.9010 - binary_accuracy: 0.7240 - val_loss: 0.8291 - val_binary_accuracy: 0.7769\n",
            "Epoch 7/60\n",
            "23428/23428 [==============================] - 0s 9us/step - loss: 0.8227 - binary_accuracy: 0.7269 - val_loss: 0.7552 - val_binary_accuracy: 0.7751\n",
            "Epoch 8/60\n",
            "23428/23428 [==============================] - 0s 9us/step - loss: 0.7559 - binary_accuracy: 0.7325 - val_loss: 0.6937 - val_binary_accuracy: 0.7747\n",
            "Epoch 9/60\n",
            "23428/23428 [==============================] - 0s 10us/step - loss: 0.7011 - binary_accuracy: 0.7354 - val_loss: 0.6489 - val_binary_accuracy: 0.7744\n",
            "Epoch 10/60\n",
            "23428/23428 [==============================] - 0s 10us/step - loss: 0.6557 - binary_accuracy: 0.7376 - val_loss: 0.6046 - val_binary_accuracy: 0.7769\n",
            "Epoch 11/60\n",
            "23428/23428 [==============================] - 0s 10us/step - loss: 0.6231 - binary_accuracy: 0.7385 - val_loss: 0.5745 - val_binary_accuracy: 0.7774\n",
            "Epoch 12/60\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "23428/23428 [==============================] - 0s 10us/step - loss: 0.5974 - binary_accuracy: 0.7424 - val_loss: 0.5537 - val_binary_accuracy: 0.7769\n",
            "Epoch 13/60\n",
            "23428/23428 [==============================] - 0s 11us/step - loss: 0.5817 - binary_accuracy: 0.7411 - val_loss: 0.5464 - val_binary_accuracy: 0.7790\n",
            "Epoch 14/60\n",
            "23428/23428 [==============================] - 0s 10us/step - loss: 0.5728 - binary_accuracy: 0.7439 - val_loss: 0.5354 - val_binary_accuracy: 0.7772\n",
            "Epoch 15/60\n",
            "23428/23428 [==============================] - 0s 10us/step - loss: 0.5635 - binary_accuracy: 0.7467 - val_loss: 0.5323 - val_binary_accuracy: 0.7789\n",
            "Epoch 16/60\n",
            "23428/23428 [==============================] - 0s 10us/step - loss: 0.5640 - binary_accuracy: 0.7489 - val_loss: 0.5262 - val_binary_accuracy: 0.7810\n",
            "Epoch 17/60\n",
            "23428/23428 [==============================] - 0s 10us/step - loss: 0.5595 - binary_accuracy: 0.7488 - val_loss: 0.5229 - val_binary_accuracy: 0.7849\n",
            "Epoch 18/60\n",
            "23428/23428 [==============================] - 0s 10us/step - loss: 0.5562 - binary_accuracy: 0.7465 - val_loss: 0.5203 - val_binary_accuracy: 0.7788\n",
            "Epoch 19/60\n",
            "23428/23428 [==============================] - 0s 10us/step - loss: 0.5517 - binary_accuracy: 0.7508 - val_loss: 0.5168 - val_binary_accuracy: 0.7824\n",
            "Epoch 20/60\n",
            "23428/23428 [==============================] - 0s 11us/step - loss: 0.5491 - binary_accuracy: 0.7484 - val_loss: 0.5128 - val_binary_accuracy: 0.7807\n",
            "Epoch 21/60\n",
            "23428/23428 [==============================] - 0s 10us/step - loss: 0.5456 - binary_accuracy: 0.7512 - val_loss: 0.5104 - val_binary_accuracy: 0.7812\n",
            "Epoch 22/60\n",
            "23428/23428 [==============================] - 0s 10us/step - loss: 0.5450 - binary_accuracy: 0.7505 - val_loss: 0.5077 - val_binary_accuracy: 0.7844\n",
            "Epoch 23/60\n",
            "23428/23428 [==============================] - 0s 11us/step - loss: 0.5405 - binary_accuracy: 0.7535 - val_loss: 0.5078 - val_binary_accuracy: 0.7794\n",
            "Epoch 24/60\n",
            "23428/23428 [==============================] - 0s 10us/step - loss: 0.5396 - binary_accuracy: 0.7551 - val_loss: 0.5054 - val_binary_accuracy: 0.7811\n",
            "Epoch 25/60\n",
            "23428/23428 [==============================] - 0s 11us/step - loss: 0.5383 - binary_accuracy: 0.7534 - val_loss: 0.5016 - val_binary_accuracy: 0.7831\n",
            "Epoch 26/60\n",
            "23428/23428 [==============================] - 0s 10us/step - loss: 0.5340 - binary_accuracy: 0.7566 - val_loss: 0.4993 - val_binary_accuracy: 0.7850\n",
            "Epoch 27/60\n",
            "23428/23428 [==============================] - 0s 9us/step - loss: 0.5331 - binary_accuracy: 0.7567 - val_loss: 0.5010 - val_binary_accuracy: 0.7800\n",
            "Epoch 28/60\n",
            "23428/23428 [==============================] - 0s 10us/step - loss: 0.5339 - binary_accuracy: 0.7524 - val_loss: 0.4992 - val_binary_accuracy: 0.7797\n",
            "Epoch 29/60\n",
            "23428/23428 [==============================] - 0s 10us/step - loss: 0.5316 - binary_accuracy: 0.7549 - val_loss: 0.4957 - val_binary_accuracy: 0.7844\n",
            "Epoch 30/60\n",
            "23428/23428 [==============================] - 0s 10us/step - loss: 0.5299 - binary_accuracy: 0.7563 - val_loss: 0.4932 - val_binary_accuracy: 0.7855\n",
            "Epoch 31/60\n",
            "23428/23428 [==============================] - 0s 10us/step - loss: 0.5293 - binary_accuracy: 0.7579 - val_loss: 0.4951 - val_binary_accuracy: 0.7811\n",
            "Epoch 32/60\n",
            "23428/23428 [==============================] - 0s 10us/step - loss: 0.5270 - binary_accuracy: 0.7592 - val_loss: 0.4904 - val_binary_accuracy: 0.7871\n",
            "Epoch 33/60\n",
            "23428/23428 [==============================] - 0s 10us/step - loss: 0.5283 - binary_accuracy: 0.7543 - val_loss: 0.4914 - val_binary_accuracy: 0.7807\n",
            "Epoch 34/60\n",
            "23428/23428 [==============================] - 0s 10us/step - loss: 0.5220 - binary_accuracy: 0.7620 - val_loss: 0.4915 - val_binary_accuracy: 0.7811\n",
            "Epoch 35/60\n",
            "23428/23428 [==============================] - 0s 10us/step - loss: 0.5233 - binary_accuracy: 0.7577 - val_loss: 0.4892 - val_binary_accuracy: 0.7822\n",
            "Epoch 36/60\n",
            "23428/23428 [==============================] - 0s 10us/step - loss: 0.5198 - binary_accuracy: 0.7587 - val_loss: 0.4867 - val_binary_accuracy: 0.7810\n",
            "Epoch 37/60\n",
            "23428/23428 [==============================] - 0s 10us/step - loss: 0.5207 - binary_accuracy: 0.7609 - val_loss: 0.4854 - val_binary_accuracy: 0.7894\n",
            "Epoch 38/60\n",
            "23428/23428 [==============================] - 0s 11us/step - loss: 0.5183 - binary_accuracy: 0.7596 - val_loss: 0.4834 - val_binary_accuracy: 0.7811\n",
            "Epoch 39/60\n",
            "23428/23428 [==============================] - 0s 10us/step - loss: 0.5172 - binary_accuracy: 0.7596 - val_loss: 0.4918 - val_binary_accuracy: 0.7744\n",
            "Epoch 40/60\n",
            "23428/23428 [==============================] - 0s 10us/step - loss: 0.5186 - binary_accuracy: 0.7625 - val_loss: 0.4826 - val_binary_accuracy: 0.7885\n",
            "Epoch 41/60\n",
            "23428/23428 [==============================] - 0s 10us/step - loss: 0.5151 - binary_accuracy: 0.7610 - val_loss: 0.4860 - val_binary_accuracy: 0.7760\n",
            "Epoch 42/60\n",
            "23428/23428 [==============================] - 0s 10us/step - loss: 0.5136 - binary_accuracy: 0.7613 - val_loss: 0.4802 - val_binary_accuracy: 0.7889\n",
            "Epoch 43/60\n",
            "23428/23428 [==============================] - 0s 10us/step - loss: 0.5163 - binary_accuracy: 0.7617 - val_loss: 0.4802 - val_binary_accuracy: 0.7895\n",
            "Epoch 44/60\n",
            "23428/23428 [==============================] - 0s 11us/step - loss: 0.5158 - binary_accuracy: 0.7589 - val_loss: 0.4802 - val_binary_accuracy: 0.7900\n",
            "Epoch 45/60\n",
            "23428/23428 [==============================] - 0s 10us/step - loss: 0.5137 - binary_accuracy: 0.7606 - val_loss: 0.4777 - val_binary_accuracy: 0.7908\n",
            "Epoch 46/60\n",
            "23428/23428 [==============================] - 0s 10us/step - loss: 0.5084 - binary_accuracy: 0.7617 - val_loss: 0.4780 - val_binary_accuracy: 0.7806\n",
            "Epoch 47/60\n",
            "23428/23428 [==============================] - 0s 10us/step - loss: 0.5088 - binary_accuracy: 0.7604 - val_loss: 0.4768 - val_binary_accuracy: 0.7797\n",
            "Epoch 48/60\n",
            "23428/23428 [==============================] - 0s 10us/step - loss: 0.5109 - binary_accuracy: 0.7582 - val_loss: 0.4768 - val_binary_accuracy: 0.7876\n",
            "Epoch 49/60\n",
            "23428/23428 [==============================] - 0s 10us/step - loss: 0.5078 - binary_accuracy: 0.7620 - val_loss: 0.4761 - val_binary_accuracy: 0.7805\n",
            "Epoch 50/60\n",
            "23428/23428 [==============================] - 0s 10us/step - loss: 0.5091 - binary_accuracy: 0.7608 - val_loss: 0.4747 - val_binary_accuracy: 0.7879\n",
            "Epoch 51/60\n",
            "23428/23428 [==============================] - 0s 10us/step - loss: 0.5063 - binary_accuracy: 0.7601 - val_loss: 0.4731 - val_binary_accuracy: 0.7885\n",
            "Epoch 52/60\n",
            "23428/23428 [==============================] - 0s 10us/step - loss: 0.5066 - binary_accuracy: 0.7617 - val_loss: 0.4722 - val_binary_accuracy: 0.7883\n",
            "Epoch 53/60\n",
            "23428/23428 [==============================] - 0s 10us/step - loss: 0.5062 - binary_accuracy: 0.7589 - val_loss: 0.4719 - val_binary_accuracy: 0.7895\n",
            "Epoch 54/60\n",
            "23428/23428 [==============================] - 0s 10us/step - loss: 0.5074 - binary_accuracy: 0.7619 - val_loss: 0.4732 - val_binary_accuracy: 0.7771\n",
            "Epoch 55/60\n",
            "23428/23428 [==============================] - 0s 10us/step - loss: 0.5063 - binary_accuracy: 0.7600 - val_loss: 0.4719 - val_binary_accuracy: 0.7880\n",
            "Epoch 56/60\n",
            "23428/23428 [==============================] - 0s 10us/step - loss: 0.5066 - binary_accuracy: 0.7607 - val_loss: 0.4711 - val_binary_accuracy: 0.7884\n",
            "Epoch 57/60\n",
            "23428/23428 [==============================] - 0s 9us/step - loss: 0.5031 - binary_accuracy: 0.7641 - val_loss: 0.4718 - val_binary_accuracy: 0.7784\n",
            "Epoch 58/60\n",
            "23428/23428 [==============================] - 0s 10us/step - loss: 0.5027 - binary_accuracy: 0.7625 - val_loss: 0.4699 - val_binary_accuracy: 0.7890\n",
            "Epoch 59/60\n",
            "23428/23428 [==============================] - 0s 9us/step - loss: 0.5039 - binary_accuracy: 0.7618 - val_loss: 0.4714 - val_binary_accuracy: 0.7790\n",
            "Epoch 60/60\n",
            "23428/23428 [==============================] - 0s 10us/step - loss: 0.5003 - binary_accuracy: 0.7667 - val_loss: 0.4688 - val_binary_accuracy: 0.7901\n",
            "23428/23428 [==============================] - 1s 45us/step\n",
            "8000/8000 [==============================] - 0s 47us/step\n",
            "\n",
            "\n",
            "Ensayando modelo con estructura [64, 32, 10, 1] y optimizador <keras.optimizers.SGD object at 0x00000180021B5CC8>\n",
            "(23428, 31) (8000, 31) (13470, 31) (23428,) (8000,) (13470,)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 23428 samples, validate on 8000 samples\n",
            "Epoch 1/60\n",
            "23428/23428 [==============================] - 1s 59us/step - loss: 0.9442 - binary_accuracy: 0.4962 - val_loss: 0.9440 - val_binary_accuracy: 0.5138\n",
            "Epoch 2/60\n",
            "23428/23428 [==============================] - 0s 6us/step - loss: 0.9421 - binary_accuracy: 0.4991 - val_loss: 0.9420 - val_binary_accuracy: 0.5179\n",
            "Epoch 3/60\n",
            "23428/23428 [==============================] - 0s 5us/step - loss: 0.9402 - binary_accuracy: 0.5038 - val_loss: 0.9400 - val_binary_accuracy: 0.5191\n",
            "Epoch 4/60\n",
            "23428/23428 [==============================] - 0s 5us/step - loss: 0.9382 - binary_accuracy: 0.5039 - val_loss: 0.9379 - val_binary_accuracy: 0.5189\n",
            "Epoch 5/60\n",
            "23428/23428 [==============================] - 0s 5us/step - loss: 0.9351 - binary_accuracy: 0.5088 - val_loss: 0.9357 - val_binary_accuracy: 0.5191\n",
            "Epoch 6/60\n",
            "23428/23428 [==============================] - 0s 5us/step - loss: 0.9334 - binary_accuracy: 0.5131 - val_loss: 0.9336 - val_binary_accuracy: 0.5191\n",
            "Epoch 7/60\n",
            "23428/23428 [==============================] - 0s 5us/step - loss: 0.9309 - binary_accuracy: 0.5162 - val_loss: 0.9315 - val_binary_accuracy: 0.5191\n",
            "Epoch 8/60\n",
            "23428/23428 [==============================] - 0s 4us/step - loss: 0.9288 - binary_accuracy: 0.5232 - val_loss: 0.9293 - val_binary_accuracy: 0.5191\n",
            "Epoch 9/60\n",
            "23428/23428 [==============================] - 0s 5us/step - loss: 0.9272 - binary_accuracy: 0.5297 - val_loss: 0.9271 - val_binary_accuracy: 0.5191\n",
            "Epoch 10/60\n",
            "23428/23428 [==============================] - 0s 5us/step - loss: 0.9246 - binary_accuracy: 0.5387 - val_loss: 0.9249 - val_binary_accuracy: 0.5191\n",
            "Epoch 11/60\n",
            "23428/23428 [==============================] - 0s 4us/step - loss: 0.9228 - binary_accuracy: 0.5437 - val_loss: 0.9227 - val_binary_accuracy: 0.5191\n",
            "Epoch 12/60\n",
            "23428/23428 [==============================] - 0s 5us/step - loss: 0.9214 - binary_accuracy: 0.5492 - val_loss: 0.9204 - val_binary_accuracy: 0.6022\n",
            "Epoch 13/60\n",
            "23428/23428 [==============================] - 0s 4us/step - loss: 0.9185 - binary_accuracy: 0.5512 - val_loss: 0.9182 - val_binary_accuracy: 0.6022\n",
            "Epoch 14/60\n",
            "23428/23428 [==============================] - 0s 4us/step - loss: 0.9163 - binary_accuracy: 0.5525 - val_loss: 0.9159 - val_binary_accuracy: 0.6022\n",
            "Epoch 15/60\n",
            "23428/23428 [==============================] - 0s 5us/step - loss: 0.9142 - binary_accuracy: 0.5585 - val_loss: 0.9135 - val_binary_accuracy: 0.6022\n",
            "Epoch 16/60\n",
            "23428/23428 [==============================] - 0s 4us/step - loss: 0.9122 - binary_accuracy: 0.5574 - val_loss: 0.9110 - val_binary_accuracy: 0.6022\n",
            "Epoch 17/60\n",
            "23428/23428 [==============================] - 0s 5us/step - loss: 0.9101 - binary_accuracy: 0.5578 - val_loss: 0.9085 - val_binary_accuracy: 0.6022\n",
            "Epoch 18/60\n",
            "23428/23428 [==============================] - 0s 5us/step - loss: 0.9078 - binary_accuracy: 0.5557 - val_loss: 0.9059 - val_binary_accuracy: 0.6022\n",
            "Epoch 19/60\n",
            "23428/23428 [==============================] - 0s 5us/step - loss: 0.9050 - binary_accuracy: 0.5615 - val_loss: 0.9031 - val_binary_accuracy: 0.6022\n",
            "Epoch 20/60\n",
            "23428/23428 [==============================] - 0s 5us/step - loss: 0.9029 - binary_accuracy: 0.5647 - val_loss: 0.9003 - val_binary_accuracy: 0.6024\n",
            "Epoch 21/60\n",
            "23428/23428 [==============================] - 0s 4us/step - loss: 0.8999 - binary_accuracy: 0.5722 - val_loss: 0.8975 - val_binary_accuracy: 0.6026\n",
            "Epoch 22/60\n",
            "23428/23428 [==============================] - 0s 5us/step - loss: 0.8968 - binary_accuracy: 0.5758 - val_loss: 0.8945 - val_binary_accuracy: 0.6034\n",
            "Epoch 23/60\n",
            "23428/23428 [==============================] - 0s 5us/step - loss: 0.8942 - binary_accuracy: 0.5764 - val_loss: 0.8913 - val_binary_accuracy: 0.6036\n",
            "Epoch 24/60\n",
            "23428/23428 [==============================] - 0s 5us/step - loss: 0.8918 - binary_accuracy: 0.5796 - val_loss: 0.8880 - val_binary_accuracy: 0.6044\n",
            "Epoch 25/60\n",
            "23428/23428 [==============================] - 0s 5us/step - loss: 0.8896 - binary_accuracy: 0.5830 - val_loss: 0.8846 - val_binary_accuracy: 0.6084\n",
            "Epoch 26/60\n",
            "23428/23428 [==============================] - 0s 4us/step - loss: 0.8863 - binary_accuracy: 0.5868 - val_loss: 0.8811 - val_binary_accuracy: 0.6150\n",
            "Epoch 27/60\n",
            "23428/23428 [==============================] - 0s 4us/step - loss: 0.8837 - binary_accuracy: 0.5855 - val_loss: 0.8773 - val_binary_accuracy: 0.6400\n",
            "Epoch 28/60\n",
            "23428/23428 [==============================] - 0s 4us/step - loss: 0.8811 - binary_accuracy: 0.5882 - val_loss: 0.8734 - val_binary_accuracy: 0.6586\n",
            "Epoch 29/60\n",
            "23428/23428 [==============================] - 0s 5us/step - loss: 0.8765 - binary_accuracy: 0.5982 - val_loss: 0.8692 - val_binary_accuracy: 0.6611\n",
            "Epoch 30/60\n",
            "23428/23428 [==============================] - 0s 5us/step - loss: 0.8738 - binary_accuracy: 0.5969 - val_loss: 0.8649 - val_binary_accuracy: 0.6755\n",
            "Epoch 31/60\n",
            "23428/23428 [==============================] - 0s 5us/step - loss: 0.8696 - binary_accuracy: 0.6037 - val_loss: 0.8603 - val_binary_accuracy: 0.6860\n",
            "Epoch 32/60\n",
            "23428/23428 [==============================] - 0s 5us/step - loss: 0.8668 - binary_accuracy: 0.6036 - val_loss: 0.8554 - val_binary_accuracy: 0.7067\n",
            "Epoch 33/60\n",
            "23428/23428 [==============================] - 0s 5us/step - loss: 0.8621 - binary_accuracy: 0.6164 - val_loss: 0.8501 - val_binary_accuracy: 0.7117\n",
            "Epoch 34/60\n",
            "23428/23428 [==============================] - 0s 4us/step - loss: 0.8590 - binary_accuracy: 0.6170 - val_loss: 0.8447 - val_binary_accuracy: 0.7212\n",
            "Epoch 35/60\n",
            "23428/23428 [==============================] - 0s 5us/step - loss: 0.8537 - binary_accuracy: 0.6161 - val_loss: 0.8389 - val_binary_accuracy: 0.7387\n",
            "Epoch 36/60\n",
            "23428/23428 [==============================] - 0s 5us/step - loss: 0.8491 - binary_accuracy: 0.6183 - val_loss: 0.8329 - val_binary_accuracy: 0.7398\n",
            "Epoch 37/60\n",
            "23428/23428 [==============================] - 0s 5us/step - loss: 0.8437 - binary_accuracy: 0.6313 - val_loss: 0.8266 - val_binary_accuracy: 0.7505\n",
            "Epoch 38/60\n",
            "23428/23428 [==============================] - 0s 5us/step - loss: 0.8390 - binary_accuracy: 0.6367 - val_loss: 0.8196 - val_binary_accuracy: 0.7586\n",
            "Epoch 39/60\n",
            "23428/23428 [==============================] - 0s 5us/step - loss: 0.8349 - binary_accuracy: 0.6394 - val_loss: 0.8124 - val_binary_accuracy: 0.7602\n",
            "Epoch 40/60\n",
            "23428/23428 [==============================] - 0s 4us/step - loss: 0.8289 - binary_accuracy: 0.6465 - val_loss: 0.8048 - val_binary_accuracy: 0.7686\n",
            "Epoch 41/60\n",
            "23428/23428 [==============================] - 0s 5us/step - loss: 0.8249 - binary_accuracy: 0.6471 - val_loss: 0.7971 - val_binary_accuracy: 0.7715\n",
            "Epoch 42/60\n",
            "23428/23428 [==============================] - 0s 4us/step - loss: 0.8173 - binary_accuracy: 0.6549 - val_loss: 0.7888 - val_binary_accuracy: 0.7728\n",
            "Epoch 43/60\n",
            "23428/23428 [==============================] - 0s 5us/step - loss: 0.8131 - binary_accuracy: 0.6546 - val_loss: 0.7800 - val_binary_accuracy: 0.7731\n",
            "Epoch 44/60\n",
            "23428/23428 [==============================] - 0s 5us/step - loss: 0.8067 - binary_accuracy: 0.6621 - val_loss: 0.7710 - val_binary_accuracy: 0.7728\n",
            "Epoch 45/60\n",
            "23428/23428 [==============================] - 0s 4us/step - loss: 0.8025 - binary_accuracy: 0.6626 - val_loss: 0.7626 - val_binary_accuracy: 0.7739\n",
            "Epoch 46/60\n",
            "23428/23428 [==============================] - 0s 4us/step - loss: 0.7964 - binary_accuracy: 0.6717 - val_loss: 0.7548 - val_binary_accuracy: 0.7710\n",
            "Epoch 47/60\n",
            "23428/23428 [==============================] - 0s 4us/step - loss: 0.7911 - binary_accuracy: 0.6722 - val_loss: 0.7462 - val_binary_accuracy: 0.7713\n",
            "Epoch 48/60\n",
            "23428/23428 [==============================] - 0s 4us/step - loss: 0.7819 - binary_accuracy: 0.6808 - val_loss: 0.7373 - val_binary_accuracy: 0.7716\n",
            "Epoch 49/60\n",
            "23428/23428 [==============================] - 0s 4us/step - loss: 0.7778 - binary_accuracy: 0.6859 - val_loss: 0.7290 - val_binary_accuracy: 0.7763\n",
            "Epoch 50/60\n",
            "23428/23428 [==============================] - 0s 4us/step - loss: 0.7746 - binary_accuracy: 0.6850 - val_loss: 0.7220 - val_binary_accuracy: 0.7753\n",
            "Epoch 51/60\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "23428/23428 [==============================] - 0s 4us/step - loss: 0.7677 - binary_accuracy: 0.6946 - val_loss: 0.7153 - val_binary_accuracy: 0.7761\n",
            "Epoch 52/60\n",
            "23428/23428 [==============================] - 0s 5us/step - loss: 0.7599 - binary_accuracy: 0.6969 - val_loss: 0.7081 - val_binary_accuracy: 0.7772\n",
            "Epoch 53/60\n",
            "23428/23428 [==============================] - 0s 5us/step - loss: 0.7544 - binary_accuracy: 0.7005 - val_loss: 0.7014 - val_binary_accuracy: 0.7761\n",
            "Epoch 54/60\n",
            "23428/23428 [==============================] - 0s 5us/step - loss: 0.7500 - binary_accuracy: 0.7005 - val_loss: 0.6958 - val_binary_accuracy: 0.7794\n",
            "Epoch 55/60\n",
            "23428/23428 [==============================] - 0s 4us/step - loss: 0.7456 - binary_accuracy: 0.7048 - val_loss: 0.6899 - val_binary_accuracy: 0.7797\n",
            "Epoch 56/60\n",
            "23428/23428 [==============================] - 0s 5us/step - loss: 0.7425 - binary_accuracy: 0.7062 - val_loss: 0.6843 - val_binary_accuracy: 0.7799\n",
            "Epoch 57/60\n",
            "23428/23428 [==============================] - 0s 4us/step - loss: 0.7343 - binary_accuracy: 0.7115 - val_loss: 0.6789 - val_binary_accuracy: 0.7793\n",
            "Epoch 58/60\n",
            "23428/23428 [==============================] - 0s 5us/step - loss: 0.7306 - binary_accuracy: 0.7138 - val_loss: 0.6736 - val_binary_accuracy: 0.7794\n",
            "Epoch 59/60\n",
            "23428/23428 [==============================] - 0s 5us/step - loss: 0.7281 - binary_accuracy: 0.7149 - val_loss: 0.6691 - val_binary_accuracy: 0.7795\n",
            "Epoch 60/60\n",
            "23428/23428 [==============================] - 0s 4us/step - loss: 0.7226 - binary_accuracy: 0.7179 - val_loss: 0.6646 - val_binary_accuracy: 0.7797\n",
            "23428/23428 [==============================] - 1s 42us/step\n",
            "8000/8000 [==============================] - 0s 42us/step\n",
            "(23428, 31) (8000, 31) (13470, 31) (23428,) (8000,) (13470,)\n",
            "Train on 23428 samples, validate on 8000 samples\n",
            "Epoch 1/60\n",
            "23428/23428 [==============================] - ETA: 0s - loss: 0.9434 - binary_accuracy: 0.544 - 0s 6us/step - loss: 0.9433 - binary_accuracy: 0.5436 - val_loss: 0.9430 - val_binary_accuracy: 0.6046\n",
            "Epoch 2/60\n",
            "23428/23428 [==============================] - 0s 5us/step - loss: 0.9412 - binary_accuracy: 0.5224 - val_loss: 0.9405 - val_binary_accuracy: 0.5171\n",
            "Epoch 3/60\n",
            "23428/23428 [==============================] - 0s 5us/step - loss: 0.9382 - binary_accuracy: 0.5014 - val_loss: 0.9383 - val_binary_accuracy: 0.5188\n",
            "Epoch 4/60\n",
            "23428/23428 [==============================] - 0s 5us/step - loss: 0.9358 - binary_accuracy: 0.5047 - val_loss: 0.9360 - val_binary_accuracy: 0.5191\n",
            "Epoch 5/60\n",
            "23428/23428 [==============================] - 0s 4us/step - loss: 0.9330 - binary_accuracy: 0.5132 - val_loss: 0.9338 - val_binary_accuracy: 0.5190\n",
            "Epoch 6/60\n",
            "23428/23428 [==============================] - 0s 5us/step - loss: 0.9310 - binary_accuracy: 0.5192 - val_loss: 0.9315 - val_binary_accuracy: 0.5191\n",
            "Epoch 7/60\n",
            "23428/23428 [==============================] - 0s 5us/step - loss: 0.9298 - binary_accuracy: 0.5281 - val_loss: 0.9293 - val_binary_accuracy: 0.5191\n",
            "Epoch 8/60\n",
            "23428/23428 [==============================] - 0s 4us/step - loss: 0.9278 - binary_accuracy: 0.5411 - val_loss: 0.9271 - val_binary_accuracy: 0.5191\n",
            "Epoch 9/60\n",
            "23428/23428 [==============================] - 0s 5us/step - loss: 0.9246 - binary_accuracy: 0.5487 - val_loss: 0.9249 - val_binary_accuracy: 0.5191\n",
            "Epoch 10/60\n",
            "23428/23428 [==============================] - 0s 5us/step - loss: 0.9227 - binary_accuracy: 0.5542 - val_loss: 0.9226 - val_binary_accuracy: 0.6022\n",
            "Epoch 11/60\n",
            "23428/23428 [==============================] - 0s 5us/step - loss: 0.9208 - binary_accuracy: 0.5521 - val_loss: 0.9202 - val_binary_accuracy: 0.6022\n",
            "Epoch 12/60\n",
            "23428/23428 [==============================] - 0s 5us/step - loss: 0.9177 - binary_accuracy: 0.5555 - val_loss: 0.9179 - val_binary_accuracy: 0.6022\n",
            "Epoch 13/60\n",
            "23428/23428 [==============================] - 0s 5us/step - loss: 0.9162 - binary_accuracy: 0.5599 - val_loss: 0.9154 - val_binary_accuracy: 0.6022\n",
            "Epoch 14/60\n",
            "23428/23428 [==============================] - 0s 5us/step - loss: 0.9135 - binary_accuracy: 0.5598 - val_loss: 0.9129 - val_binary_accuracy: 0.6022\n",
            "Epoch 15/60\n",
            "23428/23428 [==============================] - 0s 4us/step - loss: 0.9109 - binary_accuracy: 0.5572 - val_loss: 0.9104 - val_binary_accuracy: 0.6022\n",
            "Epoch 16/60\n",
            "23428/23428 [==============================] - 0s 5us/step - loss: 0.9082 - binary_accuracy: 0.5604 - val_loss: 0.9077 - val_binary_accuracy: 0.6022\n",
            "Epoch 17/60\n",
            "23428/23428 [==============================] - 0s 4us/step - loss: 0.9065 - binary_accuracy: 0.5621 - val_loss: 0.9049 - val_binary_accuracy: 0.6022\n",
            "Epoch 18/60\n",
            "23428/23428 [==============================] - 0s 5us/step - loss: 0.9036 - binary_accuracy: 0.5723 - val_loss: 0.9020 - val_binary_accuracy: 0.6024\n",
            "Epoch 19/60\n",
            "23428/23428 [==============================] - 0s 4us/step - loss: 0.9015 - binary_accuracy: 0.5777 - val_loss: 0.8990 - val_binary_accuracy: 0.6026\n",
            "Epoch 20/60\n",
            "23428/23428 [==============================] - 0s 5us/step - loss: 0.8986 - binary_accuracy: 0.5770 - val_loss: 0.8959 - val_binary_accuracy: 0.6034\n",
            "Epoch 21/60\n",
            "23428/23428 [==============================] - 0s 4us/step - loss: 0.8963 - binary_accuracy: 0.5784 - val_loss: 0.8926 - val_binary_accuracy: 0.6039\n",
            "Epoch 22/60\n",
            "23428/23428 [==============================] - 0s 5us/step - loss: 0.8931 - binary_accuracy: 0.5861 - val_loss: 0.8893 - val_binary_accuracy: 0.6053\n",
            "Epoch 23/60\n",
            "23428/23428 [==============================] - 0s 5us/step - loss: 0.8900 - binary_accuracy: 0.5917 - val_loss: 0.8857 - val_binary_accuracy: 0.6087\n",
            "Epoch 24/60\n",
            "23428/23428 [==============================] - 0s 5us/step - loss: 0.8870 - binary_accuracy: 0.5927 - val_loss: 0.8819 - val_binary_accuracy: 0.6151\n",
            "Epoch 25/60\n",
            "23428/23428 [==============================] - 0s 5us/step - loss: 0.8841 - binary_accuracy: 0.5921 - val_loss: 0.8779 - val_binary_accuracy: 0.6165\n",
            "Epoch 26/60\n",
            "23428/23428 [==============================] - 0s 5us/step - loss: 0.8795 - binary_accuracy: 0.5982 - val_loss: 0.8736 - val_binary_accuracy: 0.6499\n",
            "Epoch 27/60\n",
            "23428/23428 [==============================] - 0s 4us/step - loss: 0.8770 - binary_accuracy: 0.6009 - val_loss: 0.8692 - val_binary_accuracy: 0.6626\n",
            "Epoch 28/60\n",
            "23428/23428 [==============================] - 0s 4us/step - loss: 0.8739 - binary_accuracy: 0.6014 - val_loss: 0.8645 - val_binary_accuracy: 0.6775\n",
            "Epoch 29/60\n",
            "23428/23428 [==============================] - 0s 4us/step - loss: 0.8699 - binary_accuracy: 0.6081 - val_loss: 0.8597 - val_binary_accuracy: 0.6925\n",
            "Epoch 30/60\n",
            "23428/23428 [==============================] - 0s 4us/step - loss: 0.8668 - binary_accuracy: 0.6071 - val_loss: 0.8545 - val_binary_accuracy: 0.7117\n",
            "Epoch 31/60\n",
            "23428/23428 [==============================] - 0s 4us/step - loss: 0.8611 - binary_accuracy: 0.6167 - val_loss: 0.8488 - val_binary_accuracy: 0.7188\n",
            "Epoch 32/60\n",
            "23428/23428 [==============================] - 0s 5us/step - loss: 0.8569 - binary_accuracy: 0.6219 - val_loss: 0.8425 - val_binary_accuracy: 0.7219\n",
            "Epoch 33/60\n",
            "23428/23428 [==============================] - 0s 5us/step - loss: 0.8526 - binary_accuracy: 0.6214 - val_loss: 0.8362 - val_binary_accuracy: 0.7396\n",
            "Epoch 34/60\n",
            "23428/23428 [==============================] - 0s 4us/step - loss: 0.8493 - binary_accuracy: 0.6282 - val_loss: 0.8292 - val_binary_accuracy: 0.7495\n",
            "Epoch 35/60\n",
            "23428/23428 [==============================] - 0s 4us/step - loss: 0.8426 - binary_accuracy: 0.6356 - val_loss: 0.8214 - val_binary_accuracy: 0.7589\n",
            "Epoch 36/60\n",
            "23428/23428 [==============================] - 0s 4us/step - loss: 0.8359 - binary_accuracy: 0.6386 - val_loss: 0.8128 - val_binary_accuracy: 0.7585\n",
            "Epoch 37/60\n",
            "23428/23428 [==============================] - 0s 5us/step - loss: 0.8327 - binary_accuracy: 0.6401 - val_loss: 0.8044 - val_binary_accuracy: 0.7731\n",
            "Epoch 38/60\n",
            "23428/23428 [==============================] - 0s 5us/step - loss: 0.8233 - binary_accuracy: 0.6520 - val_loss: 0.7953 - val_binary_accuracy: 0.7730\n",
            "Epoch 39/60\n",
            "23428/23428 [==============================] - 0s 4us/step - loss: 0.8205 - binary_accuracy: 0.6521 - val_loss: 0.7866 - val_binary_accuracy: 0.7707\n",
            "Epoch 40/60\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "23428/23428 [==============================] - 0s 4us/step - loss: 0.8142 - binary_accuracy: 0.6588 - val_loss: 0.7778 - val_binary_accuracy: 0.7706\n",
            "Epoch 41/60\n",
            "23428/23428 [==============================] - 0s 5us/step - loss: 0.8074 - binary_accuracy: 0.6659 - val_loss: 0.7689 - val_binary_accuracy: 0.7710\n",
            "Epoch 42/60\n",
            "23428/23428 [==============================] - 0s 5us/step - loss: 0.8012 - binary_accuracy: 0.6640 - val_loss: 0.7598 - val_binary_accuracy: 0.7709\n",
            "Epoch 43/60\n",
            "23428/23428 [==============================] - 0s 5us/step - loss: 0.7942 - binary_accuracy: 0.6724 - val_loss: 0.7514 - val_binary_accuracy: 0.7706\n",
            "Epoch 44/60\n",
            "23428/23428 [==============================] - 0s 5us/step - loss: 0.7910 - binary_accuracy: 0.6740 - val_loss: 0.7439 - val_binary_accuracy: 0.7746\n",
            "Epoch 45/60\n",
            "23428/23428 [==============================] - 0s 5us/step - loss: 0.7832 - binary_accuracy: 0.6827 - val_loss: 0.7363 - val_binary_accuracy: 0.7750\n",
            "Epoch 46/60\n",
            "23428/23428 [==============================] - 0s 4us/step - loss: 0.7759 - binary_accuracy: 0.6858 - val_loss: 0.7283 - val_binary_accuracy: 0.7760\n",
            "Epoch 47/60\n",
            "23428/23428 [==============================] - 0s 5us/step - loss: 0.7696 - binary_accuracy: 0.6887 - val_loss: 0.7212 - val_binary_accuracy: 0.7764\n",
            "Epoch 48/60\n",
            "23428/23428 [==============================] - 0s 4us/step - loss: 0.7692 - binary_accuracy: 0.6893 - val_loss: 0.7147 - val_binary_accuracy: 0.7766\n",
            "Epoch 49/60\n",
            "23428/23428 [==============================] - 0s 4us/step - loss: 0.7613 - binary_accuracy: 0.6983 - val_loss: 0.7082 - val_binary_accuracy: 0.7766\n",
            "Epoch 50/60\n",
            "23428/23428 [==============================] - 0s 5us/step - loss: 0.7563 - binary_accuracy: 0.6986 - val_loss: 0.7021 - val_binary_accuracy: 0.7801\n",
            "Epoch 51/60\n",
            "23428/23428 [==============================] - 0s 4us/step - loss: 0.7501 - binary_accuracy: 0.7042 - val_loss: 0.6962 - val_binary_accuracy: 0.7805\n",
            "Epoch 52/60\n",
            "23428/23428 [==============================] - 0s 5us/step - loss: 0.7445 - binary_accuracy: 0.7072 - val_loss: 0.6902 - val_binary_accuracy: 0.7804\n",
            "Epoch 53/60\n",
            "23428/23428 [==============================] - 0s 5us/step - loss: 0.7410 - binary_accuracy: 0.7080 - val_loss: 0.6846 - val_binary_accuracy: 0.7812\n",
            "Epoch 54/60\n",
            "23428/23428 [==============================] - 0s 5us/step - loss: 0.7384 - binary_accuracy: 0.7085 - val_loss: 0.6801 - val_binary_accuracy: 0.7814\n",
            "Epoch 55/60\n",
            "23428/23428 [==============================] - 0s 5us/step - loss: 0.7317 - binary_accuracy: 0.7149 - val_loss: 0.6753 - val_binary_accuracy: 0.7820\n",
            "Epoch 56/60\n",
            "23428/23428 [==============================] - 0s 4us/step - loss: 0.7290 - binary_accuracy: 0.7168 - val_loss: 0.6705 - val_binary_accuracy: 0.7806\n",
            "Epoch 57/60\n",
            "23428/23428 [==============================] - 0s 5us/step - loss: 0.7244 - binary_accuracy: 0.7221 - val_loss: 0.6660 - val_binary_accuracy: 0.7810\n",
            "Epoch 58/60\n",
            "23428/23428 [==============================] - 0s 5us/step - loss: 0.7220 - binary_accuracy: 0.7168 - val_loss: 0.6619 - val_binary_accuracy: 0.7815\n",
            "Epoch 59/60\n",
            "23428/23428 [==============================] - 0s 4us/step - loss: 0.7153 - binary_accuracy: 0.7250 - val_loss: 0.6583 - val_binary_accuracy: 0.7812\n",
            "Epoch 60/60\n",
            "23428/23428 [==============================] - 0s 5us/step - loss: 0.7096 - binary_accuracy: 0.7276 - val_loss: 0.6547 - val_binary_accuracy: 0.7815\n",
            "23428/23428 [==============================] - 1s 41us/step\n",
            "8000/8000 [==============================] - 0s 43us/step\n",
            "(23428, 31) (8000, 31) (13470, 31) (23428,) (8000,) (13470,)\n",
            "Train on 23428 samples, validate on 8000 samples\n",
            "Epoch 1/60\n",
            "23428/23428 [==============================] - 0s 6us/step - loss: 0.9433 - binary_accuracy: 0.5430 - val_loss: 0.9430 - val_binary_accuracy: 0.6001\n",
            "Epoch 2/60\n",
            "23428/23428 [==============================] - 0s 5us/step - loss: 0.9411 - binary_accuracy: 0.5259 - val_loss: 0.9406 - val_binary_accuracy: 0.5173\n",
            "Epoch 3/60\n",
            "23428/23428 [==============================] - 0s 5us/step - loss: 0.9385 - binary_accuracy: 0.5206 - val_loss: 0.9383 - val_binary_accuracy: 0.5188\n",
            "Epoch 4/60\n",
            "23428/23428 [==============================] - 0s 4us/step - loss: 0.9368 - binary_accuracy: 0.5123 - val_loss: 0.9361 - val_binary_accuracy: 0.5191\n",
            "Epoch 5/60\n",
            "23428/23428 [==============================] - 0s 4us/step - loss: 0.9339 - binary_accuracy: 0.5147 - val_loss: 0.9339 - val_binary_accuracy: 0.5189\n",
            "Epoch 6/60\n",
            "23428/23428 [==============================] - 0s 4us/step - loss: 0.9315 - binary_accuracy: 0.5292 - val_loss: 0.9317 - val_binary_accuracy: 0.5191\n",
            "Epoch 7/60\n",
            "23428/23428 [==============================] - 0s 5us/step - loss: 0.9291 - binary_accuracy: 0.5315 - val_loss: 0.9294 - val_binary_accuracy: 0.5191\n",
            "Epoch 8/60\n",
            "23428/23428 [==============================] - 0s 4us/step - loss: 0.9274 - binary_accuracy: 0.5382 - val_loss: 0.9272 - val_binary_accuracy: 0.5191\n",
            "Epoch 9/60\n",
            "23428/23428 [==============================] - 0s 5us/step - loss: 0.9251 - binary_accuracy: 0.5435 - val_loss: 0.9250 - val_binary_accuracy: 0.5191\n",
            "Epoch 10/60\n",
            "23428/23428 [==============================] - 0s 4us/step - loss: 0.9228 - binary_accuracy: 0.5520 - val_loss: 0.9227 - val_binary_accuracy: 0.6022\n",
            "Epoch 11/60\n",
            "23428/23428 [==============================] - 0s 4us/step - loss: 0.9214 - binary_accuracy: 0.5521 - val_loss: 0.9204 - val_binary_accuracy: 0.6022\n",
            "Epoch 12/60\n",
            "23428/23428 [==============================] - 0s 4us/step - loss: 0.9188 - binary_accuracy: 0.5554 - val_loss: 0.9181 - val_binary_accuracy: 0.6022\n",
            "Epoch 13/60\n",
            "23428/23428 [==============================] - 0s 4us/step - loss: 0.9162 - binary_accuracy: 0.5568 - val_loss: 0.9157 - val_binary_accuracy: 0.6022\n",
            "Epoch 14/60\n",
            "23428/23428 [==============================] - 0s 4us/step - loss: 0.9141 - binary_accuracy: 0.5564 - val_loss: 0.9132 - val_binary_accuracy: 0.6022\n",
            "Epoch 15/60\n",
            "23428/23428 [==============================] - 0s 5us/step - loss: 0.9115 - binary_accuracy: 0.5556 - val_loss: 0.9107 - val_binary_accuracy: 0.6022\n",
            "Epoch 16/60\n",
            "23428/23428 [==============================] - 0s 4us/step - loss: 0.9090 - binary_accuracy: 0.5613 - val_loss: 0.9080 - val_binary_accuracy: 0.6022\n",
            "Epoch 17/60\n",
            "23428/23428 [==============================] - 0s 5us/step - loss: 0.9065 - binary_accuracy: 0.5622 - val_loss: 0.9053 - val_binary_accuracy: 0.6022\n",
            "Epoch 18/60\n",
            "23428/23428 [==============================] - 0s 5us/step - loss: 0.9042 - binary_accuracy: 0.5722 - val_loss: 0.9024 - val_binary_accuracy: 0.6025\n",
            "Epoch 19/60\n",
            "23428/23428 [==============================] - 0s 5us/step - loss: 0.9019 - binary_accuracy: 0.5750 - val_loss: 0.8994 - val_binary_accuracy: 0.6024\n",
            "Epoch 20/60\n",
            "23428/23428 [==============================] - 0s 4us/step - loss: 0.8981 - binary_accuracy: 0.5789 - val_loss: 0.8963 - val_binary_accuracy: 0.6028\n",
            "Epoch 21/60\n",
            "23428/23428 [==============================] - 0s 5us/step - loss: 0.8958 - binary_accuracy: 0.5801 - val_loss: 0.8931 - val_binary_accuracy: 0.6037\n",
            "Epoch 22/60\n",
            "23428/23428 [==============================] - 0s 4us/step - loss: 0.8927 - binary_accuracy: 0.5849 - val_loss: 0.8897 - val_binary_accuracy: 0.6041\n",
            "Epoch 23/60\n",
            "23428/23428 [==============================] - 0s 5us/step - loss: 0.8902 - binary_accuracy: 0.5834 - val_loss: 0.8861 - val_binary_accuracy: 0.6062\n",
            "Epoch 24/60\n",
            "23428/23428 [==============================] - 0s 4us/step - loss: 0.8885 - binary_accuracy: 0.5882 - val_loss: 0.8826 - val_binary_accuracy: 0.6135\n",
            "Epoch 25/60\n",
            "23428/23428 [==============================] - 0s 4us/step - loss: 0.8851 - binary_accuracy: 0.5900 - val_loss: 0.8787 - val_binary_accuracy: 0.6184\n",
            "Epoch 26/60\n",
            "23428/23428 [==============================] - 0s 4us/step - loss: 0.8801 - binary_accuracy: 0.5988 - val_loss: 0.8745 - val_binary_accuracy: 0.6499\n",
            "Epoch 27/60\n",
            "23428/23428 [==============================] - 0s 4us/step - loss: 0.8787 - binary_accuracy: 0.5967 - val_loss: 0.8702 - val_binary_accuracy: 0.6615\n",
            "Epoch 28/60\n",
            "23428/23428 [==============================] - 0s 5us/step - loss: 0.8748 - binary_accuracy: 0.6011 - val_loss: 0.8657 - val_binary_accuracy: 0.6798\n",
            "Epoch 29/60\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "23428/23428 [==============================] - 0s 4us/step - loss: 0.8701 - binary_accuracy: 0.6080 - val_loss: 0.8609 - val_binary_accuracy: 0.6940\n",
            "Epoch 30/60\n",
            "23428/23428 [==============================] - 0s 5us/step - loss: 0.8681 - binary_accuracy: 0.6064 - val_loss: 0.8558 - val_binary_accuracy: 0.7060\n",
            "Epoch 31/60\n",
            "23428/23428 [==============================] - 0s 5us/step - loss: 0.8626 - binary_accuracy: 0.6121 - val_loss: 0.8505 - val_binary_accuracy: 0.7182\n",
            "Epoch 32/60\n",
            "23428/23428 [==============================] - 0s 5us/step - loss: 0.8582 - binary_accuracy: 0.6170 - val_loss: 0.8446 - val_binary_accuracy: 0.7258\n",
            "Epoch 33/60\n",
            "23428/23428 [==============================] - 0s 5us/step - loss: 0.8551 - binary_accuracy: 0.6218 - val_loss: 0.8384 - val_binary_accuracy: 0.7396\n",
            "Epoch 34/60\n",
            "23428/23428 [==============================] - 0s 5us/step - loss: 0.8496 - binary_accuracy: 0.6281 - val_loss: 0.8319 - val_binary_accuracy: 0.7405\n",
            "Epoch 35/60\n",
            "23428/23428 [==============================] - 0s 5us/step - loss: 0.8422 - binary_accuracy: 0.6374 - val_loss: 0.8242 - val_binary_accuracy: 0.7415\n",
            "Epoch 36/60\n",
            "23428/23428 [==============================] - 0s 5us/step - loss: 0.8382 - binary_accuracy: 0.6362 - val_loss: 0.8160 - val_binary_accuracy: 0.7560\n",
            "Epoch 37/60\n",
            "23428/23428 [==============================] - 0s 5us/step - loss: 0.8321 - binary_accuracy: 0.6432 - val_loss: 0.8079 - val_binary_accuracy: 0.7709\n",
            "Epoch 38/60\n",
            "23428/23428 [==============================] - 0s 5us/step - loss: 0.8276 - binary_accuracy: 0.6515 - val_loss: 0.7993 - val_binary_accuracy: 0.7728\n",
            "Epoch 39/60\n",
            "23428/23428 [==============================] - 0s 5us/step - loss: 0.8210 - binary_accuracy: 0.6561 - val_loss: 0.7905 - val_binary_accuracy: 0.7729\n",
            "Epoch 40/60\n",
            "23428/23428 [==============================] - 0s 5us/step - loss: 0.8138 - binary_accuracy: 0.6558 - val_loss: 0.7816 - val_binary_accuracy: 0.7713\n",
            "Epoch 41/60\n",
            "23428/23428 [==============================] - 0s 5us/step - loss: 0.8086 - binary_accuracy: 0.6642 - val_loss: 0.7724 - val_binary_accuracy: 0.7717\n",
            "Epoch 42/60\n",
            "23428/23428 [==============================] - 0s 13us/step - loss: 0.8032 - binary_accuracy: 0.6679 - val_loss: 0.7637 - val_binary_accuracy: 0.7704\n",
            "Epoch 43/60\n",
            "23428/23428 [==============================] - 0s 5us/step - loss: 0.7964 - binary_accuracy: 0.6771 - val_loss: 0.7549 - val_binary_accuracy: 0.7703\n",
            "Epoch 44/60\n",
            "23428/23428 [==============================] - 0s 5us/step - loss: 0.7912 - binary_accuracy: 0.6768 - val_loss: 0.7466 - val_binary_accuracy: 0.7706\n",
            "Epoch 45/60\n",
            "23428/23428 [==============================] - 0s 4us/step - loss: 0.7858 - binary_accuracy: 0.6799 - val_loss: 0.7387 - val_binary_accuracy: 0.7746\n",
            "Epoch 46/60\n",
            "23428/23428 [==============================] - 0s 5us/step - loss: 0.7795 - binary_accuracy: 0.6822 - val_loss: 0.7315 - val_binary_accuracy: 0.7746\n",
            "Epoch 47/60\n",
            "23428/23428 [==============================] - 0s 4us/step - loss: 0.7744 - binary_accuracy: 0.6876 - val_loss: 0.7245 - val_binary_accuracy: 0.7757\n",
            "Epoch 48/60\n",
            "23428/23428 [==============================] - 0s 4us/step - loss: 0.7712 - binary_accuracy: 0.6901 - val_loss: 0.7172 - val_binary_accuracy: 0.7763\n",
            "Epoch 49/60\n",
            "23428/23428 [==============================] - 0s 5us/step - loss: 0.7657 - binary_accuracy: 0.6917 - val_loss: 0.7107 - val_binary_accuracy: 0.7766\n",
            "Epoch 50/60\n",
            "23428/23428 [==============================] - 0s 5us/step - loss: 0.7578 - binary_accuracy: 0.6983 - val_loss: 0.7041 - val_binary_accuracy: 0.7771\n",
            "Epoch 51/60\n",
            "23428/23428 [==============================] - 0s 5us/step - loss: 0.7548 - binary_accuracy: 0.6991 - val_loss: 0.6987 - val_binary_accuracy: 0.7765\n",
            "Epoch 52/60\n",
            "23428/23428 [==============================] - 0s 5us/step - loss: 0.7506 - binary_accuracy: 0.7034 - val_loss: 0.6934 - val_binary_accuracy: 0.7797\n",
            "Epoch 53/60\n",
            "23428/23428 [==============================] - 0s 5us/step - loss: 0.7430 - binary_accuracy: 0.7077 - val_loss: 0.6872 - val_binary_accuracy: 0.7810\n",
            "Epoch 54/60\n",
            "23428/23428 [==============================] - 0s 5us/step - loss: 0.7396 - binary_accuracy: 0.7155 - val_loss: 0.6816 - val_binary_accuracy: 0.7807\n",
            "Epoch 55/60\n",
            "23428/23428 [==============================] - 0s 5us/step - loss: 0.7326 - binary_accuracy: 0.7120 - val_loss: 0.6772 - val_binary_accuracy: 0.7818\n",
            "Epoch 56/60\n",
            "23428/23428 [==============================] - 0s 5us/step - loss: 0.7283 - binary_accuracy: 0.7164 - val_loss: 0.6722 - val_binary_accuracy: 0.7820\n",
            "Epoch 57/60\n",
            "23428/23428 [==============================] - 0s 4us/step - loss: 0.7249 - binary_accuracy: 0.7149 - val_loss: 0.6675 - val_binary_accuracy: 0.7806\n",
            "Epoch 58/60\n",
            "23428/23428 [==============================] - 0s 5us/step - loss: 0.7207 - binary_accuracy: 0.7208 - val_loss: 0.6629 - val_binary_accuracy: 0.7812\n",
            "Epoch 59/60\n",
            "23428/23428 [==============================] - 0s 4us/step - loss: 0.7164 - binary_accuracy: 0.7237 - val_loss: 0.6587 - val_binary_accuracy: 0.7816\n",
            "Epoch 60/60\n",
            "23428/23428 [==============================] - 0s 5us/step - loss: 0.7145 - binary_accuracy: 0.7218 - val_loss: 0.6549 - val_binary_accuracy: 0.7816\n",
            "23428/23428 [==============================] - 1s 41us/step\n",
            "8000/8000 [==============================] - 0s 43us/step\n",
            "\n",
            "\n",
            "Ensayando modelo con estructura [64, 32, 10, 1] y optimizador <keras.optimizers.Adam object at 0x00000180021B5B88>\n",
            "(23428, 31) (8000, 31) (13470, 31) (23428,) (8000,) (13470,)\n",
            "Train on 23428 samples, validate on 8000 samples\n",
            "Epoch 1/60\n",
            "23428/23428 [==============================] - 2s 67us/step - loss: 0.9066 - binary_accuracy: 0.5180 - val_loss: 0.8486 - val_binary_accuracy: 0.5191\n",
            "Epoch 2/60\n",
            "23428/23428 [==============================] - 0s 6us/step - loss: 0.8060 - binary_accuracy: 0.5591 - val_loss: 0.7180 - val_binary_accuracy: 0.7065\n",
            "Epoch 3/60\n",
            "23428/23428 [==============================] - 0s 5us/step - loss: 0.6961 - binary_accuracy: 0.6882 - val_loss: 0.6040 - val_binary_accuracy: 0.7816\n",
            "Epoch 4/60\n",
            "23428/23428 [==============================] - 0s 4us/step - loss: 0.6064 - binary_accuracy: 0.7427 - val_loss: 0.5356 - val_binary_accuracy: 0.7781\n",
            "Epoch 5/60\n",
            "23428/23428 [==============================] - 0s 4us/step - loss: 0.5595 - binary_accuracy: 0.7583 - val_loss: 0.5024 - val_binary_accuracy: 0.7875\n",
            "Epoch 6/60\n",
            "23428/23428 [==============================] - 0s 4us/step - loss: 0.5386 - binary_accuracy: 0.7642 - val_loss: 0.4904 - val_binary_accuracy: 0.7885\n",
            "Epoch 7/60\n",
            "23428/23428 [==============================] - 0s 6us/step - loss: 0.5254 - binary_accuracy: 0.7662 - val_loss: 0.4811 - val_binary_accuracy: 0.7899\n",
            "Epoch 8/60\n",
            "23428/23428 [==============================] - 0s 4us/step - loss: 0.5182 - binary_accuracy: 0.7670 - val_loss: 0.4715 - val_binary_accuracy: 0.7891\n",
            "Epoch 9/60\n",
            "23428/23428 [==============================] - 0s 4us/step - loss: 0.5085 - binary_accuracy: 0.7699 - val_loss: 0.4697 - val_binary_accuracy: 0.7901\n",
            "Epoch 10/60\n",
            "23428/23428 [==============================] - 0s 4us/step - loss: 0.5031 - binary_accuracy: 0.7684 - val_loss: 0.4664 - val_binary_accuracy: 0.7891\n",
            "Epoch 11/60\n",
            "23428/23428 [==============================] - 0s 4us/step - loss: 0.5035 - binary_accuracy: 0.7681 - val_loss: 0.4643 - val_binary_accuracy: 0.7903\n",
            "Epoch 12/60\n",
            "23428/23428 [==============================] - 0s 4us/step - loss: 0.4994 - binary_accuracy: 0.7709 - val_loss: 0.4609 - val_binary_accuracy: 0.7887\n",
            "Epoch 13/60\n",
            "23428/23428 [==============================] - 0s 4us/step - loss: 0.4984 - binary_accuracy: 0.7681 - val_loss: 0.4595 - val_binary_accuracy: 0.7896\n",
            "Epoch 14/60\n",
            "23428/23428 [==============================] - 0s 4us/step - loss: 0.4954 - binary_accuracy: 0.7694 - val_loss: 0.4591 - val_binary_accuracy: 0.7880\n",
            "Epoch 15/60\n",
            "23428/23428 [==============================] - 0s 4us/step - loss: 0.4923 - binary_accuracy: 0.7719 - val_loss: 0.4562 - val_binary_accuracy: 0.7886\n",
            "Epoch 16/60\n",
            "23428/23428 [==============================] - 0s 4us/step - loss: 0.4928 - binary_accuracy: 0.7689 - val_loss: 0.4563 - val_binary_accuracy: 0.7797\n",
            "Epoch 17/60\n",
            "23428/23428 [==============================] - 0s 4us/step - loss: 0.4913 - binary_accuracy: 0.7702 - val_loss: 0.4550 - val_binary_accuracy: 0.7897\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 18/60\n",
            "23428/23428 [==============================] - 0s 5us/step - loss: 0.4857 - binary_accuracy: 0.7720 - val_loss: 0.4533 - val_binary_accuracy: 0.7866\n",
            "Epoch 19/60\n",
            "23428/23428 [==============================] - 0s 5us/step - loss: 0.4890 - binary_accuracy: 0.7711 - val_loss: 0.4529 - val_binary_accuracy: 0.7914\n",
            "Epoch 20/60\n",
            "23428/23428 [==============================] - 0s 5us/step - loss: 0.4869 - binary_accuracy: 0.7705 - val_loss: 0.4517 - val_binary_accuracy: 0.7886\n",
            "Epoch 21/60\n",
            "23428/23428 [==============================] - 0s 4us/step - loss: 0.4860 - binary_accuracy: 0.7706 - val_loss: 0.4516 - val_binary_accuracy: 0.7876\n",
            "Epoch 22/60\n",
            "23428/23428 [==============================] - 0s 5us/step - loss: 0.4843 - binary_accuracy: 0.7718 - val_loss: 0.4502 - val_binary_accuracy: 0.7899\n",
            "Epoch 23/60\n",
            "23428/23428 [==============================] - 0s 5us/step - loss: 0.4847 - binary_accuracy: 0.7715 - val_loss: 0.4501 - val_binary_accuracy: 0.7889\n",
            "Epoch 24/60\n",
            "23428/23428 [==============================] - 0s 5us/step - loss: 0.4831 - binary_accuracy: 0.7724 - val_loss: 0.4502 - val_binary_accuracy: 0.7894\n",
            "Epoch 25/60\n",
            "23428/23428 [==============================] - 0s 5us/step - loss: 0.4869 - binary_accuracy: 0.7715 - val_loss: 0.4492 - val_binary_accuracy: 0.7899\n",
            "Epoch 26/60\n",
            "23428/23428 [==============================] - 0s 4us/step - loss: 0.4845 - binary_accuracy: 0.7732 - val_loss: 0.4498 - val_binary_accuracy: 0.7893\n",
            "Epoch 27/60\n",
            "23428/23428 [==============================] - 0s 5us/step - loss: 0.4807 - binary_accuracy: 0.7712 - val_loss: 0.4482 - val_binary_accuracy: 0.7904\n",
            "Epoch 28/60\n",
            "23428/23428 [==============================] - 0s 4us/step - loss: 0.4834 - binary_accuracy: 0.7715 - val_loss: 0.4489 - val_binary_accuracy: 0.7908\n",
            "Epoch 29/60\n",
            "23428/23428 [==============================] - 0s 5us/step - loss: 0.4830 - binary_accuracy: 0.7681 - val_loss: 0.4480 - val_binary_accuracy: 0.7893\n",
            "Epoch 30/60\n",
            "23428/23428 [==============================] - 0s 4us/step - loss: 0.4842 - binary_accuracy: 0.7724 - val_loss: 0.4479 - val_binary_accuracy: 0.7903\n",
            "Epoch 31/60\n",
            "23428/23428 [==============================] - 0s 5us/step - loss: 0.4819 - binary_accuracy: 0.7709 - val_loss: 0.4474 - val_binary_accuracy: 0.7905\n",
            "Epoch 32/60\n",
            "23428/23428 [==============================] - 0s 5us/step - loss: 0.4807 - binary_accuracy: 0.7701 - val_loss: 0.4469 - val_binary_accuracy: 0.7885\n",
            "Epoch 33/60\n",
            "23428/23428 [==============================] - 0s 5us/step - loss: 0.4812 - binary_accuracy: 0.7710 - val_loss: 0.4461 - val_binary_accuracy: 0.7915\n",
            "Epoch 34/60\n",
            "23428/23428 [==============================] - 0s 5us/step - loss: 0.4771 - binary_accuracy: 0.7729 - val_loss: 0.4455 - val_binary_accuracy: 0.7908\n",
            "Epoch 35/60\n",
            "23428/23428 [==============================] - 0s 5us/step - loss: 0.4775 - binary_accuracy: 0.7748 - val_loss: 0.4466 - val_binary_accuracy: 0.7914\n",
            "Epoch 36/60\n",
            "23428/23428 [==============================] - 0s 5us/step - loss: 0.4819 - binary_accuracy: 0.7726 - val_loss: 0.4457 - val_binary_accuracy: 0.7912\n",
            "Epoch 37/60\n",
            "23428/23428 [==============================] - 0s 5us/step - loss: 0.4774 - binary_accuracy: 0.7727 - val_loss: 0.4453 - val_binary_accuracy: 0.7896\n",
            "Epoch 38/60\n",
            "23428/23428 [==============================] - 0s 5us/step - loss: 0.4777 - binary_accuracy: 0.7726 - val_loss: 0.4448 - val_binary_accuracy: 0.7906\n",
            "Epoch 39/60\n",
            "23428/23428 [==============================] - 0s 4us/step - loss: 0.4784 - binary_accuracy: 0.7717 - val_loss: 0.4451 - val_binary_accuracy: 0.7909\n",
            "Epoch 40/60\n",
            "23428/23428 [==============================] - 0s 5us/step - loss: 0.4771 - binary_accuracy: 0.7730 - val_loss: 0.4444 - val_binary_accuracy: 0.7899\n",
            "Epoch 41/60\n",
            "23428/23428 [==============================] - 0s 5us/step - loss: 0.4794 - binary_accuracy: 0.7731 - val_loss: 0.4440 - val_binary_accuracy: 0.7911\n",
            "Epoch 42/60\n",
            "23428/23428 [==============================] - 0s 5us/step - loss: 0.4773 - binary_accuracy: 0.7716 - val_loss: 0.4446 - val_binary_accuracy: 0.7919\n",
            "Epoch 43/60\n",
            "23428/23428 [==============================] - 0s 4us/step - loss: 0.4787 - binary_accuracy: 0.7719 - val_loss: 0.4444 - val_binary_accuracy: 0.7906\n",
            "Epoch 44/60\n",
            "23428/23428 [==============================] - 0s 5us/step - loss: 0.4781 - binary_accuracy: 0.7708 - val_loss: 0.4439 - val_binary_accuracy: 0.7904\n",
            "Epoch 45/60\n",
            "23428/23428 [==============================] - 0s 4us/step - loss: 0.4751 - binary_accuracy: 0.7740 - val_loss: 0.4426 - val_binary_accuracy: 0.7894\n",
            "Epoch 46/60\n",
            "23428/23428 [==============================] - 0s 4us/step - loss: 0.4796 - binary_accuracy: 0.7732 - val_loss: 0.4446 - val_binary_accuracy: 0.7912\n",
            "Epoch 47/60\n",
            "23428/23428 [==============================] - 0s 4us/step - loss: 0.4748 - binary_accuracy: 0.7716 - val_loss: 0.4425 - val_binary_accuracy: 0.7904\n",
            "Epoch 48/60\n",
            "23428/23428 [==============================] - 0s 4us/step - loss: 0.4790 - binary_accuracy: 0.7707 - val_loss: 0.4428 - val_binary_accuracy: 0.7924\n",
            "Epoch 49/60\n",
            "23428/23428 [==============================] - 0s 4us/step - loss: 0.4761 - binary_accuracy: 0.7732 - val_loss: 0.4430 - val_binary_accuracy: 0.7916\n",
            "Epoch 50/60\n",
            "23428/23428 [==============================] - 0s 4us/step - loss: 0.4756 - binary_accuracy: 0.7712 - val_loss: 0.4420 - val_binary_accuracy: 0.7909\n",
            "Epoch 51/60\n",
            "23428/23428 [==============================] - 0s 4us/step - loss: 0.4752 - binary_accuracy: 0.7733 - val_loss: 0.4424 - val_binary_accuracy: 0.7912\n",
            "Epoch 52/60\n",
            "23428/23428 [==============================] - 0s 4us/step - loss: 0.4764 - binary_accuracy: 0.7740 - val_loss: 0.4414 - val_binary_accuracy: 0.7928\n",
            "Epoch 53/60\n",
            "23428/23428 [==============================] - 0s 4us/step - loss: 0.4751 - binary_accuracy: 0.7714 - val_loss: 0.4408 - val_binary_accuracy: 0.7934\n",
            "Epoch 54/60\n",
            "23428/23428 [==============================] - 0s 5us/step - loss: 0.4760 - binary_accuracy: 0.7746 - val_loss: 0.4423 - val_binary_accuracy: 0.7885\n",
            "Epoch 55/60\n",
            "23428/23428 [==============================] - 0s 5us/step - loss: 0.4769 - binary_accuracy: 0.7712 - val_loss: 0.4412 - val_binary_accuracy: 0.7916\n",
            "Epoch 56/60\n",
            "23428/23428 [==============================] - 0s 5us/step - loss: 0.4765 - binary_accuracy: 0.7743 - val_loss: 0.4415 - val_binary_accuracy: 0.7915\n",
            "Epoch 57/60\n",
            "23428/23428 [==============================] - 0s 5us/step - loss: 0.4719 - binary_accuracy: 0.7724 - val_loss: 0.4407 - val_binary_accuracy: 0.7936\n",
            "Epoch 58/60\n",
            "23428/23428 [==============================] - 0s 5us/step - loss: 0.4724 - binary_accuracy: 0.7750 - val_loss: 0.4399 - val_binary_accuracy: 0.7930\n",
            "Epoch 59/60\n",
            "23428/23428 [==============================] - 0s 5us/step - loss: 0.4738 - binary_accuracy: 0.7706 - val_loss: 0.4453 - val_binary_accuracy: 0.7883\n",
            "Epoch 60/60\n",
            "23428/23428 [==============================] - 0s 4us/step - loss: 0.4756 - binary_accuracy: 0.7733 - val_loss: 0.4402 - val_binary_accuracy: 0.7918\n",
            "23428/23428 [==============================] - 1s 42us/step\n",
            "8000/8000 [==============================] - 0s 36us/step\n",
            "(23428, 31) (8000, 31) (13470, 31) (23428,) (8000,) (13470,)\n",
            "Train on 23428 samples, validate on 8000 samples\n",
            "Epoch 1/60\n",
            "23428/23428 [==============================] - 0s 6us/step - loss: 0.9365 - binary_accuracy: 0.5030 - val_loss: 0.9239 - val_binary_accuracy: 0.5235\n",
            "Epoch 2/60\n",
            "23428/23428 [==============================] - 0s 4us/step - loss: 0.9174 - binary_accuracy: 0.5257 - val_loss: 0.9028 - val_binary_accuracy: 0.5211\n",
            "Epoch 3/60\n",
            "23428/23428 [==============================] - 0s 4us/step - loss: 0.8969 - binary_accuracy: 0.5407 - val_loss: 0.8725 - val_binary_accuracy: 0.6780\n",
            "Epoch 4/60\n",
            "23428/23428 [==============================] - 0s 4us/step - loss: 0.8693 - binary_accuracy: 0.6024 - val_loss: 0.8322 - val_binary_accuracy: 0.6990\n",
            "Epoch 5/60\n",
            "23428/23428 [==============================] - 0s 4us/step - loss: 0.8351 - binary_accuracy: 0.6587 - val_loss: 0.7858 - val_binary_accuracy: 0.7285\n",
            "Epoch 6/60\n",
            "23428/23428 [==============================] - 0s 4us/step - loss: 0.8017 - binary_accuracy: 0.6806 - val_loss: 0.7389 - val_binary_accuracy: 0.7582\n",
            "Epoch 7/60\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "23428/23428 [==============================] - 0s 4us/step - loss: 0.7633 - binary_accuracy: 0.7043 - val_loss: 0.6946 - val_binary_accuracy: 0.7616\n",
            "Epoch 8/60\n",
            "23428/23428 [==============================] - 0s 5us/step - loss: 0.7304 - binary_accuracy: 0.7188 - val_loss: 0.6593 - val_binary_accuracy: 0.7716\n",
            "Epoch 9/60\n",
            "23428/23428 [==============================] - 0s 5us/step - loss: 0.7085 - binary_accuracy: 0.7308 - val_loss: 0.6381 - val_binary_accuracy: 0.7754\n",
            "Epoch 10/60\n",
            "23428/23428 [==============================] - 0s 4us/step - loss: 0.6848 - binary_accuracy: 0.7418 - val_loss: 0.6227 - val_binary_accuracy: 0.7750\n",
            "Epoch 11/60\n",
            "23428/23428 [==============================] - 0s 4us/step - loss: 0.6695 - binary_accuracy: 0.7430 - val_loss: 0.6098 - val_binary_accuracy: 0.7754\n",
            "Epoch 12/60\n",
            "23428/23428 [==============================] - 0s 4us/step - loss: 0.6511 - binary_accuracy: 0.7517 - val_loss: 0.5980 - val_binary_accuracy: 0.7766\n",
            "Epoch 13/60\n",
            "23428/23428 [==============================] - 0s 4us/step - loss: 0.6417 - binary_accuracy: 0.7534 - val_loss: 0.5881 - val_binary_accuracy: 0.7793\n",
            "Epoch 14/60\n",
            "23428/23428 [==============================] - 0s 5us/step - loss: 0.6333 - binary_accuracy: 0.7563 - val_loss: 0.5795 - val_binary_accuracy: 0.7811\n",
            "Epoch 15/60\n",
            "23428/23428 [==============================] - 0s 4us/step - loss: 0.6209 - binary_accuracy: 0.7554 - val_loss: 0.5725 - val_binary_accuracy: 0.7804\n",
            "Epoch 16/60\n",
            "23428/23428 [==============================] - 0s 5us/step - loss: 0.6144 - binary_accuracy: 0.7597 - val_loss: 0.5653 - val_binary_accuracy: 0.7847\n",
            "Epoch 17/60\n",
            "23428/23428 [==============================] - 0s 5us/step - loss: 0.6076 - binary_accuracy: 0.7593 - val_loss: 0.5587 - val_binary_accuracy: 0.7849\n",
            "Epoch 18/60\n",
            "23428/23428 [==============================] - 0s 5us/step - loss: 0.6003 - binary_accuracy: 0.7594 - val_loss: 0.5533 - val_binary_accuracy: 0.7850\n",
            "Epoch 19/60\n",
            "23428/23428 [==============================] - 0s 5us/step - loss: 0.5956 - binary_accuracy: 0.7608 - val_loss: 0.5489 - val_binary_accuracy: 0.7859\n",
            "Epoch 20/60\n",
            "23428/23428 [==============================] - 0s 5us/step - loss: 0.5862 - binary_accuracy: 0.7646 - val_loss: 0.5425 - val_binary_accuracy: 0.7860\n",
            "Epoch 21/60\n",
            "23428/23428 [==============================] - 0s 5us/step - loss: 0.5848 - binary_accuracy: 0.7636 - val_loss: 0.5385 - val_binary_accuracy: 0.7870\n",
            "Epoch 22/60\n",
            "23428/23428 [==============================] - 0s 5us/step - loss: 0.5773 - binary_accuracy: 0.7638 - val_loss: 0.5344 - val_binary_accuracy: 0.7871\n",
            "Epoch 23/60\n",
            "23428/23428 [==============================] - 0s 4us/step - loss: 0.5735 - binary_accuracy: 0.7635 - val_loss: 0.5300 - val_binary_accuracy: 0.7862\n",
            "Epoch 24/60\n",
            "23428/23428 [==============================] - 0s 5us/step - loss: 0.5698 - binary_accuracy: 0.7637 - val_loss: 0.5269 - val_binary_accuracy: 0.7887\n",
            "Epoch 25/60\n",
            "23428/23428 [==============================] - 0s 5us/step - loss: 0.5667 - binary_accuracy: 0.7652 - val_loss: 0.5240 - val_binary_accuracy: 0.7872\n",
            "Epoch 26/60\n",
            "23428/23428 [==============================] - 0s 4us/step - loss: 0.5633 - binary_accuracy: 0.7655 - val_loss: 0.5202 - val_binary_accuracy: 0.7880\n",
            "Epoch 27/60\n",
            "23428/23428 [==============================] - 0s 4us/step - loss: 0.5591 - binary_accuracy: 0.7651 - val_loss: 0.5184 - val_binary_accuracy: 0.7869\n",
            "Epoch 28/60\n",
            "23428/23428 [==============================] - 0s 4us/step - loss: 0.5545 - binary_accuracy: 0.7649 - val_loss: 0.5141 - val_binary_accuracy: 0.7899\n",
            "Epoch 29/60\n",
            "23428/23428 [==============================] - 0s 4us/step - loss: 0.5496 - binary_accuracy: 0.7658 - val_loss: 0.5103 - val_binary_accuracy: 0.7897\n",
            "Epoch 30/60\n",
            "23428/23428 [==============================] - 0s 5us/step - loss: 0.5519 - binary_accuracy: 0.7671 - val_loss: 0.5086 - val_binary_accuracy: 0.7884\n",
            "Epoch 31/60\n",
            "23428/23428 [==============================] - 0s 5us/step - loss: 0.5479 - binary_accuracy: 0.7667 - val_loss: 0.5072 - val_binary_accuracy: 0.7876\n",
            "Epoch 32/60\n",
            "23428/23428 [==============================] - 0s 4us/step - loss: 0.5452 - binary_accuracy: 0.7660 - val_loss: 0.5042 - val_binary_accuracy: 0.7884\n",
            "Epoch 33/60\n",
            "23428/23428 [==============================] - 0s 4us/step - loss: 0.5422 - binary_accuracy: 0.7676 - val_loss: 0.5016 - val_binary_accuracy: 0.7890\n",
            "Epoch 34/60\n",
            "23428/23428 [==============================] - 0s 4us/step - loss: 0.5368 - binary_accuracy: 0.7695 - val_loss: 0.4982 - val_binary_accuracy: 0.7905\n",
            "Epoch 35/60\n",
            "23428/23428 [==============================] - 0s 4us/step - loss: 0.5371 - binary_accuracy: 0.7663 - val_loss: 0.4982 - val_binary_accuracy: 0.7905\n",
            "Epoch 36/60\n",
            "23428/23428 [==============================] - 0s 5us/step - loss: 0.5324 - binary_accuracy: 0.7672 - val_loss: 0.4949 - val_binary_accuracy: 0.7900\n",
            "Epoch 37/60\n",
            "23428/23428 [==============================] - 0s 5us/step - loss: 0.5339 - binary_accuracy: 0.7687 - val_loss: 0.4939 - val_binary_accuracy: 0.7899\n",
            "Epoch 38/60\n",
            "23428/23428 [==============================] - 0s 4us/step - loss: 0.5318 - binary_accuracy: 0.7695 - val_loss: 0.4911 - val_binary_accuracy: 0.7897\n",
            "Epoch 39/60\n",
            "23428/23428 [==============================] - 0s 5us/step - loss: 0.5255 - binary_accuracy: 0.7686 - val_loss: 0.4892 - val_binary_accuracy: 0.7908\n",
            "Epoch 40/60\n",
            "23428/23428 [==============================] - 0s 4us/step - loss: 0.5247 - binary_accuracy: 0.7695 - val_loss: 0.4872 - val_binary_accuracy: 0.7896\n",
            "Epoch 41/60\n",
            "23428/23428 [==============================] - 0s 5us/step - loss: 0.5227 - binary_accuracy: 0.7683 - val_loss: 0.4866 - val_binary_accuracy: 0.7889\n",
            "Epoch 42/60\n",
            "23428/23428 [==============================] - 0s 5us/step - loss: 0.5205 - binary_accuracy: 0.7690 - val_loss: 0.4831 - val_binary_accuracy: 0.7918\n",
            "Epoch 43/60\n",
            "23428/23428 [==============================] - 0s 4us/step - loss: 0.5170 - binary_accuracy: 0.7688 - val_loss: 0.4815 - val_binary_accuracy: 0.7921\n",
            "Epoch 44/60\n",
            "23428/23428 [==============================] - 0s 5us/step - loss: 0.5164 - binary_accuracy: 0.7700 - val_loss: 0.4799 - val_binary_accuracy: 0.7919\n",
            "Epoch 45/60\n",
            "23428/23428 [==============================] - 0s 5us/step - loss: 0.5170 - binary_accuracy: 0.7697 - val_loss: 0.4796 - val_binary_accuracy: 0.7890\n",
            "Epoch 46/60\n",
            "23428/23428 [==============================] - 0s 5us/step - loss: 0.5161 - binary_accuracy: 0.7701 - val_loss: 0.4771 - val_binary_accuracy: 0.7906\n",
            "Epoch 47/60\n",
            "23428/23428 [==============================] - 0s 5us/step - loss: 0.5145 - binary_accuracy: 0.7703 - val_loss: 0.4762 - val_binary_accuracy: 0.7910\n",
            "Epoch 48/60\n",
            "23428/23428 [==============================] - 0s 4us/step - loss: 0.5128 - binary_accuracy: 0.7708 - val_loss: 0.4742 - val_binary_accuracy: 0.7909\n",
            "Epoch 49/60\n",
            "23428/23428 [==============================] - 0s 5us/step - loss: 0.5090 - binary_accuracy: 0.7707 - val_loss: 0.4728 - val_binary_accuracy: 0.7911\n",
            "Epoch 50/60\n",
            "23428/23428 [==============================] - 0s 4us/step - loss: 0.5093 - binary_accuracy: 0.7742 - val_loss: 0.4725 - val_binary_accuracy: 0.7909\n",
            "Epoch 51/60\n",
            "23428/23428 [==============================] - 0s 4us/step - loss: 0.5059 - binary_accuracy: 0.7710 - val_loss: 0.4706 - val_binary_accuracy: 0.7926\n",
            "Epoch 52/60\n",
            "23428/23428 [==============================] - 0s 5us/step - loss: 0.5038 - binary_accuracy: 0.7739 - val_loss: 0.4691 - val_binary_accuracy: 0.7912\n",
            "Epoch 53/60\n",
            "23428/23428 [==============================] - 0s 4us/step - loss: 0.5033 - binary_accuracy: 0.7731 - val_loss: 0.4675 - val_binary_accuracy: 0.7910\n",
            "Epoch 54/60\n",
            "23428/23428 [==============================] - 0s 4us/step - loss: 0.5035 - binary_accuracy: 0.7725 - val_loss: 0.4666 - val_binary_accuracy: 0.7914\n",
            "Epoch 55/60\n",
            "23428/23428 [==============================] - 0s 5us/step - loss: 0.5009 - binary_accuracy: 0.7734 - val_loss: 0.4650 - val_binary_accuracy: 0.7925\n",
            "Epoch 56/60\n",
            "23428/23428 [==============================] - 0s 4us/step - loss: 0.4997 - binary_accuracy: 0.7728 - val_loss: 0.4642 - val_binary_accuracy: 0.7909\n",
            "Epoch 57/60\n",
            "23428/23428 [==============================] - 0s 4us/step - loss: 0.4999 - binary_accuracy: 0.7710 - val_loss: 0.4640 - val_binary_accuracy: 0.7918\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 58/60\n",
            "23428/23428 [==============================] - 0s 5us/step - loss: 0.4960 - binary_accuracy: 0.7718 - val_loss: 0.4621 - val_binary_accuracy: 0.7903\n",
            "Epoch 59/60\n",
            "23428/23428 [==============================] - 0s 4us/step - loss: 0.4998 - binary_accuracy: 0.7718 - val_loss: 0.4620 - val_binary_accuracy: 0.7925\n",
            "Epoch 60/60\n",
            "23428/23428 [==============================] - 0s 5us/step - loss: 0.4971 - binary_accuracy: 0.7698 - val_loss: 0.4610 - val_binary_accuracy: 0.7920\n",
            "23428/23428 [==============================] - 1s 43us/step\n",
            "8000/8000 [==============================] - 0s 48us/step\n",
            "(23428, 31) (8000, 31) (13470, 31) (23428,) (8000,) (13470,)\n",
            "Train on 23428 samples, validate on 8000 samples\n",
            "Epoch 1/60\n",
            "23428/23428 [==============================] - 0s 5us/step - loss: 0.9378 - binary_accuracy: 0.4947 - val_loss: 0.9271 - val_binary_accuracy: 0.5094\n",
            "Epoch 2/60\n",
            "23428/23428 [==============================] - 0s 4us/step - loss: 0.9232 - binary_accuracy: 0.5169 - val_loss: 0.9129 - val_binary_accuracy: 0.5195\n",
            "Epoch 3/60\n",
            "23428/23428 [==============================] - 0s 4us/step - loss: 0.9073 - binary_accuracy: 0.5300 - val_loss: 0.8931 - val_binary_accuracy: 0.5204\n",
            "Epoch 4/60\n",
            "23428/23428 [==============================] - 0s 4us/step - loss: 0.8894 - binary_accuracy: 0.5457 - val_loss: 0.8673 - val_binary_accuracy: 0.6586\n",
            "Epoch 5/60\n",
            "23428/23428 [==============================] - 0s 4us/step - loss: 0.8665 - binary_accuracy: 0.6086 - val_loss: 0.8351 - val_binary_accuracy: 0.6948\n",
            "Epoch 6/60\n",
            "23428/23428 [==============================] - 0s 4us/step - loss: 0.8422 - binary_accuracy: 0.6434 - val_loss: 0.7997 - val_binary_accuracy: 0.7117\n",
            "Epoch 7/60\n",
            "23428/23428 [==============================] - 0s 4us/step - loss: 0.8158 - binary_accuracy: 0.6719 - val_loss: 0.7643 - val_binary_accuracy: 0.7483\n",
            "Epoch 8/60\n",
            "23428/23428 [==============================] - 0s 4us/step - loss: 0.7894 - binary_accuracy: 0.6907 - val_loss: 0.7299 - val_binary_accuracy: 0.7594\n",
            "Epoch 9/60\n",
            "23428/23428 [==============================] - 0s 4us/step - loss: 0.7625 - binary_accuracy: 0.7042 - val_loss: 0.6988 - val_binary_accuracy: 0.7665\n",
            "Epoch 10/60\n",
            "23428/23428 [==============================] - 0s 4us/step - loss: 0.7377 - binary_accuracy: 0.7175 - val_loss: 0.6733 - val_binary_accuracy: 0.7759\n",
            "Epoch 11/60\n",
            "23428/23428 [==============================] - 0s 4us/step - loss: 0.7128 - binary_accuracy: 0.7282 - val_loss: 0.6521 - val_binary_accuracy: 0.7763\n",
            "Epoch 12/60\n",
            "23428/23428 [==============================] - 0s 4us/step - loss: 0.6958 - binary_accuracy: 0.7340 - val_loss: 0.6362 - val_binary_accuracy: 0.7760\n",
            "Epoch 13/60\n",
            "23428/23428 [==============================] - 0s 4us/step - loss: 0.6838 - binary_accuracy: 0.7394 - val_loss: 0.6229 - val_binary_accuracy: 0.7763\n",
            "Epoch 14/60\n",
            "23428/23428 [==============================] - 0s 4us/step - loss: 0.6692 - binary_accuracy: 0.7442 - val_loss: 0.6128 - val_binary_accuracy: 0.7759\n",
            "Epoch 15/60\n",
            "23428/23428 [==============================] - 0s 4us/step - loss: 0.6615 - binary_accuracy: 0.7451 - val_loss: 0.6032 - val_binary_accuracy: 0.7784\n",
            "Epoch 16/60\n",
            "23428/23428 [==============================] - 0s 4us/step - loss: 0.6502 - binary_accuracy: 0.7499 - val_loss: 0.5941 - val_binary_accuracy: 0.7790\n",
            "Epoch 17/60\n",
            "23428/23428 [==============================] - 0s 4us/step - loss: 0.6390 - binary_accuracy: 0.7530 - val_loss: 0.5860 - val_binary_accuracy: 0.7807\n",
            "Epoch 18/60\n",
            "23428/23428 [==============================] - 0s 4us/step - loss: 0.6310 - binary_accuracy: 0.7540 - val_loss: 0.5774 - val_binary_accuracy: 0.7812\n",
            "Epoch 19/60\n",
            "23428/23428 [==============================] - 0s 4us/step - loss: 0.6250 - binary_accuracy: 0.7544 - val_loss: 0.5702 - val_binary_accuracy: 0.7814\n",
            "Epoch 20/60\n",
            "23428/23428 [==============================] - 0s 4us/step - loss: 0.6194 - binary_accuracy: 0.7540 - val_loss: 0.5637 - val_binary_accuracy: 0.7821\n",
            "Epoch 21/60\n",
            "23428/23428 [==============================] - 0s 4us/step - loss: 0.6099 - binary_accuracy: 0.7547 - val_loss: 0.5576 - val_binary_accuracy: 0.7843\n",
            "Epoch 22/60\n",
            "23428/23428 [==============================] - 0s 4us/step - loss: 0.5984 - binary_accuracy: 0.7584 - val_loss: 0.5519 - val_binary_accuracy: 0.7844\n",
            "Epoch 23/60\n",
            "23428/23428 [==============================] - 0s 5us/step - loss: 0.5953 - binary_accuracy: 0.7591 - val_loss: 0.5467 - val_binary_accuracy: 0.7844\n",
            "Epoch 24/60\n",
            "23428/23428 [==============================] - 0s 5us/step - loss: 0.5902 - binary_accuracy: 0.7602 - val_loss: 0.5422 - val_binary_accuracy: 0.7859\n",
            "Epoch 25/60\n",
            "23428/23428 [==============================] - 0s 4us/step - loss: 0.5843 - binary_accuracy: 0.7600 - val_loss: 0.5372 - val_binary_accuracy: 0.7845\n",
            "Epoch 26/60\n",
            "23428/23428 [==============================] - 0s 4us/step - loss: 0.5800 - binary_accuracy: 0.7607 - val_loss: 0.5321 - val_binary_accuracy: 0.7855\n",
            "Epoch 27/60\n",
            "23428/23428 [==============================] - 0s 4us/step - loss: 0.5738 - binary_accuracy: 0.7611 - val_loss: 0.5278 - val_binary_accuracy: 0.7856\n",
            "Epoch 28/60\n",
            "23428/23428 [==============================] - 0s 4us/step - loss: 0.5670 - binary_accuracy: 0.7634 - val_loss: 0.5237 - val_binary_accuracy: 0.7869\n",
            "Epoch 29/60\n",
            "23428/23428 [==============================] - 0s 4us/step - loss: 0.5618 - binary_accuracy: 0.7655 - val_loss: 0.5195 - val_binary_accuracy: 0.7866\n",
            "Epoch 30/60\n",
            "23428/23428 [==============================] - 0s 4us/step - loss: 0.5615 - binary_accuracy: 0.7622 - val_loss: 0.5164 - val_binary_accuracy: 0.7857\n",
            "Epoch 31/60\n",
            "23428/23428 [==============================] - 0s 4us/step - loss: 0.5553 - binary_accuracy: 0.7655 - val_loss: 0.5129 - val_binary_accuracy: 0.7862\n",
            "Epoch 32/60\n",
            "23428/23428 [==============================] - 0s 4us/step - loss: 0.5523 - binary_accuracy: 0.7645 - val_loss: 0.5097 - val_binary_accuracy: 0.7881\n",
            "Epoch 33/60\n",
            "23428/23428 [==============================] - 0s 4us/step - loss: 0.5481 - binary_accuracy: 0.7629 - val_loss: 0.5068 - val_binary_accuracy: 0.7883\n",
            "Epoch 34/60\n",
            "23428/23428 [==============================] - 0s 4us/step - loss: 0.5467 - binary_accuracy: 0.7638 - val_loss: 0.5036 - val_binary_accuracy: 0.7885\n",
            "Epoch 35/60\n",
            "23428/23428 [==============================] - 0s 4us/step - loss: 0.5427 - binary_accuracy: 0.7665 - val_loss: 0.5003 - val_binary_accuracy: 0.7886\n",
            "Epoch 36/60\n",
            "23428/23428 [==============================] - 0s 4us/step - loss: 0.5393 - binary_accuracy: 0.7665 - val_loss: 0.4975 - val_binary_accuracy: 0.7872\n",
            "Epoch 37/60\n",
            "23428/23428 [==============================] - 0s 4us/step - loss: 0.5337 - binary_accuracy: 0.7672 - val_loss: 0.4954 - val_binary_accuracy: 0.7885\n",
            "Epoch 38/60\n",
            "23428/23428 [==============================] - 0s 4us/step - loss: 0.5323 - binary_accuracy: 0.7660 - val_loss: 0.4925 - val_binary_accuracy: 0.7879\n",
            "Epoch 39/60\n",
            "23428/23428 [==============================] - 0s 4us/step - loss: 0.5307 - binary_accuracy: 0.7691 - val_loss: 0.4896 - val_binary_accuracy: 0.7875\n",
            "Epoch 40/60\n",
            "23428/23428 [==============================] - 0s 4us/step - loss: 0.5282 - binary_accuracy: 0.7673 - val_loss: 0.4871 - val_binary_accuracy: 0.7881\n",
            "Epoch 41/60\n",
            "23428/23428 [==============================] - 0s 4us/step - loss: 0.5279 - binary_accuracy: 0.7702 - val_loss: 0.4851 - val_binary_accuracy: 0.7906\n",
            "Epoch 42/60\n",
            "23428/23428 [==============================] - 0s 4us/step - loss: 0.5240 - binary_accuracy: 0.7672 - val_loss: 0.4831 - val_binary_accuracy: 0.7910\n",
            "Epoch 43/60\n",
            "23428/23428 [==============================] - 0s 4us/step - loss: 0.5216 - binary_accuracy: 0.7663 - val_loss: 0.4814 - val_binary_accuracy: 0.7891\n",
            "Epoch 44/60\n",
            "23428/23428 [==============================] - 0s 4us/step - loss: 0.5208 - binary_accuracy: 0.7665 - val_loss: 0.4798 - val_binary_accuracy: 0.7900\n",
            "Epoch 45/60\n",
            "23428/23428 [==============================] - 0s 4us/step - loss: 0.5160 - binary_accuracy: 0.7695 - val_loss: 0.4774 - val_binary_accuracy: 0.7899\n",
            "Epoch 46/60\n",
            "23428/23428 [==============================] - 0s 4us/step - loss: 0.5154 - binary_accuracy: 0.7672 - val_loss: 0.4749 - val_binary_accuracy: 0.7903\n",
            "Epoch 47/60\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "23428/23428 [==============================] - 0s 4us/step - loss: 0.5129 - binary_accuracy: 0.7697 - val_loss: 0.4735 - val_binary_accuracy: 0.7899\n",
            "Epoch 48/60\n",
            "23428/23428 [==============================] - 0s 5us/step - loss: 0.5071 - binary_accuracy: 0.7714 - val_loss: 0.4716 - val_binary_accuracy: 0.7885\n",
            "Epoch 49/60\n",
            "23428/23428 [==============================] - 0s 5us/step - loss: 0.5093 - binary_accuracy: 0.7683 - val_loss: 0.4700 - val_binary_accuracy: 0.7912\n",
            "Epoch 50/60\n",
            "23428/23428 [==============================] - 0s 4us/step - loss: 0.5073 - binary_accuracy: 0.7707 - val_loss: 0.4694 - val_binary_accuracy: 0.7901\n",
            "Epoch 51/60\n",
            "23428/23428 [==============================] - 0s 5us/step - loss: 0.5066 - binary_accuracy: 0.7679 - val_loss: 0.4678 - val_binary_accuracy: 0.7903\n",
            "Epoch 52/60\n",
            "23428/23428 [==============================] - 0s 5us/step - loss: 0.4993 - binary_accuracy: 0.7712 - val_loss: 0.4659 - val_binary_accuracy: 0.7915\n",
            "Epoch 53/60\n",
            "23428/23428 [==============================] - 0s 5us/step - loss: 0.5025 - binary_accuracy: 0.7730 - val_loss: 0.4647 - val_binary_accuracy: 0.7920\n",
            "Epoch 54/60\n",
            "23428/23428 [==============================] - 0s 4us/step - loss: 0.5006 - binary_accuracy: 0.7726 - val_loss: 0.4643 - val_binary_accuracy: 0.7903\n",
            "Epoch 55/60\n",
            "23428/23428 [==============================] - 0s 4us/step - loss: 0.5004 - binary_accuracy: 0.7698 - val_loss: 0.4633 - val_binary_accuracy: 0.7912\n",
            "Epoch 56/60\n",
            "23428/23428 [==============================] - 0s 5us/step - loss: 0.5009 - binary_accuracy: 0.7695 - val_loss: 0.4615 - val_binary_accuracy: 0.7909\n",
            "Epoch 57/60\n",
            "23428/23428 [==============================] - 0s 4us/step - loss: 0.5001 - binary_accuracy: 0.7698 - val_loss: 0.4609 - val_binary_accuracy: 0.7916\n",
            "Epoch 58/60\n",
            "23428/23428 [==============================] - 0s 4us/step - loss: 0.4968 - binary_accuracy: 0.7713 - val_loss: 0.4595 - val_binary_accuracy: 0.7911\n",
            "Epoch 59/60\n",
            "23428/23428 [==============================] - 0s 4us/step - loss: 0.4928 - binary_accuracy: 0.7696 - val_loss: 0.4583 - val_binary_accuracy: 0.7922\n",
            "Epoch 60/60\n",
            "23428/23428 [==============================] - 0s 4us/step - loss: 0.4910 - binary_accuracy: 0.7684 - val_loss: 0.4577 - val_binary_accuracy: 0.7915\n",
            "23428/23428 [==============================] - 1s 40us/step\n",
            "8000/8000 [==============================] - 0s 41us/step\n",
            "\n",
            "\n",
            "Ensayando modelo con estructura [64, 32, 10, 1] y optimizador <keras.optimizers.RMSprop object at 0x00000180021B5C48>\n",
            "(23428, 31) (8000, 31) (13470, 31) (23428,) (8000,) (13470,)\n",
            "Train on 23428 samples, validate on 8000 samples\n",
            "Epoch 1/60\n",
            "23428/23428 [==============================] - 1s 59us/step - loss: 0.9193 - binary_accuracy: 0.5218 - val_loss: 0.8950 - val_binary_accuracy: 0.5909\n",
            "Epoch 2/60\n",
            "23428/23428 [==============================] - 0s 6us/step - loss: 0.8893 - binary_accuracy: 0.5558 - val_loss: 0.8658 - val_binary_accuracy: 0.6726\n",
            "Epoch 3/60\n",
            "23428/23428 [==============================] - 0s 4us/step - loss: 0.8645 - binary_accuracy: 0.5726 - val_loss: 0.8389 - val_binary_accuracy: 0.7125\n",
            "Epoch 4/60\n",
            "23428/23428 [==============================] - 0s 4us/step - loss: 0.8427 - binary_accuracy: 0.5936 - val_loss: 0.8110 - val_binary_accuracy: 0.7366\n",
            "Epoch 5/60\n",
            "23428/23428 [==============================] - 0s 4us/step - loss: 0.8212 - binary_accuracy: 0.6154 - val_loss: 0.7841 - val_binary_accuracy: 0.7552\n",
            "Epoch 6/60\n",
            "23428/23428 [==============================] - 0s 4us/step - loss: 0.7948 - binary_accuracy: 0.6461 - val_loss: 0.7523 - val_binary_accuracy: 0.7604\n",
            "Epoch 7/60\n",
            "23428/23428 [==============================] - 0s 4us/step - loss: 0.7723 - binary_accuracy: 0.6599 - val_loss: 0.7217 - val_binary_accuracy: 0.7779\n",
            "Epoch 8/60\n",
            "23428/23428 [==============================] - 0s 4us/step - loss: 0.7487 - binary_accuracy: 0.6767 - val_loss: 0.6915 - val_binary_accuracy: 0.7768\n",
            "Epoch 9/60\n",
            "23428/23428 [==============================] - 0s 4us/step - loss: 0.7244 - binary_accuracy: 0.6944 - val_loss: 0.6635 - val_binary_accuracy: 0.7788\n",
            "Epoch 10/60\n",
            "23428/23428 [==============================] - 0s 4us/step - loss: 0.7039 - binary_accuracy: 0.7051 - val_loss: 0.6381 - val_binary_accuracy: 0.7800\n",
            "Epoch 11/60\n",
            "23428/23428 [==============================] - 0s 4us/step - loss: 0.6809 - binary_accuracy: 0.7124 - val_loss: 0.6143 - val_binary_accuracy: 0.7811\n",
            "Epoch 12/60\n",
            "23428/23428 [==============================] - 0s 4us/step - loss: 0.6618 - binary_accuracy: 0.7255 - val_loss: 0.5938 - val_binary_accuracy: 0.7844\n",
            "Epoch 13/60\n",
            "23428/23428 [==============================] - 0s 4us/step - loss: 0.6427 - binary_accuracy: 0.7317 - val_loss: 0.5769 - val_binary_accuracy: 0.7843\n",
            "Epoch 14/60\n",
            "23428/23428 [==============================] - ETA: 0s - loss: 0.6361 - binary_accuracy: 0.733 - 0s 4us/step - loss: 0.6324 - binary_accuracy: 0.7348 - val_loss: 0.5639 - val_binary_accuracy: 0.7825\n",
            "Epoch 15/60\n",
            "23428/23428 [==============================] - 0s 4us/step - loss: 0.6167 - binary_accuracy: 0.7412 - val_loss: 0.5494 - val_binary_accuracy: 0.7845\n",
            "Epoch 16/60\n",
            "23428/23428 [==============================] - 0s 4us/step - loss: 0.6020 - binary_accuracy: 0.7469 - val_loss: 0.5380 - val_binary_accuracy: 0.7853\n",
            "Epoch 17/60\n",
            "23428/23428 [==============================] - 0s 4us/step - loss: 0.5895 - binary_accuracy: 0.7513 - val_loss: 0.5283 - val_binary_accuracy: 0.7859\n",
            "Epoch 18/60\n",
            "23428/23428 [==============================] - 0s 4us/step - loss: 0.5810 - binary_accuracy: 0.7537 - val_loss: 0.5197 - val_binary_accuracy: 0.7881\n",
            "Epoch 19/60\n",
            "23428/23428 [==============================] - 0s 4us/step - loss: 0.5712 - binary_accuracy: 0.7545 - val_loss: 0.5120 - val_binary_accuracy: 0.7885\n",
            "Epoch 20/60\n",
            "23428/23428 [==============================] - 0s 4us/step - loss: 0.5658 - binary_accuracy: 0.7561 - val_loss: 0.5066 - val_binary_accuracy: 0.7897\n",
            "Epoch 21/60\n",
            "23428/23428 [==============================] - 0s 4us/step - loss: 0.5561 - binary_accuracy: 0.7571 - val_loss: 0.5005 - val_binary_accuracy: 0.7894\n",
            "Epoch 22/60\n",
            "23428/23428 [==============================] - 0s 4us/step - loss: 0.5507 - binary_accuracy: 0.7585 - val_loss: 0.4945 - val_binary_accuracy: 0.7897\n",
            "Epoch 23/60\n",
            "23428/23428 [==============================] - 0s 4us/step - loss: 0.5413 - binary_accuracy: 0.7611 - val_loss: 0.4885 - val_binary_accuracy: 0.7881\n",
            "Epoch 24/60\n",
            "23428/23428 [==============================] - 0s 4us/step - loss: 0.5344 - binary_accuracy: 0.7631 - val_loss: 0.4851 - val_binary_accuracy: 0.7890\n",
            "Epoch 25/60\n",
            "23428/23428 [==============================] - 0s 4us/step - loss: 0.5309 - binary_accuracy: 0.7602 - val_loss: 0.4810 - val_binary_accuracy: 0.7891\n",
            "Epoch 26/60\n",
            "23428/23428 [==============================] - 0s 4us/step - loss: 0.5263 - binary_accuracy: 0.7611 - val_loss: 0.4771 - val_binary_accuracy: 0.7890\n",
            "Epoch 27/60\n",
            "23428/23428 [==============================] - 0s 4us/step - loss: 0.5204 - binary_accuracy: 0.7643 - val_loss: 0.4734 - val_binary_accuracy: 0.7895\n",
            "Epoch 28/60\n",
            "23428/23428 [==============================] - 0s 4us/step - loss: 0.5186 - binary_accuracy: 0.7662 - val_loss: 0.4710 - val_binary_accuracy: 0.7879\n",
            "Epoch 29/60\n",
            "23428/23428 [==============================] - 0s 4us/step - loss: 0.5146 - binary_accuracy: 0.7639 - val_loss: 0.4688 - val_binary_accuracy: 0.7894\n",
            "Epoch 30/60\n",
            "23428/23428 [==============================] - 0s 4us/step - loss: 0.5148 - binary_accuracy: 0.7658 - val_loss: 0.4669 - val_binary_accuracy: 0.7899\n",
            "Epoch 31/60\n",
            "23428/23428 [==============================] - 0s 4us/step - loss: 0.5105 - binary_accuracy: 0.7641 - val_loss: 0.4655 - val_binary_accuracy: 0.7894\n",
            "Epoch 32/60\n",
            "23428/23428 [==============================] - 0s 4us/step - loss: 0.5103 - binary_accuracy: 0.7665 - val_loss: 0.4648 - val_binary_accuracy: 0.7887\n",
            "Epoch 33/60\n",
            "23428/23428 [==============================] - 0s 4us/step - loss: 0.5068 - binary_accuracy: 0.7672 - val_loss: 0.4631 - val_binary_accuracy: 0.7890\n",
            "Epoch 34/60\n",
            "23428/23428 [==============================] - 0s 4us/step - loss: 0.5007 - binary_accuracy: 0.7659 - val_loss: 0.4615 - val_binary_accuracy: 0.7895\n",
            "Epoch 35/60\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "23428/23428 [==============================] - 0s 4us/step - loss: 0.4997 - binary_accuracy: 0.7692 - val_loss: 0.4612 - val_binary_accuracy: 0.7905\n",
            "Epoch 36/60\n",
            "23428/23428 [==============================] - 0s 5us/step - loss: 0.4972 - binary_accuracy: 0.7673 - val_loss: 0.4605 - val_binary_accuracy: 0.7890\n",
            "Epoch 37/60\n",
            "23428/23428 [==============================] - 0s 4us/step - loss: 0.5010 - binary_accuracy: 0.7673 - val_loss: 0.4599 - val_binary_accuracy: 0.7906\n",
            "Epoch 38/60\n",
            "23428/23428 [==============================] - 0s 4us/step - loss: 0.5004 - binary_accuracy: 0.7703 - val_loss: 0.4594 - val_binary_accuracy: 0.7894\n",
            "Epoch 39/60\n",
            "23428/23428 [==============================] - 0s 4us/step - loss: 0.4994 - binary_accuracy: 0.7667 - val_loss: 0.4574 - val_binary_accuracy: 0.7881\n",
            "Epoch 40/60\n",
            "23428/23428 [==============================] - 0s 4us/step - loss: 0.4976 - binary_accuracy: 0.7690 - val_loss: 0.4574 - val_binary_accuracy: 0.7906\n",
            "Epoch 41/60\n",
            "23428/23428 [==============================] - 0s 5us/step - loss: 0.4969 - binary_accuracy: 0.7684 - val_loss: 0.4557 - val_binary_accuracy: 0.7901\n",
            "Epoch 42/60\n",
            "23428/23428 [==============================] - 0s 4us/step - loss: 0.4944 - binary_accuracy: 0.7690 - val_loss: 0.4560 - val_binary_accuracy: 0.7914\n",
            "Epoch 43/60\n",
            "23428/23428 [==============================] - 0s 4us/step - loss: 0.4968 - binary_accuracy: 0.7673 - val_loss: 0.4553 - val_binary_accuracy: 0.7912\n",
            "Epoch 44/60\n",
            "23428/23428 [==============================] - 0s 4us/step - loss: 0.4942 - binary_accuracy: 0.7665 - val_loss: 0.4542 - val_binary_accuracy: 0.7906\n",
            "Epoch 45/60\n",
            "23428/23428 [==============================] - 0s 4us/step - loss: 0.4913 - binary_accuracy: 0.7700 - val_loss: 0.4535 - val_binary_accuracy: 0.7899\n",
            "Epoch 46/60\n",
            "23428/23428 [==============================] - 0s 4us/step - loss: 0.4927 - binary_accuracy: 0.7692 - val_loss: 0.4528 - val_binary_accuracy: 0.7915\n",
            "Epoch 47/60\n",
            "23428/23428 [==============================] - 0s 5us/step - loss: 0.4914 - binary_accuracy: 0.7704 - val_loss: 0.4532 - val_binary_accuracy: 0.7911\n",
            "Epoch 48/60\n",
            "23428/23428 [==============================] - 0s 4us/step - loss: 0.4913 - binary_accuracy: 0.7700 - val_loss: 0.4530 - val_binary_accuracy: 0.7916\n",
            "Epoch 49/60\n",
            "23428/23428 [==============================] - 0s 4us/step - loss: 0.4887 - binary_accuracy: 0.7722 - val_loss: 0.4527 - val_binary_accuracy: 0.7918\n",
            "Epoch 50/60\n",
            "23428/23428 [==============================] - 0s 4us/step - loss: 0.4889 - binary_accuracy: 0.7713 - val_loss: 0.4510 - val_binary_accuracy: 0.7916\n",
            "Epoch 51/60\n",
            "23428/23428 [==============================] - 0s 4us/step - loss: 0.4884 - binary_accuracy: 0.7692 - val_loss: 0.4507 - val_binary_accuracy: 0.7897\n",
            "Epoch 52/60\n",
            "23428/23428 [==============================] - 0s 4us/step - loss: 0.4909 - binary_accuracy: 0.7689 - val_loss: 0.4515 - val_binary_accuracy: 0.7905\n",
            "Epoch 53/60\n",
            "23428/23428 [==============================] - 0s 4us/step - loss: 0.4899 - binary_accuracy: 0.7683 - val_loss: 0.4506 - val_binary_accuracy: 0.7900\n",
            "Epoch 54/60\n",
            "23428/23428 [==============================] - 0s 4us/step - loss: 0.4883 - binary_accuracy: 0.7687 - val_loss: 0.4502 - val_binary_accuracy: 0.7901\n",
            "Epoch 55/60\n",
            "23428/23428 [==============================] - 0s 4us/step - loss: 0.4904 - binary_accuracy: 0.7693 - val_loss: 0.4519 - val_binary_accuracy: 0.7897\n",
            "Epoch 56/60\n",
            "23428/23428 [==============================] - 0s 4us/step - loss: 0.4881 - binary_accuracy: 0.7693 - val_loss: 0.4494 - val_binary_accuracy: 0.7899\n",
            "Epoch 57/60\n",
            "23428/23428 [==============================] - 0s 4us/step - loss: 0.4851 - binary_accuracy: 0.7692 - val_loss: 0.4491 - val_binary_accuracy: 0.7919\n",
            "Epoch 58/60\n",
            "23428/23428 [==============================] - 0s 5us/step - loss: 0.4811 - binary_accuracy: 0.7730 - val_loss: 0.4488 - val_binary_accuracy: 0.7900\n",
            "Epoch 59/60\n",
            "23428/23428 [==============================] - 0s 4us/step - loss: 0.4848 - binary_accuracy: 0.7699 - val_loss: 0.4497 - val_binary_accuracy: 0.7893\n",
            "Epoch 60/60\n",
            "23428/23428 [==============================] - 0s 4us/step - loss: 0.4832 - binary_accuracy: 0.7675 - val_loss: 0.4479 - val_binary_accuracy: 0.7904\n",
            "23428/23428 [==============================] - 1s 41us/step\n",
            "8000/8000 [==============================] - 0s 43us/step\n",
            "(23428, 31) (8000, 31) (13470, 31) (23428,) (8000,) (13470,)\n",
            "Train on 23428 samples, validate on 8000 samples\n",
            "Epoch 1/60\n",
            "23428/23428 [==============================] - 0s 6us/step - loss: 0.9304 - binary_accuracy: 0.5104 - val_loss: 0.9162 - val_binary_accuracy: 0.5859\n",
            "Epoch 2/60\n",
            "23428/23428 [==============================] - 0s 5us/step - loss: 0.9135 - binary_accuracy: 0.5248 - val_loss: 0.8966 - val_binary_accuracy: 0.5846\n",
            "Epoch 3/60\n",
            "23428/23428 [==============================] - 0s 4us/step - loss: 0.8956 - binary_accuracy: 0.5461 - val_loss: 0.8743 - val_binary_accuracy: 0.6035\n",
            "Epoch 4/60\n",
            "23428/23428 [==============================] - 0s 4us/step - loss: 0.8747 - binary_accuracy: 0.5849 - val_loss: 0.8492 - val_binary_accuracy: 0.7220\n",
            "Epoch 5/60\n",
            "23428/23428 [==============================] - 0s 4us/step - loss: 0.8558 - binary_accuracy: 0.6177 - val_loss: 0.8219 - val_binary_accuracy: 0.7410\n",
            "Epoch 6/60\n",
            "23428/23428 [==============================] - 0s 4us/step - loss: 0.8324 - binary_accuracy: 0.6333 - val_loss: 0.7931 - val_binary_accuracy: 0.7536\n",
            "Epoch 7/60\n",
            "23428/23428 [==============================] - 0s 4us/step - loss: 0.8067 - binary_accuracy: 0.6536 - val_loss: 0.7611 - val_binary_accuracy: 0.7646\n",
            "Epoch 8/60\n",
            "23428/23428 [==============================] - 0s 4us/step - loss: 0.7832 - binary_accuracy: 0.6648 - val_loss: 0.7301 - val_binary_accuracy: 0.7703\n",
            "Epoch 9/60\n",
            "23428/23428 [==============================] - 0s 4us/step - loss: 0.7583 - binary_accuracy: 0.6843 - val_loss: 0.6996 - val_binary_accuracy: 0.7699\n",
            "Epoch 10/60\n",
            "23428/23428 [==============================] - 0s 4us/step - loss: 0.7373 - binary_accuracy: 0.6907 - val_loss: 0.6731 - val_binary_accuracy: 0.7788\n",
            "Epoch 11/60\n",
            "23428/23428 [==============================] - 0s 4us/step - loss: 0.7161 - binary_accuracy: 0.7028 - val_loss: 0.6488 - val_binary_accuracy: 0.7781\n",
            "Epoch 12/60\n",
            "23428/23428 [==============================] - 0s 4us/step - loss: 0.6955 - binary_accuracy: 0.7120 - val_loss: 0.6257 - val_binary_accuracy: 0.7790\n",
            "Epoch 13/60\n",
            "23428/23428 [==============================] - 0s 4us/step - loss: 0.6760 - binary_accuracy: 0.7179 - val_loss: 0.6046 - val_binary_accuracy: 0.7821\n",
            "Epoch 14/60\n",
            "23428/23428 [==============================] - 0s 4us/step - loss: 0.6593 - binary_accuracy: 0.7249 - val_loss: 0.5878 - val_binary_accuracy: 0.7829\n",
            "Epoch 15/60\n",
            "23428/23428 [==============================] - 0s 4us/step - loss: 0.6444 - binary_accuracy: 0.7318 - val_loss: 0.5741 - val_binary_accuracy: 0.7801\n",
            "Epoch 16/60\n",
            "23428/23428 [==============================] - 0s 4us/step - loss: 0.6275 - binary_accuracy: 0.7399 - val_loss: 0.5610 - val_binary_accuracy: 0.7816\n",
            "Epoch 17/60\n",
            "23428/23428 [==============================] - 0s 4us/step - loss: 0.6150 - binary_accuracy: 0.7451 - val_loss: 0.5479 - val_binary_accuracy: 0.7871\n",
            "Epoch 18/60\n",
            "23428/23428 [==============================] - 0s 4us/step - loss: 0.6040 - binary_accuracy: 0.7440 - val_loss: 0.5387 - val_binary_accuracy: 0.7851\n",
            "Epoch 19/60\n",
            "23428/23428 [==============================] - 0s 4us/step - loss: 0.5892 - binary_accuracy: 0.7506 - val_loss: 0.5299 - val_binary_accuracy: 0.7850\n",
            "Epoch 20/60\n",
            "23428/23428 [==============================] - 0s 4us/step - loss: 0.5820 - binary_accuracy: 0.7496 - val_loss: 0.5222 - val_binary_accuracy: 0.7883\n",
            "Epoch 21/60\n",
            "23428/23428 [==============================] - 0s 4us/step - loss: 0.5725 - binary_accuracy: 0.7572 - val_loss: 0.5144 - val_binary_accuracy: 0.7866\n",
            "Epoch 22/60\n",
            "23428/23428 [==============================] - 0s 4us/step - loss: 0.5647 - binary_accuracy: 0.7557 - val_loss: 0.5087 - val_binary_accuracy: 0.7876\n",
            "Epoch 23/60\n",
            "23428/23428 [==============================] - 0s 4us/step - loss: 0.5555 - binary_accuracy: 0.7593 - val_loss: 0.5023 - val_binary_accuracy: 0.7886\n",
            "Epoch 24/60\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "23428/23428 [==============================] - 0s 4us/step - loss: 0.5494 - binary_accuracy: 0.7592 - val_loss: 0.4952 - val_binary_accuracy: 0.7876\n",
            "Epoch 25/60\n",
            "23428/23428 [==============================] - 0s 4us/step - loss: 0.5476 - binary_accuracy: 0.7573 - val_loss: 0.4918 - val_binary_accuracy: 0.7868\n",
            "Epoch 26/60\n",
            "23428/23428 [==============================] - 0s 5us/step - loss: 0.5395 - binary_accuracy: 0.7614 - val_loss: 0.4874 - val_binary_accuracy: 0.7883\n",
            "Epoch 27/60\n",
            "23428/23428 [==============================] - 0s 5us/step - loss: 0.5314 - binary_accuracy: 0.7620 - val_loss: 0.4822 - val_binary_accuracy: 0.7880\n",
            "Epoch 28/60\n",
            "23428/23428 [==============================] - 0s 5us/step - loss: 0.5259 - binary_accuracy: 0.7632 - val_loss: 0.4785 - val_binary_accuracy: 0.7894\n",
            "Epoch 29/60\n",
            "23428/23428 [==============================] - 0s 5us/step - loss: 0.5233 - binary_accuracy: 0.7628 - val_loss: 0.4755 - val_binary_accuracy: 0.7879\n",
            "Epoch 30/60\n",
            "23428/23428 [==============================] - 0s 4us/step - loss: 0.5186 - binary_accuracy: 0.7650 - val_loss: 0.4726 - val_binary_accuracy: 0.7896\n",
            "Epoch 31/60\n",
            "23428/23428 [==============================] - 0s 5us/step - loss: 0.5170 - binary_accuracy: 0.7655 - val_loss: 0.4707 - val_binary_accuracy: 0.7876\n",
            "Epoch 32/60\n",
            "23428/23428 [==============================] - 0s 5us/step - loss: 0.5116 - binary_accuracy: 0.7652 - val_loss: 0.4685 - val_binary_accuracy: 0.7901\n",
            "Epoch 33/60\n",
            "23428/23428 [==============================] - 0s 5us/step - loss: 0.5087 - binary_accuracy: 0.7681 - val_loss: 0.4660 - val_binary_accuracy: 0.7906\n",
            "Epoch 34/60\n",
            "23428/23428 [==============================] - 0s 4us/step - loss: 0.5076 - binary_accuracy: 0.7646 - val_loss: 0.4649 - val_binary_accuracy: 0.7909\n",
            "Epoch 35/60\n",
            "23428/23428 [==============================] - 0s 4us/step - loss: 0.5067 - binary_accuracy: 0.7671 - val_loss: 0.4643 - val_binary_accuracy: 0.7891\n",
            "Epoch 36/60\n",
            "23428/23428 [==============================] - 0s 5us/step - loss: 0.5070 - binary_accuracy: 0.7652 - val_loss: 0.4638 - val_binary_accuracy: 0.7876\n",
            "Epoch 37/60\n",
            "23428/23428 [==============================] - 0s 4us/step - loss: 0.5033 - binary_accuracy: 0.7675 - val_loss: 0.4615 - val_binary_accuracy: 0.7886\n",
            "Epoch 38/60\n",
            "23428/23428 [==============================] - 0s 4us/step - loss: 0.5037 - binary_accuracy: 0.7655 - val_loss: 0.4609 - val_binary_accuracy: 0.7903\n",
            "Epoch 39/60\n",
            "23428/23428 [==============================] - 0s 4us/step - loss: 0.5011 - binary_accuracy: 0.7664 - val_loss: 0.4593 - val_binary_accuracy: 0.7889\n",
            "Epoch 40/60\n",
            "23428/23428 [==============================] - 0s 4us/step - loss: 0.4999 - binary_accuracy: 0.7660 - val_loss: 0.4596 - val_binary_accuracy: 0.7895\n",
            "Epoch 41/60\n",
            "23428/23428 [==============================] - 0s 4us/step - loss: 0.4951 - binary_accuracy: 0.7690 - val_loss: 0.4582 - val_binary_accuracy: 0.7881\n",
            "Epoch 42/60\n",
            "23428/23428 [==============================] - 0s 4us/step - loss: 0.4987 - binary_accuracy: 0.7683 - val_loss: 0.4580 - val_binary_accuracy: 0.7883\n",
            "Epoch 43/60\n",
            "23428/23428 [==============================] - 0s 4us/step - loss: 0.4970 - binary_accuracy: 0.7690 - val_loss: 0.4576 - val_binary_accuracy: 0.7883\n",
            "Epoch 44/60\n",
            "23428/23428 [==============================] - 0s 5us/step - loss: 0.4970 - binary_accuracy: 0.7696 - val_loss: 0.4562 - val_binary_accuracy: 0.7890\n",
            "Epoch 45/60\n",
            "23428/23428 [==============================] - 0s 5us/step - loss: 0.4928 - binary_accuracy: 0.7681 - val_loss: 0.4552 - val_binary_accuracy: 0.7890\n",
            "Epoch 46/60\n",
            "23428/23428 [==============================] - 0s 5us/step - loss: 0.4937 - binary_accuracy: 0.7713 - val_loss: 0.4546 - val_binary_accuracy: 0.7890\n",
            "Epoch 47/60\n",
            "23428/23428 [==============================] - 0s 5us/step - loss: 0.4924 - binary_accuracy: 0.7714 - val_loss: 0.4541 - val_binary_accuracy: 0.7906\n",
            "Epoch 48/60\n",
            "23428/23428 [==============================] - 0s 5us/step - loss: 0.4936 - binary_accuracy: 0.7695 - val_loss: 0.4537 - val_binary_accuracy: 0.7909\n",
            "Epoch 49/60\n",
            "23428/23428 [==============================] - 0s 4us/step - loss: 0.4923 - binary_accuracy: 0.7674 - val_loss: 0.4527 - val_binary_accuracy: 0.7914\n",
            "Epoch 50/60\n",
            "23428/23428 [==============================] - 0s 4us/step - loss: 0.4918 - binary_accuracy: 0.7725 - val_loss: 0.4534 - val_binary_accuracy: 0.7916\n",
            "Epoch 51/60\n",
            "23428/23428 [==============================] - 0s 4us/step - loss: 0.4896 - binary_accuracy: 0.7685 - val_loss: 0.4514 - val_binary_accuracy: 0.7915\n",
            "Epoch 52/60\n",
            "23428/23428 [==============================] - 0s 4us/step - loss: 0.4897 - binary_accuracy: 0.7711 - val_loss: 0.4515 - val_binary_accuracy: 0.7916\n",
            "Epoch 53/60\n",
            "23428/23428 [==============================] - 0s 5us/step - loss: 0.4872 - binary_accuracy: 0.7708 - val_loss: 0.4512 - val_binary_accuracy: 0.7909\n",
            "Epoch 54/60\n",
            "23428/23428 [==============================] - 0s 4us/step - loss: 0.4896 - binary_accuracy: 0.7721 - val_loss: 0.4519 - val_binary_accuracy: 0.7905\n",
            "Epoch 55/60\n",
            "23428/23428 [==============================] - 0s 4us/step - loss: 0.4904 - binary_accuracy: 0.7713 - val_loss: 0.4525 - val_binary_accuracy: 0.7818\n",
            "Epoch 56/60\n",
            "23428/23428 [==============================] - 0s 4us/step - loss: 0.4900 - binary_accuracy: 0.7695 - val_loss: 0.4511 - val_binary_accuracy: 0.7912\n",
            "Epoch 57/60\n",
            "23428/23428 [==============================] - 0s 4us/step - loss: 0.4910 - binary_accuracy: 0.7691 - val_loss: 0.4500 - val_binary_accuracy: 0.7926\n",
            "Epoch 58/60\n",
            "23428/23428 [==============================] - 0s 4us/step - loss: 0.4877 - binary_accuracy: 0.7696 - val_loss: 0.4495 - val_binary_accuracy: 0.7899\n",
            "Epoch 59/60\n",
            "23428/23428 [==============================] - 0s 4us/step - loss: 0.4898 - binary_accuracy: 0.7690 - val_loss: 0.4493 - val_binary_accuracy: 0.7906\n",
            "Epoch 60/60\n",
            "23428/23428 [==============================] - 0s 4us/step - loss: 0.4867 - binary_accuracy: 0.7695 - val_loss: 0.4488 - val_binary_accuracy: 0.7906\n",
            "23428/23428 [==============================] - 1s 41us/step\n",
            "8000/8000 [==============================] - 0s 43us/step\n",
            "(23428, 31) (8000, 31) (13470, 31) (23428,) (8000,) (13470,)\n",
            "Train on 23428 samples, validate on 8000 samples\n",
            "Epoch 1/60\n",
            "23428/23428 [==============================] - 0s 6us/step - loss: 0.9302 - binary_accuracy: 0.5116 - val_loss: 0.9167 - val_binary_accuracy: 0.6035\n",
            "Epoch 2/60\n",
            "23428/23428 [==============================] - 0s 5us/step - loss: 0.9137 - binary_accuracy: 0.5243 - val_loss: 0.8975 - val_binary_accuracy: 0.5851\n",
            "Epoch 3/60\n",
            "23428/23428 [==============================] - 0s 4us/step - loss: 0.8951 - binary_accuracy: 0.5352 - val_loss: 0.8759 - val_binary_accuracy: 0.5910\n",
            "Epoch 4/60\n",
            "23428/23428 [==============================] - 0s 4us/step - loss: 0.8747 - binary_accuracy: 0.5689 - val_loss: 0.8515 - val_binary_accuracy: 0.6935\n",
            "Epoch 5/60\n",
            "23428/23428 [==============================] - 0s 4us/step - loss: 0.8552 - binary_accuracy: 0.6119 - val_loss: 0.8242 - val_binary_accuracy: 0.7376\n",
            "Epoch 6/60\n",
            "23428/23428 [==============================] - 0s 4us/step - loss: 0.8316 - binary_accuracy: 0.6352 - val_loss: 0.7949 - val_binary_accuracy: 0.7514\n",
            "Epoch 7/60\n",
            "23428/23428 [==============================] - 0s 4us/step - loss: 0.8083 - binary_accuracy: 0.6540 - val_loss: 0.7644 - val_binary_accuracy: 0.7655\n",
            "Epoch 8/60\n",
            "23428/23428 [==============================] - 0s 4us/step - loss: 0.7828 - binary_accuracy: 0.6668 - val_loss: 0.7320 - val_binary_accuracy: 0.7688\n",
            "Epoch 9/60\n",
            "23428/23428 [==============================] - 0s 4us/step - loss: 0.7610 - binary_accuracy: 0.6764 - val_loss: 0.7040 - val_binary_accuracy: 0.7781\n",
            "Epoch 10/60\n",
            "23428/23428 [==============================] - 0s 4us/step - loss: 0.7385 - binary_accuracy: 0.6895 - val_loss: 0.6741 - val_binary_accuracy: 0.7795\n",
            "Epoch 11/60\n",
            "23428/23428 [==============================] - 0s 4us/step - loss: 0.7148 - binary_accuracy: 0.7032 - val_loss: 0.6480 - val_binary_accuracy: 0.7793\n",
            "Epoch 12/60\n",
            "23428/23428 [==============================] - 0s 4us/step - loss: 0.6911 - binary_accuracy: 0.7138 - val_loss: 0.6235 - val_binary_accuracy: 0.7809\n",
            "Epoch 13/60\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "23428/23428 [==============================] - 0s 4us/step - loss: 0.6727 - binary_accuracy: 0.7206 - val_loss: 0.6039 - val_binary_accuracy: 0.7815\n",
            "Epoch 14/60\n",
            "23428/23428 [==============================] - 0s 4us/step - loss: 0.6539 - binary_accuracy: 0.7289 - val_loss: 0.5868 - val_binary_accuracy: 0.7804\n",
            "Epoch 15/60\n",
            "23428/23428 [==============================] - 0s 5us/step - loss: 0.6424 - binary_accuracy: 0.7327 - val_loss: 0.5736 - val_binary_accuracy: 0.7797\n",
            "Epoch 16/60\n",
            "23428/23428 [==============================] - 0s 5us/step - loss: 0.6314 - binary_accuracy: 0.7394 - val_loss: 0.5600 - val_binary_accuracy: 0.7831\n",
            "Epoch 17/60\n",
            "23428/23428 [==============================] - 0s 5us/step - loss: 0.6149 - binary_accuracy: 0.7432 - val_loss: 0.5487 - val_binary_accuracy: 0.7859\n",
            "Epoch 18/60\n",
            "23428/23428 [==============================] - 0s 4us/step - loss: 0.6035 - binary_accuracy: 0.7474 - val_loss: 0.5396 - val_binary_accuracy: 0.7854\n",
            "Epoch 19/60\n",
            "23428/23428 [==============================] - 0s 4us/step - loss: 0.5921 - binary_accuracy: 0.7495 - val_loss: 0.5294 - val_binary_accuracy: 0.7870\n",
            "Epoch 20/60\n",
            "23428/23428 [==============================] - 0s 5us/step - loss: 0.5793 - binary_accuracy: 0.7533 - val_loss: 0.5209 - val_binary_accuracy: 0.7883\n",
            "Epoch 21/60\n",
            "23428/23428 [==============================] - 0s 5us/step - loss: 0.5752 - binary_accuracy: 0.7541 - val_loss: 0.5145 - val_binary_accuracy: 0.7876\n",
            "Epoch 22/60\n",
            "23428/23428 [==============================] - 0s 5us/step - loss: 0.5655 - binary_accuracy: 0.7553 - val_loss: 0.5082 - val_binary_accuracy: 0.7879\n",
            "Epoch 23/60\n",
            "23428/23428 [==============================] - 0s 4us/step - loss: 0.5565 - binary_accuracy: 0.7595 - val_loss: 0.5010 - val_binary_accuracy: 0.7880\n",
            "Epoch 24/60\n",
            "23428/23428 [==============================] - 0s 4us/step - loss: 0.5541 - binary_accuracy: 0.7578 - val_loss: 0.4965 - val_binary_accuracy: 0.7881\n",
            "Epoch 25/60\n",
            "23428/23428 [==============================] - 0s 4us/step - loss: 0.5447 - binary_accuracy: 0.7590 - val_loss: 0.4920 - val_binary_accuracy: 0.7872\n",
            "Epoch 26/60\n",
            "23428/23428 [==============================] - 0s 4us/step - loss: 0.5381 - binary_accuracy: 0.7624 - val_loss: 0.4865 - val_binary_accuracy: 0.7893\n",
            "Epoch 27/60\n",
            "23428/23428 [==============================] - 0s 4us/step - loss: 0.5316 - binary_accuracy: 0.7627 - val_loss: 0.4830 - val_binary_accuracy: 0.7897\n",
            "Epoch 28/60\n",
            "23428/23428 [==============================] - 0s 4us/step - loss: 0.5295 - binary_accuracy: 0.7614 - val_loss: 0.4799 - val_binary_accuracy: 0.7891\n",
            "Epoch 29/60\n",
            "23428/23428 [==============================] - 0s 4us/step - loss: 0.5237 - binary_accuracy: 0.7642 - val_loss: 0.4753 - val_binary_accuracy: 0.7890\n",
            "Epoch 30/60\n",
            "23428/23428 [==============================] - 0s 4us/step - loss: 0.5227 - binary_accuracy: 0.7639 - val_loss: 0.4735 - val_binary_accuracy: 0.7906\n",
            "Epoch 31/60\n",
            "23428/23428 [==============================] - 0s 4us/step - loss: 0.5142 - binary_accuracy: 0.7653 - val_loss: 0.4698 - val_binary_accuracy: 0.7891\n",
            "Epoch 32/60\n",
            "23428/23428 [==============================] - 0s 4us/step - loss: 0.5104 - binary_accuracy: 0.7648 - val_loss: 0.4683 - val_binary_accuracy: 0.7886\n",
            "Epoch 33/60\n",
            "23428/23428 [==============================] - 0s 4us/step - loss: 0.5097 - binary_accuracy: 0.7643 - val_loss: 0.4663 - val_binary_accuracy: 0.7896\n",
            "Epoch 34/60\n",
            "23428/23428 [==============================] - 0s 4us/step - loss: 0.5100 - binary_accuracy: 0.7643 - val_loss: 0.4647 - val_binary_accuracy: 0.7880\n",
            "Epoch 35/60\n",
            "23428/23428 [==============================] - 0s 4us/step - loss: 0.5014 - binary_accuracy: 0.7697 - val_loss: 0.4636 - val_binary_accuracy: 0.7893\n",
            "Epoch 36/60\n",
            "23428/23428 [==============================] - 0s 4us/step - loss: 0.5049 - binary_accuracy: 0.7666 - val_loss: 0.4626 - val_binary_accuracy: 0.7895\n",
            "Epoch 37/60\n",
            "23428/23428 [==============================] - 0s 4us/step - loss: 0.5029 - binary_accuracy: 0.7666 - val_loss: 0.4617 - val_binary_accuracy: 0.7887\n",
            "Epoch 38/60\n",
            "23428/23428 [==============================] - 0s 4us/step - loss: 0.5028 - binary_accuracy: 0.7652 - val_loss: 0.4605 - val_binary_accuracy: 0.7899\n",
            "Epoch 39/60\n",
            "23428/23428 [==============================] - 0s 4us/step - loss: 0.5007 - binary_accuracy: 0.7670 - val_loss: 0.4601 - val_binary_accuracy: 0.7881\n",
            "Epoch 40/60\n",
            "23428/23428 [==============================] - 0s 4us/step - loss: 0.4994 - binary_accuracy: 0.7692 - val_loss: 0.4584 - val_binary_accuracy: 0.7900\n",
            "Epoch 41/60\n",
            "23428/23428 [==============================] - 0s 4us/step - loss: 0.4997 - binary_accuracy: 0.7675 - val_loss: 0.4575 - val_binary_accuracy: 0.7891\n",
            "Epoch 42/60\n",
            "23428/23428 [==============================] - 0s 5us/step - loss: 0.5000 - binary_accuracy: 0.7683 - val_loss: 0.4577 - val_binary_accuracy: 0.7890\n",
            "Epoch 43/60\n",
            "23428/23428 [==============================] - 0s 5us/step - loss: 0.4966 - binary_accuracy: 0.7684 - val_loss: 0.4568 - val_binary_accuracy: 0.7891\n",
            "Epoch 44/60\n",
            "23428/23428 [==============================] - 0s 5us/step - loss: 0.4974 - binary_accuracy: 0.7673 - val_loss: 0.4559 - val_binary_accuracy: 0.7904\n",
            "Epoch 45/60\n",
            "23428/23428 [==============================] - 0s 4us/step - loss: 0.4980 - binary_accuracy: 0.7680 - val_loss: 0.4560 - val_binary_accuracy: 0.7881\n",
            "Epoch 46/60\n",
            "23428/23428 [==============================] - 0s 4us/step - loss: 0.4960 - binary_accuracy: 0.7660 - val_loss: 0.4540 - val_binary_accuracy: 0.7895\n",
            "Epoch 47/60\n",
            "23428/23428 [==============================] - 0s 4us/step - loss: 0.4945 - binary_accuracy: 0.7697 - val_loss: 0.4538 - val_binary_accuracy: 0.7896\n",
            "Epoch 48/60\n",
            "23428/23428 [==============================] - 0s 5us/step - loss: 0.4897 - binary_accuracy: 0.7688 - val_loss: 0.4542 - val_binary_accuracy: 0.7901\n",
            "Epoch 49/60\n",
            "23428/23428 [==============================] - 0s 4us/step - loss: 0.4944 - binary_accuracy: 0.7678 - val_loss: 0.4543 - val_binary_accuracy: 0.7911\n",
            "Epoch 50/60\n",
            "23428/23428 [==============================] - 0s 5us/step - loss: 0.4900 - binary_accuracy: 0.7692 - val_loss: 0.4531 - val_binary_accuracy: 0.7924\n",
            "Epoch 51/60\n",
            "23428/23428 [==============================] - 0s 5us/step - loss: 0.4929 - binary_accuracy: 0.7692 - val_loss: 0.4529 - val_binary_accuracy: 0.7912\n",
            "Epoch 52/60\n",
            "23428/23428 [==============================] - 0s 5us/step - loss: 0.4906 - binary_accuracy: 0.7685 - val_loss: 0.4518 - val_binary_accuracy: 0.7890\n",
            "Epoch 53/60\n",
            "23428/23428 [==============================] - 0s 5us/step - loss: 0.4914 - binary_accuracy: 0.7705 - val_loss: 0.4515 - val_binary_accuracy: 0.7901\n",
            "Epoch 54/60\n",
            "23428/23428 [==============================] - 0s 4us/step - loss: 0.4910 - binary_accuracy: 0.7675 - val_loss: 0.4523 - val_binary_accuracy: 0.7881\n",
            "Epoch 55/60\n",
            "23428/23428 [==============================] - 0s 5us/step - loss: 0.4890 - binary_accuracy: 0.7674 - val_loss: 0.4513 - val_binary_accuracy: 0.7919\n",
            "Epoch 56/60\n",
            "23428/23428 [==============================] - 0s 4us/step - loss: 0.4885 - binary_accuracy: 0.7699 - val_loss: 0.4513 - val_binary_accuracy: 0.7893\n",
            "Epoch 57/60\n",
            "23428/23428 [==============================] - 0s 4us/step - loss: 0.4896 - binary_accuracy: 0.7666 - val_loss: 0.4516 - val_binary_accuracy: 0.7890\n",
            "Epoch 58/60\n",
            "23428/23428 [==============================] - 0s 5us/step - loss: 0.4890 - binary_accuracy: 0.7673 - val_loss: 0.4502 - val_binary_accuracy: 0.7906\n",
            "Epoch 59/60\n",
            "23428/23428 [==============================] - 0s 5us/step - loss: 0.4867 - binary_accuracy: 0.7701 - val_loss: 0.4498 - val_binary_accuracy: 0.7908\n",
            "Epoch 60/60\n",
            "23428/23428 [==============================] - 0s 4us/step - loss: 0.4864 - binary_accuracy: 0.7697 - val_loss: 0.4498 - val_binary_accuracy: 0.7909\n",
            "23428/23428 [==============================] - 1s 55us/step\n",
            "8000/8000 [==============================] - 0s 47us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8yVk3IQuYhbV",
        "colab_type": "text"
      },
      "source": [
        "Generamos un dataframe con las métricas por fold para luego seleccionar los mejores resultados:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sS-npAAXYhbY",
        "colab_type": "code",
        "colab": {},
        "outputId": "5f075ff1-5778-47b8-96f4-f316e7eb1370"
      },
      "source": [
        "import pandas as pd\n",
        "df = pd.DataFrame(global_history)\n",
        "df.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>fold</th>\n",
              "      <th>layers</th>\n",
              "      <th>optimizer</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>train_acc</th>\n",
              "      <th>val_loss</th>\n",
              "      <th>val_acc</th>\n",
              "      <th>history</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>[512, 10, 1]</td>\n",
              "      <td>&lt;keras.optimizers.SGD object at 0x00000180021B...</td>\n",
              "      <td>0.743590</td>\n",
              "      <td>0.777958</td>\n",
              "      <td>0.736991</td>\n",
              "      <td>0.788125</td>\n",
              "      <td>&lt;keras.callbacks.callbacks.History object at 0...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>[512, 10, 1]</td>\n",
              "      <td>&lt;keras.optimizers.SGD object at 0x00000180021B...</td>\n",
              "      <td>0.736232</td>\n",
              "      <td>0.776976</td>\n",
              "      <td>0.729643</td>\n",
              "      <td>0.786625</td>\n",
              "      <td>&lt;keras.callbacks.callbacks.History object at 0...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>[512, 10, 1]</td>\n",
              "      <td>&lt;keras.optimizers.SGD object at 0x00000180021B...</td>\n",
              "      <td>0.736522</td>\n",
              "      <td>0.778043</td>\n",
              "      <td>0.729899</td>\n",
              "      <td>0.788375</td>\n",
              "      <td>&lt;keras.callbacks.callbacks.History object at 0...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>[512, 10, 1]</td>\n",
              "      <td>&lt;keras.optimizers.Adam object at 0x00000180021...</td>\n",
              "      <td>0.471405</td>\n",
              "      <td>0.779793</td>\n",
              "      <td>0.463725</td>\n",
              "      <td>0.790375</td>\n",
              "      <td>&lt;keras.callbacks.callbacks.History object at 0...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>[512, 10, 1]</td>\n",
              "      <td>&lt;keras.optimizers.Adam object at 0x00000180021...</td>\n",
              "      <td>0.465682</td>\n",
              "      <td>0.779537</td>\n",
              "      <td>0.457773</td>\n",
              "      <td>0.789500</td>\n",
              "      <td>&lt;keras.callbacks.callbacks.History object at 0...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   fold        layers                                          optimizer  \\\n",
              "0     0  [512, 10, 1]  <keras.optimizers.SGD object at 0x00000180021B...   \n",
              "1     1  [512, 10, 1]  <keras.optimizers.SGD object at 0x00000180021B...   \n",
              "2     2  [512, 10, 1]  <keras.optimizers.SGD object at 0x00000180021B...   \n",
              "3     0  [512, 10, 1]  <keras.optimizers.Adam object at 0x00000180021...   \n",
              "4     1  [512, 10, 1]  <keras.optimizers.Adam object at 0x00000180021...   \n",
              "\n",
              "   train_loss  train_acc  val_loss   val_acc  \\\n",
              "0    0.743590   0.777958  0.736991  0.788125   \n",
              "1    0.736232   0.776976  0.729643  0.786625   \n",
              "2    0.736522   0.778043  0.729899  0.788375   \n",
              "3    0.471405   0.779793  0.463725  0.790375   \n",
              "4    0.465682   0.779537  0.457773  0.789500   \n",
              "\n",
              "                                             history  \n",
              "0  <keras.callbacks.callbacks.History object at 0...  \n",
              "1  <keras.callbacks.callbacks.History object at 0...  \n",
              "2  <keras.callbacks.callbacks.History object at 0...  \n",
              "3  <keras.callbacks.callbacks.History object at 0...  \n",
              "4  <keras.callbacks.callbacks.History object at 0...  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i67XVdjyYhbd",
        "colab_type": "code",
        "colab": {},
        "outputId": "900fb16e-f3ab-46c5-d2c3-1d9df4c90a4b"
      },
      "source": [
        "df.head(19)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>fold</th>\n",
              "      <th>layers</th>\n",
              "      <th>optimizer</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>train_acc</th>\n",
              "      <th>val_loss</th>\n",
              "      <th>val_acc</th>\n",
              "      <th>history</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>[512, 10, 1]</td>\n",
              "      <td>&lt;keras.optimizers.SGD object at 0x00000180021B...</td>\n",
              "      <td>0.743590</td>\n",
              "      <td>0.777958</td>\n",
              "      <td>0.736991</td>\n",
              "      <td>0.788125</td>\n",
              "      <td>&lt;keras.callbacks.callbacks.History object at 0...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>[512, 10, 1]</td>\n",
              "      <td>&lt;keras.optimizers.SGD object at 0x00000180021B...</td>\n",
              "      <td>0.736232</td>\n",
              "      <td>0.776976</td>\n",
              "      <td>0.729643</td>\n",
              "      <td>0.786625</td>\n",
              "      <td>&lt;keras.callbacks.callbacks.History object at 0...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>[512, 10, 1]</td>\n",
              "      <td>&lt;keras.optimizers.SGD object at 0x00000180021B...</td>\n",
              "      <td>0.736522</td>\n",
              "      <td>0.778043</td>\n",
              "      <td>0.729899</td>\n",
              "      <td>0.788375</td>\n",
              "      <td>&lt;keras.callbacks.callbacks.History object at 0...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>[512, 10, 1]</td>\n",
              "      <td>&lt;keras.optimizers.Adam object at 0x00000180021...</td>\n",
              "      <td>0.471405</td>\n",
              "      <td>0.779793</td>\n",
              "      <td>0.463725</td>\n",
              "      <td>0.790375</td>\n",
              "      <td>&lt;keras.callbacks.callbacks.History object at 0...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>[512, 10, 1]</td>\n",
              "      <td>&lt;keras.optimizers.Adam object at 0x00000180021...</td>\n",
              "      <td>0.465682</td>\n",
              "      <td>0.779537</td>\n",
              "      <td>0.457773</td>\n",
              "      <td>0.789500</td>\n",
              "      <td>&lt;keras.callbacks.callbacks.History object at 0...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>[512, 10, 1]</td>\n",
              "      <td>&lt;keras.optimizers.Adam object at 0x00000180021...</td>\n",
              "      <td>0.468963</td>\n",
              "      <td>0.778982</td>\n",
              "      <td>0.461303</td>\n",
              "      <td>0.788250</td>\n",
              "      <td>&lt;keras.callbacks.callbacks.History object at 0...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>[512, 10, 1]</td>\n",
              "      <td>&lt;keras.optimizers.RMSprop object at 0x00000180...</td>\n",
              "      <td>0.476183</td>\n",
              "      <td>0.778641</td>\n",
              "      <td>0.468621</td>\n",
              "      <td>0.787500</td>\n",
              "      <td>&lt;keras.callbacks.callbacks.History object at 0...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>[512, 10, 1]</td>\n",
              "      <td>&lt;keras.optimizers.RMSprop object at 0x00000180...</td>\n",
              "      <td>0.477406</td>\n",
              "      <td>0.778812</td>\n",
              "      <td>0.469716</td>\n",
              "      <td>0.787250</td>\n",
              "      <td>&lt;keras.callbacks.callbacks.History object at 0...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>2</td>\n",
              "      <td>[512, 10, 1]</td>\n",
              "      <td>&lt;keras.optimizers.RMSprop object at 0x00000180...</td>\n",
              "      <td>0.476675</td>\n",
              "      <td>0.778684</td>\n",
              "      <td>0.468830</td>\n",
              "      <td>0.790125</td>\n",
              "      <td>&lt;keras.callbacks.callbacks.History object at 0...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>0</td>\n",
              "      <td>[64, 32, 10, 1]</td>\n",
              "      <td>&lt;keras.optimizers.SGD object at 0x00000180021B...</td>\n",
              "      <td>0.669320</td>\n",
              "      <td>0.771086</td>\n",
              "      <td>0.664577</td>\n",
              "      <td>0.779750</td>\n",
              "      <td>&lt;keras.callbacks.callbacks.History object at 0...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>1</td>\n",
              "      <td>[64, 32, 10, 1]</td>\n",
              "      <td>&lt;keras.optimizers.SGD object at 0x00000180021B...</td>\n",
              "      <td>0.659891</td>\n",
              "      <td>0.771897</td>\n",
              "      <td>0.654663</td>\n",
              "      <td>0.781500</td>\n",
              "      <td>&lt;keras.callbacks.callbacks.History object at 0...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11</td>\n",
              "      <td>2</td>\n",
              "      <td>[64, 32, 10, 1]</td>\n",
              "      <td>&lt;keras.optimizers.SGD object at 0x00000180021B...</td>\n",
              "      <td>0.660171</td>\n",
              "      <td>0.772025</td>\n",
              "      <td>0.654867</td>\n",
              "      <td>0.781625</td>\n",
              "      <td>&lt;keras.callbacks.callbacks.History object at 0...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12</td>\n",
              "      <td>0</td>\n",
              "      <td>[64, 32, 10, 1]</td>\n",
              "      <td>&lt;keras.optimizers.Adam object at 0x00000180021...</td>\n",
              "      <td>0.448964</td>\n",
              "      <td>0.781501</td>\n",
              "      <td>0.440215</td>\n",
              "      <td>0.791750</td>\n",
              "      <td>&lt;keras.callbacks.callbacks.History object at 0...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13</td>\n",
              "      <td>1</td>\n",
              "      <td>[64, 32, 10, 1]</td>\n",
              "      <td>&lt;keras.optimizers.Adam object at 0x00000180021...</td>\n",
              "      <td>0.468873</td>\n",
              "      <td>0.782098</td>\n",
              "      <td>0.461023</td>\n",
              "      <td>0.792000</td>\n",
              "      <td>&lt;keras.callbacks.callbacks.History object at 0...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14</td>\n",
              "      <td>2</td>\n",
              "      <td>[64, 32, 10, 1]</td>\n",
              "      <td>&lt;keras.optimizers.Adam object at 0x00000180021...</td>\n",
              "      <td>0.466353</td>\n",
              "      <td>0.780989</td>\n",
              "      <td>0.457691</td>\n",
              "      <td>0.791500</td>\n",
              "      <td>&lt;keras.callbacks.callbacks.History object at 0...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15</td>\n",
              "      <td>0</td>\n",
              "      <td>[64, 32, 10, 1]</td>\n",
              "      <td>&lt;keras.optimizers.RMSprop object at 0x00000180...</td>\n",
              "      <td>0.456683</td>\n",
              "      <td>0.781159</td>\n",
              "      <td>0.447912</td>\n",
              "      <td>0.790375</td>\n",
              "      <td>&lt;keras.callbacks.callbacks.History object at 0...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16</td>\n",
              "      <td>1</td>\n",
              "      <td>[64, 32, 10, 1]</td>\n",
              "      <td>&lt;keras.optimizers.RMSprop object at 0x00000180...</td>\n",
              "      <td>0.457316</td>\n",
              "      <td>0.781074</td>\n",
              "      <td>0.448773</td>\n",
              "      <td>0.790625</td>\n",
              "      <td>&lt;keras.callbacks.callbacks.History object at 0...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>17</td>\n",
              "      <td>2</td>\n",
              "      <td>[64, 32, 10, 1]</td>\n",
              "      <td>&lt;keras.optimizers.RMSprop object at 0x00000180...</td>\n",
              "      <td>0.457859</td>\n",
              "      <td>0.782226</td>\n",
              "      <td>0.449773</td>\n",
              "      <td>0.790875</td>\n",
              "      <td>&lt;keras.callbacks.callbacks.History object at 0...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    fold           layers                                          optimizer  \\\n",
              "0      0     [512, 10, 1]  <keras.optimizers.SGD object at 0x00000180021B...   \n",
              "1      1     [512, 10, 1]  <keras.optimizers.SGD object at 0x00000180021B...   \n",
              "2      2     [512, 10, 1]  <keras.optimizers.SGD object at 0x00000180021B...   \n",
              "3      0     [512, 10, 1]  <keras.optimizers.Adam object at 0x00000180021...   \n",
              "4      1     [512, 10, 1]  <keras.optimizers.Adam object at 0x00000180021...   \n",
              "5      2     [512, 10, 1]  <keras.optimizers.Adam object at 0x00000180021...   \n",
              "6      0     [512, 10, 1]  <keras.optimizers.RMSprop object at 0x00000180...   \n",
              "7      1     [512, 10, 1]  <keras.optimizers.RMSprop object at 0x00000180...   \n",
              "8      2     [512, 10, 1]  <keras.optimizers.RMSprop object at 0x00000180...   \n",
              "9      0  [64, 32, 10, 1]  <keras.optimizers.SGD object at 0x00000180021B...   \n",
              "10     1  [64, 32, 10, 1]  <keras.optimizers.SGD object at 0x00000180021B...   \n",
              "11     2  [64, 32, 10, 1]  <keras.optimizers.SGD object at 0x00000180021B...   \n",
              "12     0  [64, 32, 10, 1]  <keras.optimizers.Adam object at 0x00000180021...   \n",
              "13     1  [64, 32, 10, 1]  <keras.optimizers.Adam object at 0x00000180021...   \n",
              "14     2  [64, 32, 10, 1]  <keras.optimizers.Adam object at 0x00000180021...   \n",
              "15     0  [64, 32, 10, 1]  <keras.optimizers.RMSprop object at 0x00000180...   \n",
              "16     1  [64, 32, 10, 1]  <keras.optimizers.RMSprop object at 0x00000180...   \n",
              "17     2  [64, 32, 10, 1]  <keras.optimizers.RMSprop object at 0x00000180...   \n",
              "\n",
              "    train_loss  train_acc  val_loss   val_acc  \\\n",
              "0     0.743590   0.777958  0.736991  0.788125   \n",
              "1     0.736232   0.776976  0.729643  0.786625   \n",
              "2     0.736522   0.778043  0.729899  0.788375   \n",
              "3     0.471405   0.779793  0.463725  0.790375   \n",
              "4     0.465682   0.779537  0.457773  0.789500   \n",
              "5     0.468963   0.778982  0.461303  0.788250   \n",
              "6     0.476183   0.778641  0.468621  0.787500   \n",
              "7     0.477406   0.778812  0.469716  0.787250   \n",
              "8     0.476675   0.778684  0.468830  0.790125   \n",
              "9     0.669320   0.771086  0.664577  0.779750   \n",
              "10    0.659891   0.771897  0.654663  0.781500   \n",
              "11    0.660171   0.772025  0.654867  0.781625   \n",
              "12    0.448964   0.781501  0.440215  0.791750   \n",
              "13    0.468873   0.782098  0.461023  0.792000   \n",
              "14    0.466353   0.780989  0.457691  0.791500   \n",
              "15    0.456683   0.781159  0.447912  0.790375   \n",
              "16    0.457316   0.781074  0.448773  0.790625   \n",
              "17    0.457859   0.782226  0.449773  0.790875   \n",
              "\n",
              "                                              history  \n",
              "0   <keras.callbacks.callbacks.History object at 0...  \n",
              "1   <keras.callbacks.callbacks.History object at 0...  \n",
              "2   <keras.callbacks.callbacks.History object at 0...  \n",
              "3   <keras.callbacks.callbacks.History object at 0...  \n",
              "4   <keras.callbacks.callbacks.History object at 0...  \n",
              "5   <keras.callbacks.callbacks.History object at 0...  \n",
              "6   <keras.callbacks.callbacks.History object at 0...  \n",
              "7   <keras.callbacks.callbacks.History object at 0...  \n",
              "8   <keras.callbacks.callbacks.History object at 0...  \n",
              "9   <keras.callbacks.callbacks.History object at 0...  \n",
              "10  <keras.callbacks.callbacks.History object at 0...  \n",
              "11  <keras.callbacks.callbacks.History object at 0...  \n",
              "12  <keras.callbacks.callbacks.History object at 0...  \n",
              "13  <keras.callbacks.callbacks.History object at 0...  \n",
              "14  <keras.callbacks.callbacks.History object at 0...  \n",
              "15  <keras.callbacks.callbacks.History object at 0...  \n",
              "16  <keras.callbacks.callbacks.History object at 0...  \n",
              "17  <keras.callbacks.callbacks.History object at 0...  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cK56gjxTYhbi",
        "colab_type": "text"
      },
      "source": [
        "Verificamos que el maximo accuracy en validacion se obtuvo en el indice 13, con optimizador Adam, y estructura de layers [64, 32, 10, 1]."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fghn79_6Yhbi",
        "colab_type": "code",
        "colab": {},
        "outputId": "eed1b105-655e-479a-fb16-f1ef569c29c7"
      },
      "source": [
        "best_iteration = df.iloc[np.argmax(df['val_acc'])]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "C:\\Users\\maxir\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:56: FutureWarning: \n",
            "The current behaviour of 'Series.argmax' is deprecated, use 'idxmax'\n",
            "instead.\n",
            "The behavior of 'argmax' will be corrected to return the positional\n",
            "maximum in the future. For now, use 'series.values.argmax' or\n",
            "'np.argmax(np.array(values))' to get the position of the maximum\n",
            "row.\n",
            "  return getattr(obj, method)(*args, **kwds)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6j-G50DRYhbx",
        "colab_type": "code",
        "colab": {},
        "outputId": "d94c44e6-beba-4c7b-ad8b-d49c71458c95"
      },
      "source": [
        "best_iteration"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "fold                                                          1\n",
              "layers                                          [64, 32, 10, 1]\n",
              "optimizer     <keras.optimizers.Adam object at 0x00000180021...\n",
              "train_loss                                             0.468873\n",
              "train_acc                                              0.782098\n",
              "val_loss                                               0.461023\n",
              "val_acc                                                   0.792\n",
              "history       <keras.callbacks.callbacks.History object at 0...\n",
              "Name: 13, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5HyN0ImBYhb3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "layers = best_iteration[1]\n",
        "optimizer = best_iteration[2]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ra0O4oHwYhcA",
        "colab_type": "text"
      },
      "source": [
        "Construimos nuevamente el modelo a fines de plotear los scores en cada epoch:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c_8proxBYhcB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_model(input_shape, layers=layers, optimizer=optimizer):\n",
        "    # Instanciamos la clase del modelo secuencial\n",
        "    model = Sequential(name='Modelo de base')\n",
        "    \n",
        "    model.add(Dense(layers[0], activation='relu', input_shape=(X_train_partial_vec.shape[1],), kernel_regularizer=regularizers.l1(0.001)))\n",
        "    model.add(Dropout(0.5))\n",
        "\n",
        "    for l in layers[1:-1]:\n",
        "        model.add(Dense(units=l, activation='relu'))\n",
        "        model.add(Dropout(0.5))\n",
        "\n",
        "    # Agregamos la última capa \n",
        "    model.add(Dense(units=layers[-1], activation='sigmoid'))\n",
        "\n",
        "    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['binary_accuracy'])\n",
        "    # Retornamos el modelo compilado\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UO0WW6I_YhcG",
        "colab_type": "text"
      },
      "source": [
        "En base a los parametros seleccionados, la arquitectura del modelo es la siguiente:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1PT1x_c9YhcH",
        "colab_type": "code",
        "colab": {},
        "outputId": "5c15ad34-c865-4b8d-bd97-e3385d259a7c"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"Modelo de base\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_18 (Dense)             (None, 64)                2048      \n",
            "_________________________________________________________________\n",
            "dropout_13 (Dropout)         (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_19 (Dense)             (None, 32)                2080      \n",
            "_________________________________________________________________\n",
            "dropout_14 (Dropout)         (None, 32)                0         \n",
            "_________________________________________________________________\n",
            "dense_20 (Dense)             (None, 10)                330       \n",
            "_________________________________________________________________\n",
            "dropout_15 (Dropout)         (None, 10)                0         \n",
            "_________________________________________________________________\n",
            "dense_21 (Dense)             (None, 1)                 11        \n",
            "=================================================================\n",
            "Total params: 4,469\n",
            "Trainable params: 4,469\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QpxjfFNNYhcM",
        "colab_type": "code",
        "colab": {},
        "outputId": "f495d9a7-2b82-434b-fcee-0b771369b032"
      },
      "source": [
        "model.load_weights('initial_weights.h5')\n",
        "history=model.fit(x=X_train_partial_vec,\n",
        "                  y=y_train_partial,\n",
        "                  batch_size=batch_size,\n",
        "                  epochs=epochs, \n",
        "                  validation_split=0.3,\n",
        "                  verbose=1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 16399 samples, validate on 7029 samples\n",
            "Epoch 1/60\n",
            "16399/16399 [==============================] - 0s 7us/step - loss: 0.9330 - binary_accuracy: 0.5116 - val_loss: 0.9211 - val_binary_accuracy: 0.6137\n",
            "Epoch 2/60\n",
            "16399/16399 [==============================] - 0s 6us/step - loss: 0.9197 - binary_accuracy: 0.5175 - val_loss: 0.9060 - val_binary_accuracy: 0.6090\n",
            "Epoch 3/60\n",
            "16399/16399 [==============================] - 0s 5us/step - loss: 0.9079 - binary_accuracy: 0.5285 - val_loss: 0.8908 - val_binary_accuracy: 0.6031\n",
            "Epoch 4/60\n",
            "16399/16399 [==============================] - 0s 5us/step - loss: 0.8934 - binary_accuracy: 0.5558 - val_loss: 0.8740 - val_binary_accuracy: 0.6907\n",
            "Epoch 5/60\n",
            "16399/16399 [==============================] - 0s 4us/step - loss: 0.8782 - binary_accuracy: 0.5894 - val_loss: 0.8554 - val_binary_accuracy: 0.7038\n",
            "Epoch 6/60\n",
            "16399/16399 [==============================] - 0s 5us/step - loss: 0.8624 - binary_accuracy: 0.6110 - val_loss: 0.8355 - val_binary_accuracy: 0.7273\n",
            "Epoch 7/60\n",
            "16399/16399 [==============================] - 0s 5us/step - loss: 0.8442 - binary_accuracy: 0.6299 - val_loss: 0.8146 - val_binary_accuracy: 0.7398\n",
            "Epoch 8/60\n",
            "16399/16399 [==============================] - 0s 5us/step - loss: 0.8297 - binary_accuracy: 0.6441 - val_loss: 0.7933 - val_binary_accuracy: 0.7431\n",
            "Epoch 9/60\n",
            "16399/16399 [==============================] - 0s 5us/step - loss: 0.8094 - binary_accuracy: 0.6552 - val_loss: 0.7699 - val_binary_accuracy: 0.7492\n",
            "Epoch 10/60\n",
            "16399/16399 [==============================] - 0s 5us/step - loss: 0.7954 - binary_accuracy: 0.6568 - val_loss: 0.7485 - val_binary_accuracy: 0.7574\n",
            "Epoch 11/60\n",
            "16399/16399 [==============================] - 0s 5us/step - loss: 0.7743 - binary_accuracy: 0.6727 - val_loss: 0.7262 - val_binary_accuracy: 0.7681\n",
            "Epoch 12/60\n",
            "16399/16399 [==============================] - 0s 4us/step - loss: 0.7611 - binary_accuracy: 0.6803 - val_loss: 0.7048 - val_binary_accuracy: 0.7672\n",
            "Epoch 13/60\n",
            "16399/16399 [==============================] - 0s 4us/step - loss: 0.7400 - binary_accuracy: 0.6890 - val_loss: 0.6842 - val_binary_accuracy: 0.7678\n",
            "Epoch 14/60\n",
            "16399/16399 [==============================] - 0s 4us/step - loss: 0.7205 - binary_accuracy: 0.6983 - val_loss: 0.6644 - val_binary_accuracy: 0.7678\n",
            "Epoch 15/60\n",
            "16399/16399 [==============================] - 0s 4us/step - loss: 0.7064 - binary_accuracy: 0.7080 - val_loss: 0.6474 - val_binary_accuracy: 0.7675\n",
            "Epoch 16/60\n",
            "16399/16399 [==============================] - 0s 5us/step - loss: 0.6904 - binary_accuracy: 0.7149 - val_loss: 0.6289 - val_binary_accuracy: 0.7685\n",
            "Epoch 17/60\n",
            "16399/16399 [==============================] - 0s 5us/step - loss: 0.6776 - binary_accuracy: 0.7219 - val_loss: 0.6169 - val_binary_accuracy: 0.7688\n",
            "Epoch 18/60\n",
            "16399/16399 [==============================] - 0s 4us/step - loss: 0.6642 - binary_accuracy: 0.7250 - val_loss: 0.6030 - val_binary_accuracy: 0.7702\n",
            "Epoch 19/60\n",
            "16399/16399 [==============================] - 0s 5us/step - loss: 0.6502 - binary_accuracy: 0.7327 - val_loss: 0.5916 - val_binary_accuracy: 0.7697\n",
            "Epoch 20/60\n",
            "16399/16399 [==============================] - 0s 4us/step - loss: 0.6428 - binary_accuracy: 0.7343 - val_loss: 0.5806 - val_binary_accuracy: 0.7702\n",
            "Epoch 21/60\n",
            "16399/16399 [==============================] - 0s 4us/step - loss: 0.6342 - binary_accuracy: 0.7405 - val_loss: 0.5711 - val_binary_accuracy: 0.7712\n",
            "Epoch 22/60\n",
            "16399/16399 [==============================] - 0s 4us/step - loss: 0.6262 - binary_accuracy: 0.7381 - val_loss: 0.5628 - val_binary_accuracy: 0.7705\n",
            "Epoch 23/60\n",
            "16399/16399 [==============================] - 0s 4us/step - loss: 0.6108 - binary_accuracy: 0.7439 - val_loss: 0.5530 - val_binary_accuracy: 0.7745\n",
            "Epoch 24/60\n",
            "16399/16399 [==============================] - 0s 4us/step - loss: 0.6025 - binary_accuracy: 0.7472 - val_loss: 0.5458 - val_binary_accuracy: 0.7745\n",
            "Epoch 25/60\n",
            "16399/16399 [==============================] - 0s 4us/step - loss: 0.5972 - binary_accuracy: 0.7506 - val_loss: 0.5398 - val_binary_accuracy: 0.7744\n",
            "Epoch 26/60\n",
            "16399/16399 [==============================] - 0s 5us/step - loss: 0.5925 - binary_accuracy: 0.7513 - val_loss: 0.5351 - val_binary_accuracy: 0.7725\n",
            "Epoch 27/60\n",
            "16399/16399 [==============================] - 0s 4us/step - loss: 0.5838 - binary_accuracy: 0.7565 - val_loss: 0.5284 - val_binary_accuracy: 0.7741\n",
            "Epoch 28/60\n",
            "16399/16399 [==============================] - 0s 4us/step - loss: 0.5745 - binary_accuracy: 0.7591 - val_loss: 0.5237 - val_binary_accuracy: 0.7742\n",
            "Epoch 29/60\n",
            "16399/16399 [==============================] - 0s 4us/step - loss: 0.5688 - binary_accuracy: 0.7555 - val_loss: 0.5185 - val_binary_accuracy: 0.7746\n",
            "Epoch 30/60\n",
            "16399/16399 [==============================] - 0s 4us/step - loss: 0.5637 - binary_accuracy: 0.7575 - val_loss: 0.5140 - val_binary_accuracy: 0.7741\n",
            "Epoch 31/60\n",
            "16399/16399 [==============================] - 0s 4us/step - loss: 0.5573 - binary_accuracy: 0.7586 - val_loss: 0.5096 - val_binary_accuracy: 0.7739\n",
            "Epoch 32/60\n",
            "16399/16399 [==============================] - 0s 4us/step - loss: 0.5509 - binary_accuracy: 0.7605 - val_loss: 0.5051 - val_binary_accuracy: 0.7754\n",
            "Epoch 33/60\n",
            "16399/16399 [==============================] - 0s 4us/step - loss: 0.5472 - binary_accuracy: 0.7610 - val_loss: 0.5018 - val_binary_accuracy: 0.7755\n",
            "Epoch 34/60\n",
            "16399/16399 [==============================] - 0s 4us/step - loss: 0.5461 - binary_accuracy: 0.7616 - val_loss: 0.4984 - val_binary_accuracy: 0.7751\n",
            "Epoch 35/60\n",
            "16399/16399 [==============================] - 0s 4us/step - loss: 0.5392 - binary_accuracy: 0.7609 - val_loss: 0.4951 - val_binary_accuracy: 0.7755\n",
            "Epoch 36/60\n",
            "16399/16399 [==============================] - 0s 4us/step - loss: 0.5319 - binary_accuracy: 0.7663 - val_loss: 0.4922 - val_binary_accuracy: 0.7759\n",
            "Epoch 37/60\n",
            "16399/16399 [==============================] - 0s 5us/step - loss: 0.5285 - binary_accuracy: 0.7637 - val_loss: 0.4897 - val_binary_accuracy: 0.7755\n",
            "Epoch 38/60\n",
            "16399/16399 [==============================] - 0s 4us/step - loss: 0.5260 - binary_accuracy: 0.7621 - val_loss: 0.4872 - val_binary_accuracy: 0.7758\n",
            "Epoch 39/60\n",
            "16399/16399 [==============================] - 0s 4us/step - loss: 0.5210 - binary_accuracy: 0.7654 - val_loss: 0.4853 - val_binary_accuracy: 0.7761\n",
            "Epoch 40/60\n",
            "16399/16399 [==============================] - 0s 4us/step - loss: 0.5213 - binary_accuracy: 0.7690 - val_loss: 0.4834 - val_binary_accuracy: 0.7775\n",
            "Epoch 41/60\n",
            "16399/16399 [==============================] - 0s 5us/step - loss: 0.5186 - binary_accuracy: 0.7671 - val_loss: 0.4813 - val_binary_accuracy: 0.7759\n",
            "Epoch 42/60\n",
            "16399/16399 [==============================] - 0s 4us/step - loss: 0.5173 - binary_accuracy: 0.7619 - val_loss: 0.4800 - val_binary_accuracy: 0.7764\n",
            "Epoch 43/60\n",
            "16399/16399 [==============================] - 0s 4us/step - loss: 0.5166 - binary_accuracy: 0.7668 - val_loss: 0.4785 - val_binary_accuracy: 0.7752\n",
            "Epoch 44/60\n",
            "16399/16399 [==============================] - 0s 4us/step - loss: 0.5101 - binary_accuracy: 0.7677 - val_loss: 0.4764 - val_binary_accuracy: 0.7766\n",
            "Epoch 45/60\n",
            "16399/16399 [==============================] - 0s 5us/step - loss: 0.5126 - binary_accuracy: 0.7637 - val_loss: 0.4756 - val_binary_accuracy: 0.7761\n",
            "Epoch 46/60\n",
            "16399/16399 [==============================] - 0s 5us/step - loss: 0.5082 - binary_accuracy: 0.7722 - val_loss: 0.4743 - val_binary_accuracy: 0.7765\n",
            "Epoch 47/60\n",
            "16399/16399 [==============================] - 0s 5us/step - loss: 0.5079 - binary_accuracy: 0.7683 - val_loss: 0.4733 - val_binary_accuracy: 0.7765\n",
            "Epoch 48/60\n",
            "16399/16399 [==============================] - 0s 4us/step - loss: 0.5094 - binary_accuracy: 0.7645 - val_loss: 0.4724 - val_binary_accuracy: 0.7762\n",
            "Epoch 49/60\n",
            "16399/16399 [==============================] - 0s 4us/step - loss: 0.5055 - binary_accuracy: 0.7671 - val_loss: 0.4721 - val_binary_accuracy: 0.7762\n",
            "Epoch 50/60\n",
            "16399/16399 [==============================] - 0s 4us/step - loss: 0.5033 - binary_accuracy: 0.7670 - val_loss: 0.4718 - val_binary_accuracy: 0.7778\n",
            "Epoch 51/60\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "16399/16399 [==============================] - 0s 5us/step - loss: 0.5005 - binary_accuracy: 0.7691 - val_loss: 0.4711 - val_binary_accuracy: 0.7766\n",
            "Epoch 52/60\n",
            "16399/16399 [==============================] - 0s 5us/step - loss: 0.5013 - binary_accuracy: 0.7696 - val_loss: 0.4706 - val_binary_accuracy: 0.7771\n",
            "Epoch 53/60\n",
            "16399/16399 [==============================] - 0s 5us/step - loss: 0.5031 - binary_accuracy: 0.7668 - val_loss: 0.4707 - val_binary_accuracy: 0.7771\n",
            "Epoch 54/60\n",
            "16399/16399 [==============================] - 0s 5us/step - loss: 0.5017 - binary_accuracy: 0.7679 - val_loss: 0.4693 - val_binary_accuracy: 0.7771\n",
            "Epoch 55/60\n",
            "16399/16399 [==============================] - 0s 5us/step - loss: 0.4975 - binary_accuracy: 0.7668 - val_loss: 0.4685 - val_binary_accuracy: 0.7771\n",
            "Epoch 56/60\n",
            "16399/16399 [==============================] - 0s 4us/step - loss: 0.4999 - binary_accuracy: 0.7694 - val_loss: 0.4673 - val_binary_accuracy: 0.7785\n",
            "Epoch 57/60\n",
            "16399/16399 [==============================] - 0s 4us/step - loss: 0.5013 - binary_accuracy: 0.7674 - val_loss: 0.4673 - val_binary_accuracy: 0.7774\n",
            "Epoch 58/60\n",
            "16399/16399 [==============================] - 0s 4us/step - loss: 0.4976 - binary_accuracy: 0.7699 - val_loss: 0.4667 - val_binary_accuracy: 0.7775\n",
            "Epoch 59/60\n",
            "16399/16399 [==============================] - 0s 4us/step - loss: 0.5031 - binary_accuracy: 0.7669 - val_loss: 0.4673 - val_binary_accuracy: 0.7795\n",
            "Epoch 60/60\n",
            "16399/16399 [==============================] - 0s 4us/step - loss: 0.4991 - binary_accuracy: 0.7671 - val_loss: 0.4665 - val_binary_accuracy: 0.7748\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pi4jOZIoYhcr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def plot_history(name, history, legend, plot_val=True):\n",
        "    fig, ax = plt.subplots(1,2,figsize=(14,6))\n",
        "    fig.suptitle(name)\n",
        "    \n",
        "    if not isinstance(history, list):\n",
        "        history = [history]\n",
        "        \n",
        "    for h in history:\n",
        "        acc = h.history['binary_accuracy']\n",
        "        loss = h.history['loss']\n",
        "        if plot_val:\n",
        "            val_loss = h.history['val_loss']\n",
        "            val_acc = h.history['val_binary_accuracy']\n",
        "        epochs = range(1, len(acc) + 1)\n",
        "\n",
        "        ax[0].set_title('Loss')\n",
        "        ax[0].set_xticks(ticks=epochs)\n",
        "        lb, ub = ax[0].get_xlim( )\n",
        "        ax[0].set_xticks( np.linspace(lb, ub, 6 ) )\n",
        "        ax[0].set_ylim([0, 1])\n",
        "        ax[0].set_yticks( np.linspace(0, 1, 6 ) )\n",
        "        ax[0].set_ylabel('Loss')\n",
        "        \n",
        "        ax[0].plot(epochs, loss)\n",
        "        if plot_val:\n",
        "            ax[0].plot(epochs, val_loss)\n",
        "            \n",
        "        ax[1].set_title('Accuracy')\n",
        "        ax[1].set_xticks(ticks=list(epochs))\n",
        "        ax[1].set_xlabel('Epochs')\n",
        "        lb, ub = ax[1].get_xlim( )\n",
        "        ax[1].set_xticks( np.linspace(lb, ub, 6 ) )\n",
        "        ax[1].set_ylim([0, 1])\n",
        "        ax[1].set_yticks( np.linspace(0, 1, 6 ) )\n",
        "        ax[1].set_ylabel('Accuracy')\n",
        "        ax[1].plot(epochs, acc)\n",
        "        \n",
        "        if plot_val:\n",
        "            ax[1].plot(epochs, val_acc)\n",
        "        \n",
        "    ax[0].legend([l+' loss' for l in legend])\n",
        "    ax[1].legend([l+' accuracy' for l in legend])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GBUJGVI3Yhcx",
        "colab_type": "code",
        "colab": {},
        "outputId": "6ee9be96-186e-4fac-dfce-89595278c44f"
      },
      "source": [
        "plot_history('Scores Modelo', history, ['train', 'val'])\n",
        "plt.savefig('Scores Red.jpg', dpi=300)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0AAAAGeCAYAAAC9yGE0AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+j8jraAAAgAElEQVR4nOzdd3gVVf7H8fdJL6SSUBJaQKQnlIAISBHFXhARlCIo2N21rKs/u+u6q66uvaGLFAso2LuogAooRXqvAqEkQHpPzu+PuQkBAgRIcpPcz+t55rm5M2dmvne4D2e+95Qx1lpEREREREQ8gZe7AxAREREREakuSoBERERERMRjKAESERERERGPoQRIREREREQ8hhIgERERERHxGEqARERERETEYygBEhERj2GMedQY804Fy842xoyr6phERKR6KQESEfEQxpg+xph5xpg0Y8x+Y8yvxpju7o6rPMaY/sYYa4z56LD1Ca71s90UmoiI1HJKgEREPIAxJhT4AngJiARigceAvEo+j3clHi4Z6GWMqV9m3bXA+ko8h4iIeBglQCIinuF0AGvt+9baImttjrX2O2vt8pICxpjxxpg1xpgMY8xqY0xX1/p2ru5gqcaYVcaYS8vsM8kY85ox5itjTBYwwBgTY4yZaYxJNsZsMcb8pUz5HsaYRcaYdGPMHmPMf48Rcz7wCTDcta83cBXwbtlCxphexpiFrpathcaYXmW2xRlj5rg+0/dA1GH79nS1iqUaY5YZY/qXF4gxxssY86AxZpsxZq8xZooxJuzYl1xERGoiJUAiIp5hPVBkjJlsjLnAGBNRdqMxZijwKDAaCAUuBfYZY3yBz4HvgAbA7cC7xpg2ZXa/BngCCAHmucovw2llGgjcYYw5z1X2BeAFa20o0Ar44DhxT3HFBHAesApIKhN3JPAl8CJQH/gv8GWZVqP3gMU4ic/jOC1IJfvGuvb9J06r2N+AmcaY6HLiGONaBgAtgXrAy8eJXUREaiAlQCIiHsBamw70ASzwJpBsjPnMGNPQVWQc8LS1dqF1bLTWbgN64tzsP2mtzbfW/ojTle7qMof/1Fr7q7W2GOgERFtr/+Eqv9l1vuGusgXAacaYKGttprV2wXHingdEuhKu0TgJUVkXARustVOttYXW2veBtcAlxphmQHfgIWttnrV2Lk5yVmIk8JW19itrbbG19ntgEXBhOaGMAP5rrd1src0E/g8YbozxOVb8IiJS8ygBEhHxENbaNdbaMdbaJkBHIAZ43rW5KbCpnN1igO2u5KbENpzWnRLby/zdHIhxdSlLNcakAvcDJYnW9Tjd8da6uqtdXIHQpwK34bS+fFxOfNsOW1cSXwxwwFqbddi2srEOPSzWPkDjcmI4/DzbAJ8yn0tERGoJ/XIlIuKBrLVrjTGTgBtdq7bjdEk7XBLQ1BjjVSYJasahExHYMn9vB7ZYa1sf5bwbgKuNMV7AFcAMY0z9w5KUw00FNgJTrLXZxpjD42t+WPlmwDfALiDCGBNc5vjNysS7HZhqrR1/jHMf7TzNgEJgTwX2FRGRGkQtQCIiHsAY09YYc7cxponrfVOcbmwlXdDeAv5mjOlmHKcZY5oDvwFZwN+NMb6uSQIuAaYd5VS/A+nGmHuNMYHGGG9jTMeS6baNMSONMdGuZCrVtU/RsWK31m4B+gEPlLP5K+B0Y8w1xhgfY8wwoD3whasL3yLgMWOMnzGmjyv2Eu/gdJU7zxVngGv67SblnOd94E7XpAr1gH8B0621hceKXUREah4lQCIiniEDOAP4zTVb2wJgJXA3gLX2Q5yJDN5zlf0EiLTW5uNMiHABkAK8Coy21q4t7yTW2iKcJKMzsMW1z1tAyYxp5wOrjDGZOBMiDLfW5h4veGvtL9bapHLW7wMudn2OfcDfgYuttSmuIte4Pvd+4BHKjCGy1m4HLsPpopeM0yJ0D+XXjRNxWqLmuj5XLs6EECIiUssYa+3xS4mIiIiIiNQBagESERERERGPoQRIREREREQ8hhIgERERERHxGEqARERERETEYygBEhERERERj6EESEREREREPIYSIBERERER8RhKgERERERExGMoARIREREREY+hBEhERERERDyGEiAREREREfEYSoBERERERMRjKAESERERERGPoQRIREREREQ8hhIgERERERHxGEqARERERETEYygBEhERERERj6EESKQSGGO2GmPOcXccIiJS9xljZhtjDhhj/N0di0htpARIREREpJYwxrQAzgIscGk1ntenus4lUtWUAIlUEWOMvzHmeWNMkmt5vuTXOmNMlDHmC2NMqjFmvzHmZ2OMl2vbvcaYncaYDGPMOmPMQPd+EhERqUFGAwuAScC1JSuNMU2NMR8ZY5KNMfuMMS+X2TbeGLPGVa+sNsZ0da23xpjTypSbZIz5p+vv/saYHa46aTfwtjEmwlV3JbtaoL4wxjQps3+kMeZtV513wBjziWv9SmPMJWXK+RpjUowxnavsKokcgxIgkarzANAT6AwkAD2AB13b7gZ2ANFAQ+B+wBpj2gC3Ad2ttSHAecDW6g1bRERqsNHAu67lPGNMQ2OMN/AFsA1oAcQC0wCMMUOBR137heK0Gu2r4LkaAZFAc+AGnPvGt13vmwE5wMtlyk8FgoAOQAPgOdf6KcDIMuUuBHZZa5dWMA6RSqXmTJGqMwK43Vq7F8AY8xjwBvAQUAA0BppbazcCP7vKFAH+QHtjTLK1dqs7AhcRkZrHGNMHJ/n4wFqbYozZBFyD0yIUA9xjrS10Ff/F9ToOeNpau9D1fuMJnLIYeMRam+d6nwPMLBPPE8BPrr8bAxcA9a21B1xF5rhe3wEeMsaEWmvTgVE4yZKIW6gFSKTqxOD8Gldim2sdwH9wKqHvjDGbjTH3AbiSoTtwfq3ba4yZZoyJQURExOny9p21NsX1/j3XuqbAtjLJT1lNgU0neb5ka21uyRtjTJAx5g1jzDZjTDowFwh3tUA1BfaXSX5KWWuTgF+BIcaYcJxE6d2TjEnklCkBEqk6STi/1JVo5lqHtTbDWnu3tbYlcAlwV8lYH2vte9bakl/5LPBU9YYtIiI1jTEmELgK6GeM2e0al3MnThfrPUCzo0xUsB1odZTDZuN0WSvR6LDt9rD3dwNtgDOstaFA35LwXOeJdCU45ZmM0w1uKDDfWrvzKOVEqpwSIJHK42uMCShZgPeBB40x0caYKOBhnG4AGGMuNsacZowxQDpQBBQZY9oYY852TZaQi9PdoMg9H0dERGqQy3Hqg/Y4Y0s7A+1wulBfDuwCnjTGBLvqod6u/d4C/maM6WYcpxljSn6cWwpcY4zxNsacD/Q7TgwhOPVSqjEmEnikZIO1dhfwNfCqa7IEX2NM3zL7fgJ0Bf6KMyZIxG2UAIlUnq9wKoaSJQBYBCwHVgBLgH+6yrYGZgGZwHzgVWvtbJzxP08CKcBunEGk91fbJxARkZrqWuBta+2f1trdJQvOJARX4/QmOA34E2eSnWEA1toPgSdwustl4CQika5j/tW1XyrOuNVPjhPD80AgTh21APjmsO2jcMa4rgX24nTpxhVHyfihOOCjE/zsIpXKWHt466aIiIiISOUyxjwMnG6tHXncwiJVSLPAiYiIiEiVcnWZux6nlUjEraqsC5wxZqIxZq8xZuVRthtjzIvGmI3GmOUlD+USERGpDqqnRKqHMWY8ziQJX1tr57o7HpGqHAM0CTj/GNsvwBkH0Rrn4VqvVWEsIiIih5uE6imRKmetfdNaG2ytvcndsYhAFSZArgx//zGKXAZMsY4FOPPIN66qeERERMpSPSUi4pncOQtcLE5zaIkdrnUiIiI1geopEZE6yJ2TIJhy1pU7JZ0x5gac7gcEBwd3a9u2bVXGJSIix7F48eIUa220u+OoYqqnRERqqWPVU+5MgHYATcu8bwIklVfQWjsBmACQmJhoFy1aVPXRiYjIURljtrk7hmqgekpEpJY6Vj3lzi5wnwGjXbPs9ATSXE8RFhERqQlUT4mI1EFV1gJkjHkf6A9EGWN2AI8AvgDW2teBr4ALgY1ANjC2qmIRERE5nOopERHPVGUJkLX26uNst8CtVXV+ERGRY1E9JSLimdw5BkhEpEoVFBSwY8cOcnNz3R1KrRUQEECTJk3w9fV1dygiItVKdUjtcDL1lBIgEamzduzYQUhICC1atMCY8ib0kmOx1rJv3z527NhBXFycu8MREalWqkNqvpOtp9w5CYKISJXKzc2lfv36qrhOkjGG+vXr69dPEfFIqkNqvpOtp5QAiUidporr1Oj6iYgn0/+BNd/J/BspARIRqSKpqam8+uqrJ7XvhRdeSGpqaoXLP/roozzzzDMndS4REal5qrMO8TRKgEREqsixKq+ioqJj7vvVV18RHh5eFWGJiEgtUBfrEGstxcXF7g5DCZCISFW577772LRpE507d+aee+5h9uzZDBgwgGuuuYZOnToBcPnll9OtWzc6dOjAhAkTSvdt0aIFKSkpbN26lXbt2jF+/Hg6dOjAoEGDyMnJOeZ5ly5dSs+ePYmPj2fw4MEcOHAAgBdffJH27dsTHx/P8OHDAZgzZw6dO3emc+fOdOnShYyMjCq6GiIiciKqsw75/PPPOeOMM+jSpQvnnHMOe/bsASAzM5OxY8fSqVMn4uPjmTlzJgDffPMNXbt2JSEhgYEDBwJH9kTo2LEjW7duLY3hlltuoWvXrmzfvp2bb76ZxMREOnTowCOPPFK6z8KFC+nVqxcJCQn06NGDjIwMzjrrLJYuXVpapnfv3ixfvvyUrq1mgRMRj/DY56tYnZReqcdsHxPKI5d0OOr2J598kpUrV5b+xz179mx+//13Vq5cWTpbzcSJE4mMjCQnJ4fu3bszZMgQ6tevf8hxNmzYwPvvv8+bb77JVVddxcyZMxk5cuRRzzt69Gheeukl+vXrx8MPP8xjjz3G888/z5NPPsmWLVvw9/cv7RrxzDPP8Morr9C7d28yMzMJCAg41csiIlLn1PU6pE+fPixYsABjDG+99RZPP/00zz77LI8//jhhYWGsWLECgAMHDpCcnMz48eOZO3cucXFx7N+//7ifdd26dbz99tulLVpPPPEEkZGRFBUVMXDgQJYvX07btm0ZNmwY06dPp3v37qSnpxMYGMi4ceOYNGkSzz//POvXrycvL4/4+PiKX+hyqAVIRKQa9ejR45CpOl988UUSEhLo2bMn27dvZ8OGDUfsExcXR+fOnQHo1q0bW7duPerx09LSSE1NpV+/fgBce+21zJ07F4D4+HhGjBjBO++8g4+P8/tX7969ueuuu3jxxRdJTU0tXS8iIjVPVdUhO3bs4LzzzqNTp0785z//YdWqVQDMmjWLW289+DzoiIgIFixYQN++fUvjiIyMPG7czZs3p2fPnqXvP/jgA7p27UqXLl1YtWoVq1evZt26dTRu3Jju3bsDEBoaio+PD0OHDuWLL76goKCAiRMnMmbMmONfqONQTSciHuFYv7JVp+Dg4NK/Z8+ezaxZs5g/fz5BQUH079+/3Kk8/f39S//29vY+bhe4o/nyyy+ZO3cun332GY8//jirVq3ivvvu46KLLuKrr76iZ8+ezJo1i7Zt257U8UVE6qq6Xofcfvvt3HXXXVx66aXMnj2bRx99FHDG7Bw+y1p56wB8fHwOGd9TNpaycW/ZsoVnnnmGhQsXEhERwZgxY8jNzT3qcYOCgjj33HP59NNP+eCDD1i0aFF5l+aEqAVIRKSKhISEHHNMTVpaGhEREQQFBbF27VoWLFhwyucMCwsjIiKCn3/+GYCpU6fSr18/iouL2b59OwMGDODpp58mNTWVzMxMNm3aRKdOnbj33ntJTExk7dq1pxyDiIicuuqsQ9LS0oiNjQVg8uTJpesHDRrEyy+/XPr+wIEDnHnmmcyZM4ctW7YAlHaBa9GiBUuWLAFgyZIlpdsPl56eTnBwMGFhYezZs4evv/4agLZt25KUlMTChQsByMjIoLCwEIBx48bxl7/8he7du1eoxel4lACJiFSR+vXr07t3bzp27Mg999xzxPbzzz+fwsJC4uPjeeihhw7pHnAqJk+ezD333EN8fDxLly7l4YcfpqioiJEjR9KpUye6dOnCnXfeSXh4OM8//zwdO3YkISGBwMBALrjggkqJQURETk111iGPPvooQ4cO5ayzziIqKqp0/YMPPsiBAwdK64mffvqJ6OhoJkyYwBVXXEFCQgLDhg0DYMiQIezfv5/OnTvz2muvcfrpp5d7roSEBLp06UKHDh247rrr6N27NwB+fn5Mnz6d22+/nYSEBM4999zSVqRu3boRGhrK2LFjT/ozlmWstZVyoOqSmJhoK6PpS0TqvjVr1tCuXTt3h1HrlXcdjTGLrbWJbgqpRlM9JVI3qA6pOZKSkujfvz9r167Fy+vI9psTrafUAiQiIiIiIjXSlClTOOOMM3jiiSfKTX5OhiZBEBERERGRGmn06NGMHj26Uo/pUS1AKZl5zNuU4u4wRERERETETTwqAXr2u/WMfOs33pizido29klERERERE6dR3WBe/CidqTnFPDvr9eyfGca/7kyniA/j7oEIiIiIiIezaNagIL9fXj5mi783wVt+XrFLga/Mo+tKVnuDktERERERKqJRyVApG7HrJjBjf1aMfm6HuzJyOXSl3/hp7V73R2ZiAgA9erVO6H1IiIiJVRXVIxnJUC/PAcfjYO5z3DWaVF8flsfmkQEcd3khfzn27UUFBW7O0IRERERkVqpsLDQ3SFUiGclQOc/CZ2Gwo+Pw9d/p2m4PzNv7sVV3Zryyk+bGPLaPDYlZ7o7ShGpI+69915effXV0vePPvoozz77LJmZmQwcOJCuXbvSqVMnPv300wof01rLPffcQ8eOHenUqRPTp08HYNeuXfTt25fOnTvTsWNHfv75Z4qKihgzZkxp2eeee67SP6OIiFSNyqxDLr/8crp160aHDh2YMGFC6fpvvvmGrl27kpCQwMCBAwHIzMxk7NixdOrUifj4eGbOnAkc2ro0Y8YMxowZA8CYMWO46667GDBgAPfeey+///47vXr1okuXLvTq1Yt169YBUFRUxN/+9rfS47700kv88MMPDB48uPS433//PVdcccXJX7QK8qwZAHz8YPAEqNcQ5r8MmXsJvGICT10Zz4C20dz30QouevFnHryoPSPOaIYxxt0Ri0hl+fo+2L2ico/ZqBNc8ORRNw8fPpw77riDW265BYAPPviAb775hoCAAD7++GNCQ0NJSUmhZ8+eXHrppRX6P+ejjz5i6dKlLFu2jJSUFLp3707fvn157733OO+883jggQcoKioiOzubpUuXsnPnTlauXAlAampq5XxuERFPU8vrkIkTJxIZGUlOTg7du3dnyJAhFBcXM378eObOnUtcXBz79+8H4PHHHycsLIwVK5zPe+DAgeN+lPXr1zNr1iy8vb1JT09n7ty5+Pj4MGvWLO6//35mzpzJhAkT2LJlC3/88Qc+Pj7s37+fiIgIbr31VpKTk4mOjubtt99m7NixJ3IVT4pnJUAAXl5w3hMQ0gi+exCy98Hwdzm/Y2O6NIvgbx8u48FPVvLT2r08dWU8UfX83R2xiNRSXbp0Ye/evSQlJZGcnExERATNmjWjoKCA+++/n7lz5+Ll5cXOnTvZs2cPjRo1Ou4xf/nlF66++mq8vb1p2LAh/fr1Y+HChXTv3p3rrruOgoICLr/8cjp37kzLli3ZvHkzt99+OxdddBGDBg2qhk8tIiKVoTLrkBdffJGPP/4YgO3bt7NhwwaSk5Pp27cvcXFxAERGRgIwa9Yspk2bVrpvRETEcWMdOnQo3t7eAKSlpXHttdeyYcMGjDEUFBSUHvemm27Cx8fnkPONGjWKd955h7FjxzJ//nymTJlyopfqhHleAlSi1+0Q3AA+vQXevghGzqBhaCMmj+3B5Plb+ffXa7nwhZ/537Xd6dQkzN3RisipOsavbFXpyiuvZMaMGezevZvhw4cD8O6775KcnMzixYvx9fWlRYsW5ObmVuh4R3uGWd++fZk7dy5ffvklo0aN4p577mH06NEsW7aMb7/9lldeeYUPPviAiRMnVtpnExHxGLW4Dpk9ezazZs1i/vz5BAUF0b9/f3Jzc7HWlttqdLT1Zdcdfr7g4ODSvx966CEGDBjAxx9/zNatW+nfv/8xjzt27FguueQSAgICGDp0aGmCVJU8awzQ4RKGwTUfwP7N8L9zIWUjXl6Gsb3j+PTW3vh6e3HVG/P5Yc0ed0cqIrXU8OHDmTZtGjNmzODKK68EnF/HGjRogK+vLz/99BPbtm2r8PH69u3L9OnTKSoqIjk5mblz59KjRw+2bdtGgwYNGD9+PNdffz1LliwhJSWF4uJihgwZwuOPP86SJUuq6mOKiEgVqIw6JC0tjYiICIKCgli7di0LFiwA4Mwzz2TOnDls2bIFoLQL3KBBg3j55ZdL9y/pAtewYUPWrFlDcXFxaWvS0c4XGxsLwKRJk0rXDxo0iNdff710ooSS88XExBATE8M///nP0nFFVc2zEyCA0wbCmM8hPwsmDoKdiwFo1ziUj2/pxWkN6jF+yiKmzN/q1jBFpHbq0KEDGRkZxMbG0rhxYwBGjBjBokWLSExM5N1336Vt27YVPt7gwYOJj48nISGBs88+m6effppGjRoxe/ZsOnfuTJcuXZg5cyZ//etf2blzJ/3796dz586MGTOGf//731X1MUVEpApURh1y/vnnU1hYSHx8PA899BA9e/YEIDo6mgkTJnDFFVeQkJDAsGHDAHjwwQc5cOAAHTt2JCEhgZ9++gmAJ598kosvvpizzz67NJby/P3vf+f//u//6N27N0VFRaXrx40bR7NmzUrrsPfee69024gRI2jatCnt27c/uQt1gszRulPUVImJiXbRokWVf+CUjfDOYMjaB8OmOokRkJ1fyF/e/4NZa/Yyrk8c91/YDi8vTY4gUhusWbOGdu3auTuMWq+862iMWWytTXRTSDValdVTIlKtVIdUn9tuu40uXbpw/fXXn9T+J1pPqQWoRNRpcN13EBkH710Fyz8EIMjPhzdGJTKmVwve+mULN7+7mJz8ouMcTEREREREjqdbt24sX76ckSNHVts5lQCVFdoYxnwJTc9wHpi64DUAvL0Mj17agYcvbs93q/cw8n+/kZqd7+ZgRURERERqt8WLFzN37lz8/atv5mUlQIcLDIeRH0Hbi+Gb++DXF0o3Xdcnjleu6cqKHWkMfX0+u9Jy3BioiIiIiIicKCVA5fENgKGTocNg+P7hQ5KgCzs1ZtJ13dmdlsuQV+excW+GGwMVkeOpbeMcaxpdPxHxZPo/sOY7mX8jJUBH4+0DV7wFHa5wkqBfni/d1KtVFNNu7El+keXK1+ezeNvxn5ArItUvICCAffv2qQI7SdZa9u3bR0BAgLtDERGpdqpDar6Trac890GoFeHtA1e86fw96xHntc8dAHSICeOjm3sxeuJvjHhrAa+O6MrZbRu6KVARKU+TJk3YsWMHycnJ7g6l1goICKBJkybuDkNEpNqpDqkdTqaeUgJ0PCVJkDFHJEHN6gcx4+ZejH17ITdMWcx/h3Xm0oQYNwYrImX5+voSFxfn7jBERKQWUh1Sd6kLXEV4+8DgCdBxiJMEuWaHA4iq589748+ga/MI/jrtD9777U83BioiIiIiIseiBKiiSpKgdpc4s8OtmFG6KSTAlynX9aD/6dHc//EKJszd5MZARURERETkaJQAnYiSiRGa94aPb4JNP5VuCvD15o1RiVwU35h/fbWWZ79bp0FzIiIiIiI1jBKgE+UbAMPfg6jTYfpISFpausnPx4sXh3dhePemvPTjRh77fDXFxUqCRERERERqCiVAJyMwHEbOhMAIePdK2L+5dJO3l+HfV3RiXJ84Js3byj++WK2WIBERERGRGkIJ0MkKbQwjP4LiIpg6GDL3lm4yxvDARe243pUEvfTjRjcGKiIiIiIiJZQAnYro0+GaDyBjD7w7FPKzSjcZY3jgwnZc0TWW/36/nqnzt7otTBERERERcSgBOlVNu8PQSbBrGXx8IxQXl27y8jI8NSSec9o14OHPVvHZsiT3xSkiIiIiIkqAKkWb82HQP2HN5/DTPw/Z5OvtxcvXdKV780jumr6U2ev2HuUgIiIiIiJS1ZQAVZYzb4Wu18LPz8KyaYdsCvD15q0xibRuGMLN7yxh8bYDbgpSRERERMSzKQGqLMbARc9Ci7Pgs9th2/xDNoe6HpbaMNSfUf/7jZ/WqiVIRERERKS6KQGqTN6+cNUUCGsK00fAga2HbI4O8Wf6jWcSFxXM9ZMXamIEEREREZFqpgSosgVFOjPDFRfCe8MgN/2QzQ1DA/jgxjMZ0KYBD326in9+sZoiPSxVRERERKRaKAGqClGnOS1BKRuOmBkOINjfhwmjExnTqwVv/bKFW95dTE5+kZuCFRERERHxHEqAqkrL/nDev2DdVzD36SM2e3sZHr20A49c0p7vVu9h+IT5pGbnV3uYIiIiIiKeRAlQVTrjRki4Gmb/G9Z+WW6Rsb3jmDAqkTW7Mrjl3SUUFBWXW05ERERERE6dEqCqZAxc/BzEdIGPboTk9eUWO7d9Q/51RSfmbdrHPz5fXc1BioiIiIh4DiVAVc03EIa9A74BMO1qyE0rt9iV3ZpwQ9+WTF2wjakLtlVzkCIiIiIinkEJUHUIawJDJzvTYn90wxGTIpS49/y2DGgTzaOfrWLeppTqjVFERERExAMoAaouLXrDef+G9d/AnCfLLeLtZXjx6i60jArmlneXsG1fVjUHKSIiIiJStykBqk49xkPnETDnKVj3dblFQgJ8eevaRACun7yIjNyC6oxQRERERKROUwJUnYyBi56Fxp2drnD7NpVbrHn9YF4d0ZWtKVmMm7yItGwlQSIiIiIilaFKEyBjzPnGmHXGmI3GmPvK2d7MGPOTMeYPY8xyY8yFVRlPjeAbCMOmgpcPTBsBeZnlFuvVKopnr0pgyZ8HGPzqr2xJUXc4EZHKpnpKRMTzVFkCZIzxBl4BLgDaA1cbY9ofVuxB4ANrbRdgOPBqVcVTo3LnjO8AACAASURBVIQ3gysnQso6+Ow2sLbcYpd1juW98T1JzSng8ld+Zd5GTYwgIlJZVE+JiHimqmwB6gFstNZuttbmA9OAyw4rY4FQ199hQFIVxlOztBoA5zwKqz6GeS8dtVj3FpF8emtvGob6M3ri77z7m6bIFhGpJKqnREQ8UFUmQLHA9jLvd7jWlfUoMNIYswP4Cri9vAMZY24wxiwyxixKTk6uiljdo9dfoP1lMOsR2DznqMWaRgYx8+ZenNU6igc+Xsljn6+iuLj8ViMREakw1VMiIh6oKhMgU866w+/arwYmWWubABcCU40xR8RkrZ1grU201iZGR0dXQahuYgxc9gpEnQ4zxkLajqMWdWaH6851veN4+9etPD9rfTUGKiJSJ6meEhHxQFWZAO0AmpZ534Qjuw5cD3wAYK2dDwQAUVUYU83jHwLD3oHCPPjoRiguOmpRby/DQxe346rEJrz440a+W7W7GgMVEalzVE+JiHigqkyAFgKtjTFxxhg/nMGjnx1W5k9gIIAxph1OxeJ5fQeiWsMFT8O2X+DX549Z1BjDPy7rSEKTMO76YBkb95Y/i5yIiByX6ikREQ9UZQmQtbYQuA34FliDM4vOKmPMP4wxl7qK3Q2MN8YsA94Hxlh7lCnR6rrO10CHwfDTv2DH4mMWDfD15rWR3fD38eLGqXpYqojIyVA9JSLimUxt+388MTHRLlq0yN1hVI2cA/D6Wc4zgm762ekedwwLNu9jxFu/MbBtA14f2Q0vr/K6s4uIVD5jzGJrbaK746iJ6nQ9JSJSmXLTIWM3RDQHH/9KPfSx6imfSj2TnJrACLhiAky6CL6+Fy4/9uMmeraszwMXtuMfX6zm1dkbue3s1tUUqIiIiEglshaKC8F4gZd3xfexxVBU4OxbXAAFOZCVAlnJkL3v4KtfPajX0LU0cF79Q6Ao/7ClEPyCICAc/IKdCavKKi6C7P3OcXP2Oz9a+/iDTyD4BoBPABhvKMpzjldYctyj9NYxgJevcxwvH+eze/kceV5rIS8DsvZC5l7I3OO85qaCX4hzDxkY7rwGhIOXl3POwrwyn+1UewxZyE1zXV/XNc5Kdq55QJhz/oDwg7EYb+ffp+ySn+lM+pW2A1K3Q16ac2jfIGhxFpw2EE47ByJbHnkNKpESoJqmeS84628w92nnS9BxyDGLj+3dghU703j2+/W0aRTKue0bVlOgIiIiUmHZ++HPBbBzEeRnl1/GeDk3rqZk8YbgaAiLhdAYCI113hsv50Yys8zNcHaKc4N7+A1nUaHr5jfv4A1xcQF4+zs37r6Bzk27b6DrRrzs+b2cY2Tvg6ySZCLl4E1v2fMUFwMWvH2dY3v7OX/7+LviKJsMlFkK8w8mC2WvQ8kxfPychKC40FmKCg8mO8WFFbv2JZ/jRHn5HkwqjJfr8+/nyMki3cQn0IktPxPy0qv3vPWiISgKQho73528dOd7sm+Tk5TlprmuuXGSupLvk08AhDWF8GbOPW9YU+c7nbQENs6CDd8654hoAaed64yR96r8ETtKgGqifvfC5p/g8zuhSXfnS3IUxhj+NbgTG/dmcuPURdx3QVvGn9USU4VZs4iISJ1QmO/c0OemOzfapb/iBx68cS/IgcJcZynIhYIsp3xeuvOam+b8Mu/l7bQY+AY5LQi+wU6ZP+fDtvmQvMY5p5ePs+0I9sjkpbjwyBt3Lx8nMSg4ShJ1NCUJj7evc2NflOckQwU5VOiG3jcYgqOcJSQG/OsdmigZL8Ac2tpQcg7j5bQQlCRF3v4HkyNvv0MX7KGtFoV5znXwLttKUtJS4uta732wFcXH37mhLl3qO60SBTmHtZ7scf7dDo/Fy8dJKHJSnRv5nAPO37bIuWEPdt34B0dBUKTrO5ILhTkHr6ctdiVv/hQZHxb8mcmvW9KIrhfAaQ3qcVqDejQKC8BgDv47H76U+28QBCGNDrZi+dU72EpSVOh8F3MOOHEXFznfaW+/g5/R25fyZ/8/AQGhzvf8eEqG2JS5Hy0utuQXFRPgW04LX+erndf9m2HjD86yd02VJD+gMUA11/4tznigBu1gzJfOl/gYMvMK+fuMZXy1YjcXdWrMU1fGU89f+a2IVA2NATo6j6mnTtSWnyHpD+fG2S/E9VrPuakryi+TZLgSjqM9FqLkBti75EbY1yl/SEKSDnmZB1sJSlsOCpwyWclOF56S7jdVyT8UmvaAZmc6N9AxXZ0kqyKsdX5VT98J6UlOt6H0JOd6lXTjKnkNinLuFQ5PSkqu19F+GLXWOV5BzsGEq+wCEBjpJHW1yN6MXJb+mcof21NZuTONAF9vYsMDiQkPICY8kJjwQOLqBxMRfOz7KwBrLem5hWTkFpCRW0hGbiGZeQXk5BfTplE9WkXXO+KH56Jiy6dLd/LiDxvYui+buKhg9mXmkZ7rJDf1g/3o2jyCbs0jSGweQcfYsPITA9f5kzPzyMgtxMsYvAx4GYMxUFhkSUrLYccBZ9l5IIcdB7JJzy0kr7CIvIJi8gqLyC0oxgAto4M5vWEIbRqF0LphCKc3rIevtxdpOQWkZheQlpNPWo7z2UICfAgL9CUs0JfQQF9CAnxIzshjU3Imm/ZmOa/JmaRmFxATHkBseCBNIoKIjQikcVgA+zLz2ZySyebkLDYnZ7FlXxYFRcW0qB9M20YhtG0USptGTiyBvt4UWUtxsaXYWoqKLd4GmkfVO+nvwLHqKSVANdnKj5wHpPa4AS78z3GLW2t58+fNPPn1WlpF1+P1Ud1oFX3yXxwRkaNRAnR0HlVPVURWCnzzf7Dig+o7p3+ok1yVtBqUth54O9uCo12tGa5X/9CDSVjZX/ON96EtQr6BzuIf6vwSHhAG/mHOWBJb5HRtK8hyvWY7+0S3rfiYFg9krWX1rnS+WbmbWWv24u0FraLrHVwaBNOifvBRkwOA1Ox8ViWlsyopjeU70vjjz1R2puYA4ONlaNMohMIiy87UHDLzDrauGAPdmkVwTvuGnNOuIa2ig0sTmcy8Qn7ZkMKc9XuZvS6ZXWm5Rz1//WA/EltE0COuPj1aRLI5JZMXftjA5uQs2jUO5c5zWnNu+4ZYC5uSM1m07QCLXcuWlCwA/Ly96BgbSmKLSNo2CmF3eu4hSUZG7vG7+xkDjUKdRCQi2A9/Hy8CfL3x9/HC38ebouJiNiZnsn5PJskZeRX69znWuWLDA2kVXY/IYD+SUp0EbHd6LkXFB3MLby9Ds8ggWkYF0zI6mEA/H9bvzmDdngy27sviWGlITFgA8/5v4CnEqASo9vr2AZj/MgyeAAnDKrTLvI0p3Pb+H+QXFvPsVQmc16FRFQcpIp5GCdDReVw9dTTWwrJp8O39Tlejs+6Cnjc7iUVehrPkZzotD96+hyYZPgFOwnLkQZ2WobJdhYoKnP1KkhK/kCrrNiMHFRdb8gqLyS0oIq/wYCtDbkERuQVF5BQ47/MKi/AyhiA/bwL9vAn28yHIz5v03EK+W7Wbb1btZtu+bLwMJLaIJMDXm017M0sTmBKhAT40CA0gup4/0SH+pTfdq5LSDykbGx5I52bhdGkaTpdm4XSIObRlJT23gF2puSSl5rB0eyo/rN3Dyp3O+Jm4qGD6nBbFxr2ZLNq2n4IiSz1/H/qcFkXX5uGEBfoSEuC0hIQE+OLrbVi5M43ftxxg4db9/Ln/YLfEto1CuOOc1gxq3+iYs/SmZOaxxJUMLdp2gBU70sgvclreGoUG0KpBMC2j6tEq2mmtshaKraXY9eptDI3DA2gSHkSjsAD8fCr23d+flc/6PRls2JNBsYXwIN/S1p7wID8CfL3IyC0kLaeA9JyC0tf69fxpFV2PuKhgAv2OTEoLi4rZnZ7LrrRcIoL8aBYZdNSYsvMLWb8nkw17Migosnh7OS1b3l6m9Dsz6BTuYZUA1WZFhTDlMti5GMZ9D406VWi3nak53PLOYpbtSOONUd2UBIlIpVICdHQeV0+VZ/8W+OJOZzxr0zPgkhecLt1S5QqKivlzf7ar21EmXsYQ3ySMjrFhBB+ja3xxsSUjt5D03AJnyXH+Ts3OZ1daLrvTcklKy2VXag670nIPaUk5WT5ehl6nRXFBx0ac274hUfUOToOck1/E5pRMNiVn8ee+LJIz8kjOzCM5I4+9GXmkZOTRMDSA9jGhdIwNo0NMKB1iwoisQJe2wyWl5vDDmj18v2YvCzbto2V0MP3aRDOgTQO6NY/A17tiScXutFx+37qfYD9vBrRpcFKPJ8krLOLPfdk0CgsgJMD3hPeXg5QA1XaZe+GNfk7f3htmOzN+VEBuQRHD3pjPpuQsPrm1F6c1OPZzhUREKkoJ0NF5ZD1VIn0XzHsRFk10xuac+yh0u04tMlVob3ous9bsZc76vWzYm8mf+7IpLD7y3s7LwOkNQ+jcNJw2jUJIzS5gZ2oOSak57EzNYVdqbmnLQ3mi6vkTEx5A47AAGocFEhHkh7+vV2n3qgDfg6+Bvt74+zp/B/h6U1xsyc4vci2FZOcX4e1l6N0qirCgmnWTb63VRFJ1hBKgumD77/D2hdBqAFw9vcKVSVJqDpe89AthQb58cmtvQvVrgohUAiVAR+eR9VTaTvj1eVg82emWFj8MBj7kTN0slcpay6bkTL5bvYfvVu1h6fZUAJpEBNIxJoyW0cG0jK5Hy+hgWkXVo6C4mOU7Ulm6PY1l21NZtiOV1OwCjIEGIf7EhgcSGxFEbHgg0SH+hAb4lA54Dw1wukQ1CPXH30fjmKR20YNQ64KmPeD8f8NXrmcE9b+vQrvFhAfyyoiujHjrN+6avowJo7qdVJOsiIjIEfZtcsap/vGOM2NY52ugz10QGefuyGql4mLL5pQsVu5MY8XONNbsSnfNyHVo60lJC098kzDuPvd0BnVoxOkNj5yJrMTZbRtydlvnOYElM4qFB/pVeLyISF2jBKg26T7OGQs0+0mITYTW51Rot54t6/PgRe147PPVvPTjRv56TusqDlREROqs7P2w6mNngoMdvztd3bqOgj53HvO5dXKoomLL5uRMVialsWJHOit3prEqKY2sfGf6b38fL9o2DqVRaACBft4E+XkT5JpAIDYikIFtG9IorILTaZdhjKFByInvJ1KXKAGqTYyBi5+DXcvh4xvgpl8q3L1gTK8WrNiZxnOz1tMxNpSB7RpWcbAiIlInWAsZu5yu2Cs+hA3fOVNGR7eDcx5zuruFNnZ3lDWetZYlfx7gqxW7Wbo9ldVJ6eQUOMlOgK8X7RqHMqRbEzrGhtEpNozWDerhU8HB9yJyYpQA1Ta+gXDVZGdShBnXw7WfOw+DOw5jDP8a3In1ezK4Y9pSPr2tNy31jCARETlcbroze9uuZQeXrGRnW3C00xshYTg0ij/6wzU9SHJGHt+t3o2vlxctooJpERVEdD1/jDFYa1mzK4PPliXx+bIkdqbm4O/jRUKTcIb3aErHmDA6NQmjZVSwkh2RaqQEqDaKag2XPA8fjYefnoBzHqnQbgG+3rw+shuXvvwr109exEc396rQE5BFRMRD7FgEH46FtD+d5/BEt4PWg6BxgrPEJlboR7e6LregiB/X7mXm4h3MXp98yIMfAer5+9C8fhB5hcVs3JuJt5fhrNZR3D3odM5t31DTG4u4mf4Xq63ir4Ktv8Av/4XmvaD1uRXarUlEEG+M6saIN3/jxncWM/X6HprZRUTE01nrTGYw61Gna/WoT6DZmeDr2WNFiost+7Ly2ZuRy970PPZm5LJ8RxqfL0siPbeQhqH+jD+rJVd0jcXfx4stKVlsTcli675stqRkUVRsubZXCy7s2Ij6ZZ5xIyLupQSoNrvgKWdShI9c44HCYiu0W/cWkfxnaDx/nbaU+2au4L9XJWjOexERT5W9Hz65GdZ/A+0ugUtfhsBwd0flNrkFRXy0ZCeT521lY3LmEa07Ab5enNehEUO6NqH3aVF4l5lZtXn9YGhT3RGLyIlSAlSb+QbC0EkwoT/MuA7GfAHeFWtWv6xzLNv3Z/PMd+tpFhnEneeeXqWhiohIDfTnb079kbUXLvgP9BjvseN69mfl886CbUyet5V9Wfl0ig3jpn4taRASQIMQfxqElrzqmTgitZ0SoNouqjVc8gLMvB5++AcMerzCu9464DS27svmhR820CwyiCHdmlRhoCIiUqPsXAKTLoKwJnD9dxDTxd0RVStrLXvS81i9K40f1+5lxuId5BYUM6BNNDf0bUXPlpHqHSFSRykBqgs6XQnbfoV5L0KT7tD+0grtVjIzXFJqDvd9tJyY8EDObFW/ioMVERG3y8uEmeOgXgMY/yMERbo7oipXUFTMj2v3smjrflbvSmd1UjoHsgsA8PP24vIuMYw7qyWnNwxxc6QiUtWUANUV5z/pPB/ok1sgui1EV6xLm5+PF6+N7MaQ1+Zx49RFvDk6kTNaKgkSEanTvrkP9m92uk7X8eRnb3ou7/++nfd+38ae9Dz8fLxo2yiEQe0b0T4mlHaNQ2nXOEQzs4l4ECVAdYWPP1w1Bd7oC9NHOr/o+VfsOT9hgb5MGtud0RN/Z+T/fuNfgzsxNLFpFQcsIiJusfpT+GMq9LkLWvRxdzRVwlrLwq0HmDJ/K9+s3E1hsaXv6dE8cXlz+reJ1jN3RDycEqC6JCwWrpwIUy+HT291JkioYP/lJhFBfHxzb255bzH3zFjO5pQs7hnUBi8v9X8WEakz0nbCZ3+BmK4w4H53R1PpMnIL+OSPnbyz4E/W7ckgNMCHa3u1YGTP5sRFBbs7PBGpIZQA1TUt+8HAR2DWIzD/Feh1W4V3DQvyZdLYHjzy2Spem72JzcmZPDesM0F++pqIiNR6xUXw8Y1QVABD3qrwrKG1weqkdN75bRuf/rGTrPwiOsWG8dSQTlyaEEugn2ZsE5FD6c62Lur9V9i5CL5/GGI6n1AXB19vL564vCOtouvxxJerueqN+bw1ujuNwjz7YXgiIrXevBdh68/Oc37qt3J3NJVi5c40nvhyDfM378Pfx4tLEmIY1bM5CU099zlGInJ8SoDqImPgslcheSB8OAZumFPhh6Q6uxuu7xNHXFQQt7/3B1e+Po93x53hPOBNRERqn+2/w4//hPaXQZeR7o7mlKVk5vHsd+uYtnA7EUF+PHBhO4YmNiE8yM/doYlILaBRgHVVQCgMewcKcmHaNVCQc8KHOLttQ96/oSeZeYVc9cZ8Nu7NqIJARUSkSv3xLky+BEJjnOfG1eJn2+QXFvPWz5sZ8J/ZfLhoB9f1juOnv/VnfN+WSn5EpMKUANVl0W1gyJuwaxl8ehtYe8KHiG8SzvQbzqSoGK56YwErd6ZVQaAiIlLpCvPg8zvg01ucZ8SN+xECI9wd1Umx1jJr9R7Of2Eu//xyDd1aRPDNHX156OL2hAXWnbFMIlI9lADVdW0ugLMfhJUz4NfnT+4QjUL48KYzCfDx4uo3F7B424FKDlJERCpV6naYeD4sfht63wGjPoF60e6O6qSs3JnGNW/+xrgpiwB4e0x3Jo3twWkNKvaoBxGRwykB8gRn3Q0droBZj8H6b0/qEHFRwXx4cy/qB/sx6n+/MW9jSiUHKSIilWLzbJjQD1I2wFVT4dzHwLv2DflNSs3hrulLufilX1i3J4N/XNaBb+/oy4C2DdwdmojUckqAPIExcNkr0KgTzBwHyetO6jCx4YF8cNOZNI0IYszbC5m6YBv2JLrViYhIFcnPhvevhqAouOEnaH+puyM6YanZ+Tz1zVoGPDObL1bs4ub+rZh9T39Gn9kCXz3AVEQqgf4n8RR+QTD8PfDxdyrHnJPrxtYgJIDpN/bkzFb1eeiTldw5fSnZ+YWVHKyIiJyUvauhIBsGPgxRrd0dzQnJyC3ghVkbOOupn3h9ziYu6NiIH+/ux73ntyU0QON8RKTy1L42cTl54U2d7hCTL3Gmxx4x46QehBce5MfbY7rz8k8beW7WelYlpfPayG7qjy0i4m67ljmvjePdG8cJyM4vZMr8bbw+ZxOp2QWc16Ehd557Om0bhbo7NBGpo9QC5GmanwkXP+f0Ef/67yc1MxyAl5fhLwNbM/W6M9iXlc+lL//CZ8uSKjdWERE5MbuXQ0A4hDV1dyTHlZZTwKuzN9L36dk8+fVaOjcN57PbevPGqEQlPyJSpdQC5Im6joKU9c5TwaPaQM+bTvpQfVpH8eVf+nDbe3/wl/f/YMm2AzxwUTv10xYRcYddy53Wnxr8rJ+dqTlM/GUL037/k6z8IvqcFsVfz+lK9xaR7g5NRDyEEiBPdc5jsH8zfPt/EBkHp5930odqHBbItBt68q+v1vD2r1tZsyudV0Z0JaqefyUGLCIix1RU6IwB6j7O3ZGUa3NyJi/9uJHPlyVhgYvjGzP+rJZ0jA1zd2gi4mH0M72n8vKCKyZAw44w4zrYs+qUDufr7cUjl3Tgv1clsHR7Kpe+9AvLd6RWUrAiInJcKeuhMBcaJ7g7kiMs3naAy175lW9X7Wb0mS2Yc09/XhjeRcmPiLiFEiBP5hcMV08D/xB4bxhk7j3lQ17RtQkzb+6FMYYrX5/PzMU7KiFQERE5rt3LnddGNWsChF83pjDqf79RP9iP7+7sy8OXtKdJRJC7wxIRD6YEyNOFxcLV70NWijM9dn7WKR+yY2wYn93Wm8TmEdz94TIe/WwVhUXFlRCsiIgc1a7l4BNYo6a//n71Hsa+vZBmkUF8cNOZSnxEpEZQAiQQ0wWGvAlJS5yWoPzsUz5k/Xr+TLmuB9f3iWPSvK2MeXshadkFlRCsiIiUa/dyaNgBvLzdHQkAny7dyU3vLKZdTCjTbuhJg5AAd4ckIgIoAZIS7S6BwW/Atl/h/eFQkHPKh/Tx9uKhi9vz9JXx/LZlH5e/+iubkjMrIVgRETmEtU4C5Obn/1hrOZCVz5T5W7lj+lK6t4jg3XFnEB7k59a4RETK0ixwclD8VVBcBJ/c7HSHu3oa+J76L3ZXJTYlLiqYG6cuZvArv/LKiK6c1Tq6EgIWEREAUrdBblq1jv+x1vLhoh3MWrOHvRl5JLuWfFeX57PbNuDVEV0J8K0ZLVIiIiWUAMmhOl8Ntgg+vQ2mj4Bh71ZKEtS9RSSf3tqbcZMXMebthTxySXtGn9ni1OMVERFn/A9UWwvQ9v3Z3DtzOfM27aNF/SCaRgbRMjqYBiEBNAjxJzYikLPbNtAz4USkRlICJEfqMhJsMXx2O3wwGoZNBZ9Tf6ZP08ggZt7Sizum/cHDn67iqxW7GNenJWe3bYCXV819aJ+ISI23ezkYb2jQoUpPU1xsmTJ/K099sw5vL8O/Bnfi6h5NMTX4wasiIodTAiTl6zra6Q73xR0w83oYOrlSBtbW8/fhjVGJTJq3lf/9vJlxUxbRMiqYsX3iGNI1liA/fSVFRE7YruUQ3aZSWuyPZktKFn+fsYyFWw/Q7/Ro/n1FJ2LCA6vsfCIiVUVt03J0iWPhvH/Dms/hy7udQbaVwNvLcH2fOOb8fQAvXt2FkAAfHvpkJb2e/JFnvl1HanZ+pZxHRMRj7F5epeN/vlqxiwtemMu63Rk8MzSBSWO7K/kRkVpLP7fLsZ15C2TthV+eg3oNYMD9lXZoX28vLk2I4ZL4xizedoA3f97MK7M3MnneVq7rE8d1feIIC/SttPOJiNRJmcmQsatKxv9Ya3l19ib+8+06ujYL57WR3WgYqumsRaR2UwIkxzfwEchKhjlPQXA09BhfqYc3xpDYIpLEFpGs3Z3OC7M28MIPG3j71y2MP6slY/vEUc9fX1URkXLtXua8VnILUH5hMQ98vIIPF+/g0oQYnr4yXjO6iUidoLtKOT5j4OIXIGsffHUPBEdBh8FVcqq2jUJ5bWQ3ViWl8dz3G3j2+/VM/HUL1/eJY9SZLdQiJCJyuJIZ4Bp1qrRDpmbnc9M7i1mweT9/HdiaO85prYkORKTO0BggqRhvH7hyIjQ9Az66ATbPqdLTdYgJ461rE/nstt50aRbBM9+tp8+TP/L0N2tJycyr0nOLiNQqu5dDeHMIDK+Uw23bl8UVr85jybZUnh/WmTvPPV3Jj4jUKUqApOL8guCaaRDZCt4fDn+8W2kTIxxNfJNwJo7pzpd/6UPfNtG8NmcTvZ/8kUc/W0VSak6VnltEpFbYtbzSxv/sSc/lmjd/40B2Pu+OP4PLu8RWynFFRGoSJUByYgIjYPQnENMVPr0FZlwHOalVftoOMWG8ck1XfrirH5d1juGdBds47/m5LNz6/+3dd5xddZ3/8ddnejKT3nsjAQFJgNARpIoNLCiguFhZXPsq6uo2dfe3lrWhrNjAAoqIgoAgICBNwBQIJISShJCEhPQ6k8xkZr6/P84NGUMCIZk792bO6/l4nMe958yZez85SeYz7/s953vWFP29JalsNW+ENfNh6OS9fqmNW7by3iumsa6phV9+4CiOGNu/EwqUpPJjANIr12soXHADnPxv8Pgf4LLj4dkHuuStxw9q4OtnT+aOT5/IoIZazv/JQ9wxd3mXvLcklZ3nZ2ePezkC1NLazkVXzuDp5Rv5wfmHc/CIPp1QnCSVJwOQ9kxFJZzwGfjAbdnzn70B7vp/0NbaJW8/ZkA9v73oGPYf2osLfzmD381Y0iXvK0ll5fltEyDseQBqb0989tpZ3D9vNV97+yGcMGlQJxUnSeXJAKS9M3IqXHQfHHJuNk32lW+Fpq45LW1AQy2/+tDRHD2+P5/+7Sx+fM+CLnlfSSqapjWw/hV8oLPs0ez2BL2G7vFbfu3WJ7j+kaVc/Lr9efvhI/f4dSRpX2EA0t6r7QVv/QG85Qew6EH4ySmw8skueeuG2iouf+8RvOHVQ/nvm+fy1VueoL29uBMzSFLRPHYtfPsguPQouPWLMO8O2Lpl1/s/Pysb/dnDWdp+dv8z/PDuBZx/9Gj+6bUT9rBoSdq3GIDUeaa8C977x+yi3J+cCk/f3iVvTx1aygAAIABJREFUW1tVyffOO4x3HTWay+6ezxsuuZdbHltmEJK075l4Gpz+X9BrGPztR3Dl2+BrY+HKs2HGz2Dz2u37trbAiif2+Pqfe55ayZduepzTDhzCl8482KmuJeVGUQNQRJwREU9GxLyI+Pwu9nlnRDweEXMi4lfFrEddYNSR8KG7oN8Y+NU74a/fL/pU2QCVFcF/v+VgvnPOFFpa2/nwVTN5wyX38sdHDUKSdq3s+lT/cXDsx7LZNj+3EN71Wzj8gmymtxs/Af87Ca5+N8y5HpY9Au1b9+j6n+fWbeYTVz/MpMG9+O65U6isMPxIyo9IRfrlNCIqgaeA04AlwDTgvJTS4x32mQhcA5ycUlobEYNTSite6nWnTp2apk+fXpSa1YlaGuG6i2DuDTD5XfCGb0BtQ5e8dVt74sZZS7nkzqdZsLKRSUMa+NBrxnP6gUPp07O6S2qQuruImJFSmlrqOvbGPtWnUsoCz6O/hdnXwqblEBWQ2uFjM2HA7p++1tzaxjt/+CALVmziDx89jvGDuuZnsyR1pZfqU1VFfN8jgXkppQWFIq4GzgIe77DPh4BLU0prAV6uqWgfUlMP7/g53P1VuPvr8Oz92TVCY48r+ltXVgRvOXQEb548nJseXcr37pzHxdc+yr9UPMYxEwZw+kFDed2BQxjcu67otUgqa/tOn4qA4Ydmy+lfgWfuzsJQ8wboN+4VvdRXbnqcWYvXcdn5hxt+JOVSMQPQCGBxh/UlwFE77DMJICLuByqB/0wp/WnHF4qIC4ELAUaPHl2UYlUEFRVw0hdg/Gvh+g/Dz94Ix3wETv5XqO5R9LevrAjOmjKCNx8ynEefW8+tc57nT7Of59+un82//2E2h47qy0n7D+aESYN49Yg+VHgKiJQ3+2afqqiECSdnyyv0+5lLuPLBRfzjCeM54+A9nzlOkvZlxQxAO/ttcsfz7aqAicBrgZHAvRFxcEpp3d99U0o/An4E2akFnV+qimrMsXDR/XD7v8MD388mR3jrZTDisC55+4qKYMqovkwZ1ZfPvm5/nl6xiVtnP89tjy/nm7c/xTdvf4p+Pas5fuIgTpg4kJMPGMyAhtouqU1SSeWqT81dtoEvXPcYR43rz8Wv27/U5UhSyRQzAC0BRnVYHwks3ck+D6aUtgLPRMSTZI1mWhHrUinUNsCbvgUHvBFu+Fg2S9zxn4ITPwtVXRc2IoJJQ3oxaUgvPnbKRFZvaua+eau4+6mV3PPUKm6ctZQe1ZW897ix/OMJ4+nbs6bLapPU5XLTpzZs2cqHr5xB77pqvveuQ6mqdBJYSflVzJ+A04CJETEuImqAc4EbdtjneuAkgIgYSHaqgXez7M72OwU+/FeYfC7c+79w2Wtgcel+jxjQUMtZU0bwrXdOYdoXT+Gmjx3PaQcO4bK75/Oar93Fd//8NBu3bC1ZfZKKKjd96mu3PMGStZu59N2HMbiX1z9KyreiBaCUUivwUeBWYC5wTUppTkR8OSLOLOx2K7A6Ih4H7gIuTimtLlZNKhM9+sJb/g/O/102W9xPT4M/fQFamkpaVkRw8Ig+XHLeodzyiddwzIQBfPvPT/Gar9/FZXfPZ8vWtpLWJ6lz5aVPLVnbxDXTF3PekaM5Ymz/UpcjSSVXtGmwi8VpsLuZLRvgz/8J038K/cbCmy+B8SeWuqoXPLpkHd+87Snufmolw/vUcfEZ+3PW5BFOmKDc6w7TYBdLufWpL1z3GNdOX8JfLn4tw/sWfwIaSSoHL9WnPAlYpVXXO7s26L1/BAJ+cSb85nxYUx5nmBwysi8/f/+R/PpDR9O/oYZP/WYWZ116Pw/M36c+AJaUU0vWNvHb6Ys554hRhh9JKjAAqTyMPR7+6QE46V9h3p3w/SPh1i/C5nUv/71d4JgJA7jhI8fz7XMms3pTM+f9+EE++PPpzH5uPfvaKKqk/Pi/v8wnCD782t2/UaokdXcGIJWP6h5w4sXw8ZnZJAkPXAqXHAp/+zG0lX4igoqK4K2HjuTOz7yWi1+3Pw8uWM2bvncfp3zrbr59+1PMW7Gp1CVK0gsc/ZGknTMAqfz0GgpnfR/+8R4YchDc/Bn43uEw4+fQ2lLq6qirruQjJ+3HfZ87if9526sZ0quOS+58mlO/dTev/+69XHrXPOat2OjIkKSScvRHknbOSRBU3lLKbpz6l/+BpTOhz2h4zT/DlHdDVfnco2fFhi388bFl3DhrKTMXZaftjRnQk1NfNYRTXjWYI8b2p9r7bqgbcRKEXSuHPrVkbRMn/e9fOPeI0XzlLQeXtBZJKoWX6lPFvBGqtPciYNLpMPE0mPdn+MtX4aZPwr3fhBM+A4e+ByoqS10lg3vX8b7jxvG+48axdN1m7nhiBXfMXc4vH3iWn973DL3rqnjdQUN5x9RRHDG2HxHOIiepeBz9kaRdMwBp3xCRhaD9ToX5d2RB6MZPwLSfwOu/AWOOKXWFLxjetwfvOXoM7zl6DI3Nrdz79Cpue/x5bn5sGb+dsYRxA+s5+/CRvP2wkQzt4w0Jpb0VEW8Cbk4ptZe6lnKw7dqfc48Y7bU/krQTnpOjfUtEFoI+cDucfQU0rYErzoDffRA2LC11dS9SX1vFGQcP5VvvnMLfvngq//uOyQzqVcs3bn2SY796B++94m/c/Ngymlu9yaq0F84Fno6Ir0fEq0pdTKk5+iNJL80RIO2bIuDgt8Gk18F934H7vwtP3AwnfBqOughq6ktd4YvU11Zx9uEjOfvwkSxc1ci1M5Zw7Ywl/NNVM+nbs5q3TBnBO6eO4sDhvUtdqrRPSSmdHxG9gfOAKyIiAVcAv04pbSxtdV1r3oqNXDNtMecd6eiPJO2KkyCoe1i7MLtv0BM3QV2f7NqgIz4I/ceVurKX1NaeuG/eKq6Zvpjb5yynpa2dg4b35rQDh3DY6H5MGd2X3nXVpS5TepFynAQhIgYC5wOfBOYC+wGXpJS+15V1lKpPpZQ490cP8sTzG7nz0ycyoKG2y2uQpHLhJAjq/vqNhXOvgsV/g4cuy5YHLs1GiI68ECacnI0alZnKiuDESYM4cdIg1ja2cMOspfxu5hK+e8fTpFS49GlwA4eN7sdBI/rQr2c1veqq6V1XRe8e1fSuq2ZgQ42TKijXIuLNwPuBCcAvgSNTSisioidZEOrSAFQqv5/5HA89s4b/edurDT+S9BIMQOpeRh2ZLRuWwfTLYcYVcOXboP94OOQcePU7YEB5nhffr76GC44dywXHjmXjlq3MWryemYvWMnPRWm6Z/TxXT1u80++bOLiB848ew1sPG+FokfLqHcC3U0r3dNyYUmqKiPeXqKYuta6phf9381wOG92Xc6aOKnU5klTWPAVO3VtrM8y5Hh65Ep65F0gwYmoWhg5+G9QPLHWFu6W9PbFyUzMbNm9lw5atbNjcyoYtW1m5sZkbZy1l1pL19Kyp5C2HjuD8o8Z4HZGKrpxOgYuIccCylNKWwnoPYEhKaWEp6ilFn/rCdY/xm2mLufGjx/v/X5J46T5lAFJ+rH8OZv8OHr0Glj8GUZmNFu13Sjaz3NDJULFvTow4a/E6rnzwWW6YtZTm1namjunH+44bx+sOGkKVN2BVEZRZAJoOHJtSaims1wD3p5SOKEU9Xd2nZi5ay9t/8Ffef9w4/u1NB3bZ+0pSOTMASTtaPgdm/z67ueqyR7Jt9YNgwilwwBth/9dD5b53Otm6phaunbGEXzzwLIvWNDG8Tx3/cOxYzjtiNH167nt/HpWvMgtAj6SUpuywbVZKaXIp6unKPtXa1s6bv38/axtb+POnT6Sh1jPbJQmcBEF6sSEHZcsp/wabVsD8O7Mw9PRt8OjVUD8YprwLDvuHsr1maGf69qzhg68Zz/uOG8edT6zg8vue4au3PMF3//w0bztsBKceOIRXj+jDQC+QVveyMiLOTCndABARZwGrSlxTl/j5A88yd9kGfvDuwww/krSbHAGSOmpvy4LQjJ/BU7dCaoNxJ8BhF8DE06Fu3zu3fu6yDVxx/zNc/8hSWlrbARjWp46DR/ThkBF9OHB4b8YPamBUvx6eLqfdVmYjQBOAq4DhQACLgX9IKc0rRT1d1aeWrd/Mqd+8myPG9eeK9x7hbJCS1IGnwEl7YsNSePgqmPkLWL+ocM3QUbDfydmpcsOm7FPXDG1qbmX2c+uZ/dx6Hl2SPS5Y1fjC16sqgtEDejJ+YAMTBtVz1Pj+HDthIHXVlSWsWuWqnALQNhHRQNbXSnrz067oU3+avYx/vX4Om5q3ctsnT2T0gJ5FfT9J2tfsdQCKiHpgc0qpPSImAQcAt6SUtnZuqS/PAKQu194Oix7IRobm3wHLZmXbew6A/U7Lrhna7xSoqS9tnXtgw5atPL18IwtWNrJgVSPPrGxkwapNLFzVREtbO3XVFRy/30BOedUQTj5gMEN615W6ZJWJcgtAEfFG4CDghX+kKaUvl6KWYvapVZua+Y8b5vDHR5dx0PDefP3sQzhoeJ+ivJck7cs64xqge4DXREQ/4A5gOnAO8O7OKVEqYxUVMPa4bDn1P2DTSlhwV+GaoVuza4aq6mD8SdsnUNhHptfuXVfN4WP6c/iY/n+3vbm1jQcXrOGOucu5Y+4K/jx3BQBDe9fRlhJt7Ymtbe20tiXaU2JEvx5MGNTAfoMbXngc1a8HfXvWUFnhaTkqroi4DOgJnAT8BDgb+FtJi+pkKSVufHQZ/3nDHDZtaeUzp0/iH0+cQLWnrUrSK7a7I0AzU0qHRcTHgB4ppa9HxMMppUOLX+LfcwRIZaWtNRsdeuKP2bJ+ERAw7BAY/9psGX0MVPcoaZl7I6XEk8s3csfcFTyzqpHqyqCyIqiqqKC6Mgs3i9dsZv7KTSxc3cjWtu0/UyKgb49q+tfX0L++hn49a6ivraJHTSU9qyvpWVNJj5oqJg5u4MT9B/nL3D6knEaAIuLRlNIhHR4bgN+nlE4vRT2d2ae2bG3jridW8Otpi7nnqZVMHtWXb5x9CJOG9OqU15ek7qozRoAiIo4hG/H5wCv8Xqn7qqyCca/JljP+B55/DJ76Eyy4Gx74P7j/u1BZm91vaPyJ2SjRsCnZ9+0jIoIDhvbmgKEvPwHE1rZ2Fq9pYt6KTSxbv4XVjS2sbWxhTWFZtKaJxpZWNre00dTSxuatbWz7DGZgQy1vP2wE75g6iv0GN+z09dc3baW2usLrkrSjLYXHpogYDqwGxpWwnr3S2tbO/fNX84dHnuO2OcvZ1NzKwIZavvCGA3j/ceOcrESS9tLu/hb2SeBfgOtSSnMiYjxwV/HKkvZBURj5GXYInPhZaGmEZx/ITpdbcDfc+V/ZUtsbxh4P407MRogG7Z99bzdQXVnB+EENjB+08wCzo5QSm7e28dd5q7lm+mJ+et8z/PCeBRw+ph9vfPUwNm5pZeHqRp5Z1cjC1Y2sa9pKdWVw6Oh+HDdhIMfuN4Apo/ru9shRc2sbza3t9K7znkjdzI0R0Rf4BjATSMCPS1vSnrn+4ef4yk2Ps7qxhV51Vbz+4KGcOWU4x4wfYPCRpE7yimeBi4gKoCGltKE4Jb00T4HTPqtxFTxzdxaGnrkb1i7Mtvcalo0MTTgpC0QNg0tYZGmt3NjMdQ8v4TfTFjN/ZSMRMLxPD8YO7MnYAfWMHVDPqk3N/HX+amYvXU9K0LOmkskj+9JQV0VVxbbT84Kqygoam1tZvamFVZuaWbWpmQ1bWgEYO6Anh4/pzxFj+zF1bD8mDGpwCuFXqFxOgSv0pKNTSn8trNcCdSml9aWqaW/61F/nreKqhxZx5pThnDhpkKOdkrSHOmMWuF8BFwFtwAygD/CtlNI3OrPQ3WEAUrexdmEWhhbcBQv+ApvXZtuHvBpGHw3DJsPwKTDoAKjM14hFSonn1m1mYEPtLn8BXNfUwoML1vDX+at4dMl6WlrbaW1vp7U90dqWaG1rp0dNJQMbahnYq5aB9TUMbKiloiJ4ZPE6Zjy7ljWNLQD07VnN/kN6Mbp/z2wZ0JNR/Xsypn9PBuzFTWO3trVTVRElDVcppaK8f7kEIICIeCCldEyp69jGPiVJpdcZAeiRlNKUiHg3cDjwOWBGSumQzi315dlY1C21t2XTa28LQ8/NhJZN2deq6mDIQdm1Q8MPzW0o6mwpJRasamTGwrXMeHYtC1ZtYtGaJpZvaP67/frX17Df4AYmbluG9KKmKhtdaipcy9TU0sr6pq08v2ELyzdsYdn6LTxfuAaqqiLoV1/DgMJEEP3raxjRrwdHjevPEWP70+tlTsdrbm1jbeNWVjc2v3At1YbNWxk3sIFDRvXZ6el8KzZu4dY5y/nT7GU8tGANYwfWc8TY/hw5rh9HjO3PiL499joUlVkA+hLwKNnEByW/uZ19SpJKrzMC0BxgCvAr4PsppbsjYlZKaXLnlvrybCzKhfZ2WDMflj4Cyx4pPM6ClsL9HavqYMjB2wPRsMmGok6yuaWNJWubWLSmiWdWNTJ/5SaeXr6Jp5ZvfOEUul3pX1/DkN51DOtTx9A+dQzuVUtLaztrGltY3WEyiOfWbqalrZ2KgFeP6MPR4wcwdWx/mlpaeXZ1EwtXN/Ls6iaeXd3Eqk3Nu3y/CNhvUAOHju7LlFH9aG5t45bHnmfas2tICcYPrOeESYNYuDoLehubs/qH9anj9AOH8KWzDt7j41RmAWgjUA+0kk2IEEBKKb38zB1FYJ+SpNLrjFngfggsBGYB90TEGKAk1wBJuVBRAQMnZssh78i2tbfDmgWw9OFCKHoYZv0aphWu9a6szUaKhk+BoYdkzwe/CmqdLveV6FFTycQhvZi4wzTDKSVWbmxm3opNtCfoWZtN492zuoqetZU01Fbt9vUaW7a2MXPRWh6cv5oHF6zh8vuzyR+2GdanjjEDenLqqwYzom8PBjTU0r++mv71tfSvr6Ghtoqnlm/kkcXreHjRWm5/fDnXTF8CwP5DevGJUyby+oOHMWnI9mub2toTTz6/kWkL1/C3hWvY2l7ygZJOk1LyH7kkabe94kkQXvjGiKqU0kt/HFoEfrImdbDjSNGyWdnS3OHzib6jYXAhDA2cBAMmQP8J0LN/t5l9bl+3uaWNOUvX07tHNaP793zFF76nlFi0pomUYOzA+iJV+ffKbATohJ1tTynd09W1gH1KksrBXo8ARUQf4D+AbU3mbuDLQMlm2ZHErkeK1j0LK+bCijnZ4/LHYd7t0N7hM4u6PlkQGjABBkwsPO6XPTpq1KV61FQydWz/Pf7+iGDMgK4JPmXq4g7P64AjySbsObk05UiSytnungJ3OTAbeGdh/T3AFcDbilGUpL1QUQH9x2XLAW/Yvr21JQtGq+dno0bbHhc9CI9dS3brlIKGodBvDPQZCb1HQJ9R2fM+I6D3SEePVFZSSm/uuB4Ro4Cvl6gcSVKZ290ANCGl9PYO61+KiEeKUZCkIqmq2T5atKOtm7Pri1bPKyzzYd2ibDa6uTdCW8sOr1UHvYcXwtHI7DS7jkvvkVC5uz9epE63BNjzGR4kSd3a7v6Gsjkijk8p3QcQEccBm4tXlqQuVd0jmzRhyEEv/lp7OzStgvWLYf0S2LB0++OG5+CZe2HjUkjt278nKrMbuvYcmI0W9RywfakfCPWDOiwDoUc/R5S0xyLie2wfwqwgm7V0VukqkiSVs90NQBcBvyhcCwSwFrigOCVJKisVFVmYaRgMIw7f+T6tLVkYWreosDwLG5dB0xpoWp1NzNC0Gras2/n3V9W9eBSpzyio7Q019YWlAWp6Ql3f7FHaruOMA63Ar1NK95eqGElSedutAJRSmgVMjojehfUNEfFJshvPScq7qprt1x29lLbWLAg1riwsq6BxRTaatC08PTcTNq95mffrURhR6l8YQeqfTepQ1wfqemePtdsee/39UtMAFVWOOHUv1wJbUkptABFRGRE9U0pNJa5LklSGXtFJ+imljvf++WfgO51bjqRurbIKeg3JlpfSvDELRc2boGUTtDQWlo2wZX0WoprWZAGqaTWseSab+nvL+r+f6e6lVFTtsFQWglHhsaICKqo7jD7Vb1+q6qCqFiprsseq2mxfEqTU4ZEskPUckJ0OWD8we+zRN3s/dZY7gFOBTYX1HsBtwLElq0iSVLb25iplPz6VVBy1vWDQ/q/8+1LKJnTYsj5bmjdmwah54/alpRHat2ZBqW0rtLcV1tuybamt8Lwtm/xha1P2PZue3x7EWrdkp/21Ne9+4NpRVY/sVL6aeqguBKsefbOw1GPbdVP9oGYXU5JX1RSCWF12DVdVLUQFbN0CrZuhtTk7Fm1bobquEN56ZY+1DdkIWY++e1Z7+alLKW0LP6SUNkWE50lKknZqbwJQ97mNuKTuIaIQKnpC72Fd857tbVnYaN8KROHUuth+it22Eatto1VNq2Hz2g6jWo3bR7kaV8Gqp6BpbTbaVUwjDocP3Vnc9+g6jRFxWEppJkBEHI4T9UiSduElA1BEbGTnQSfITjGQpHyrqHzpSRlq6rMpw1+p1ubtQWlXX2/dsn3ZuiWbia+6Lhtdqi6MDlXWZCNB24LWtlGwuj47f9190yeB30bE0sL6MOCcEtYjSSpjLxmAUkreDl6SSqGqFnoNLXUV+4SU0rSIOADYn+wDuidSSltLXJYkqUxVlLoASZL2RkR8BKhPKc1OKT0GNETEP5W6LklSeTIASZL2dR9KKb1wk6mU0lrgQyWsR5JUxgxAkqR9XUXE9hs7RUQlUFPCeiRJZWxvZoGTJKkc3ApcExGXkU3ccxFwS2lLkiSVKwOQJGlf9zngQuDDZJMgPEw2E5wkSS/iKXCSpH1aSqkdeBBYAEwFTgHmlrQoSVLZcgRIkrRPiohJwLnAecBq4DcAKaWTSlmXJKm8GYAkSfuqJ4B7gTenlOYBRMSnSluSJKnceQqcJGlf9XbgeeCuiPhxRJxCdg2QJEm7ZACSJO2TUkrXpZTOAQ4A/gJ8ChgSET+IiNNLWpwkqWwZgCRJ+7SUUmNK6aqU0puAkcAjwOdLXJYkqUwZgCRJ3UZKaU1K6YcppZNLXYskqTwZgCRJkiTlhgFIkiRJUm4YgCRJkiTlhgFIkiRJUm4UNQBFxBkR8WREzIuIXc7IExFnR0SKiKnFrEeSpI7sU5KUP0ULQBFRCVwKvB44EDgvIg7cyX69gI8DDxWrFkmSdmSfkqR8KuYI0JHAvJTSgpRSC3A1cNZO9vsK8HVgSxFrkSRpR/YpScqhYgagEcDiDutLCtteEBGHAqNSSje91AtFxIURMT0ipq9cubLzK5Uk5ZF9SpJyqJgBKHayLb3wxYgK4NvAp1/uhVJKP0opTU0pTR00aFAnlihJyjH7lCTlUDED0BJgVIf1kcDSDuu9gIOBv0TEQuBo4AYvMJUkdRH7lCTlUDED0DRgYkSMi4ga4Fzghm1fTCmtTykNTCmNTSmNBR4EzkwpTS9iTZIkbWOfkqQcKloASim1Ah8FbgXmAteklOZExJcj4sxiva8kSbvDPiVJ+VRVzBdPKd0M3LzDtn/fxb6vLWYtkiTtyD4lSflT1BuhSpIkSVI5MQBJkiRJyg0DkCRJkqTcMABJkiRJyg0DkCRJkqTcMABJkiRJyg0DkCRJkqTcMABJkiRJyg0DkCRJkqTcMABJkiRJyg0DkCRJkqTcMABJkiRJyg0DkCRJkqTcMABJkiRJyg0DkCRJkqTcMABJkiRJyg0DkCRJkqTcMABJkiRJyg0DkCRJkqTcMABJkiRJyg0DkCRJkqTcMABJkiRJyg0DkCRJkqTcMABJkiRJyg0DkCRJkqTcMABJkiRJyg0DkCRJkqTcMABJkiRJyg0DkCRJkqTcMABJkiRJyg0DkCRJkqTcMABJkiRJyg0DkCRJkqTcMABJkiRJyg0DkCRJkqTcMABJkiRJyg0DkCRJkqTcMABJkiRJyg0DkCRJkqTcMABJkiRJyg0DkCRJkqTcMABJkiRJyg0DkCRJkqTcMABJkiRJyg0DkCRJkqTcMABJkiRJyg0DkCRJkqTcMABJkiRJyg0DkCRJkqTcMABJkiRJyg0DkCRJkqTcMABJkiRJyg0DkCRJkqTcMABJkiRJyg0DkCRJkqTcMABJkiRJyo2iBqCIOCMinoyIeRHx+Z18/Z8j4vGIeDQi7oiIMcWsR5KkjuxTkpQ/RQtAEVEJXAq8HjgQOC8iDtxht4eBqSmlQ4Brga8Xqx5JkjqyT0lSPhVzBOhIYF5KaUFKqQW4Gjir4w4ppbtSSk2F1QeBkUWsR5KkjuxTkpRDxQxAI4DFHdaXFLbtygeAW4pYjyRJHdmnJCmHqor42rGTbWmnO0acD0wFTtzF1y8ELgQYPXp0Z9UnSco3+5Qk5VAxR4CWAKM6rI8Elu64U0ScCnwRODOl1LyzF0op/SilNDWlNHXQoEFFKVaSlDv2KUnKoWIGoGnAxIgYFxE1wLnADR13iIhDgR+SNZUVRaxFkqQd2ackKYeKFoBSSq3AR4FbgbnANSmlORHx5Yg4s7DbN4AG4LcR8UhE3LCLl5MkqVPZpyQpn4p5DRAppZuBm3fY9u8dnp9azPeXJOml2KckKX+KeiNUSZIkSSonBiBJkiRJuWEAkiRJkpQbBiBJkiRJuWEAkiRJkpQbBiBJkiRJuWEAkiRJkpQbBiBJkiRJuWEAkiRJkpQbBiBJkiRJuWEAkiRJkpQbBiBJkiRJuWEAkiRJkpQbBiBJkiRJuWEAkiRJkpQbBiBJkiRJuWEAkiRJkpQbBiBJkiRJuWEAkiRJkpQbBiBJkiRJuWEAkiRJkpQbBiBJkiRJuWEAkiRJkpQbBiBJkiRJuWEAkiRJkpQbBiBJkiRJuWEAkiRJkpQbBiBJkiRJuWEAkiRJkpQbBiBJkiRJuWEAkiRJkpQbBiBJkiRJuWEAkiRJkpQbBiBJkiRJuWEAkiRJkpQbBiBJkiRJuWEAkiRJkpSlBcWOAAAJOElEQVQbBiBJkiRJuWEAkiRJkpQbBiBJkiRJuWEAkiRJkpQbBiBJkiRJuWEAkiRJkpQbBiBJkiRJuWEAkiRJkpQbBiBJkiRJuWEAkiRJkpQbBiBJkiRJuWEAkiRJkpQbBiBJkiRJuWEAkiRJkpQbBiBJkiRJuWEAkiRJkpQbBiBJkiRJuWEAkiRJkpQbBiBJkiRJuWEAkiRJkpQbRQ1AEXFGRDwZEfMi4vM7+XptRPym8PWHImJsMeuRJKkj+5Qk5U/RAlBEVAKXAq8HDgTOi4gDd9jtA8DalNJ+wLeBrxWrHkmSOrJPSVI+FXME6EhgXkppQUqpBbgaOGuHfc4Cfl54fi1wSkREEWuSJGkb+5Qk5VBVEV97BLC4w/oS4Khd7ZNSao2I9cAAYFXHnSLiQuDCwuqmiHhyN2sYuONr5ZDHIONxyHgcPAbb7O1xGNNZhZSQfap8eBw8Btt4HDwG2xStTxUzAO3sE7K0B/uQUvoR8KNXXEDE9JTS1Ff6fd2JxyDjcch4HDwG23gcAPtU2fA4eAy28Th4DLYp5nEo5ilwS4BRHdZHAkt3tU9EVAF9gDVFrEmSpG3sU5KUQ8UMQNOAiRExLiJqgHOBG3bY5wbggsLzs4E7U0ov+mRNkqQisE9JUg4V7RS4wrnSHwVuBSqBy1NKcyLiy8D0lNINwE+BX0bEPLJP1M7t5DJe8ekI3ZDHIONxyHgcPAbb5P442KfKisfBY7CNx8FjsE3RjkP4QZYkSZKkvCjqjVAlSZIkqZwYgCRJkiTlRrcMQBFxRkQ8GRHzIuLzpa6nq0TE5RGxIiJmd9j2jYh4IiIejYjrIqJvKWsstogYFRF3RcTciJgTEZ/Y4eufiYgUEQNLVWNXiIi6iPhbRMwqHIcvFbZHRPx3RDxVOEYfL3WtXSEiKiPi4Yi4qbB+SkTMjIhHIuK+iNiv1DUWU0T0jYhrCz8L5kbEMRHRPyJuj4inC4/9Sl1nntin8tunwF4F9qkd5b1PQdf2qm4XgCKiErgUeD1wIHBeRBxY2qq6zM+AM3bYdjtwcErpEOAp4F+6uqgu1gp8OqX0KuBo4CPb/v4jYhRwGrCohPV1lWbg5JTSZGAKcEZEHA28l2xK3wMKx+jq0pXYpT4BzO2w/gPg3SmlKcCvgH8tSVVd57vAn1JKBwCTyY7F54E7UkoTgTsK6+oC9qnc9ymwV4F9akd571PQhb2q2wUg4EhgXkppQUqphew/zlklrqlLpJTuYYf7U6SUbksptRZWHyS7z0W3lVJallKaWXi+kew/z4jCl78NfJad3MSwu0mZTYXV6sKSgA8DX04ptRf2W1GiErtMRIwE3gj8pMPmBPQuPO/Di+/90m1ERG/gBLLZzEgptaSU1pH9XPx5YbefA28pTYW5ZJ/6+2256lNgrwL7VEd571PQ9b2qOwagEcDiDutL2P5DJe/eD9xS6iK6SkSMBQ4FHoqIM4HnUkqzSlpUFyoMpz8CrABuTyk9BEwAzomI6RFxS0RMLG2VXeI7ZL9MtHfY9kHg5ohYArwH+GopCusi44GVwBWF0yt+EhH1wJCU0jLIfhkDBpeyyJyxT+1arvoU5LtX2adekPc+BV3cq7pjAIqdbOvWn6Lsjoj4ItmQ+1WlrqUrREQD8Dvgk2R/7i8C/17SorpYSqmtMHQ+EjgyIg4GaoEtKaWpwI+By0tZY7FFxJuAFSmlGTt86VPAG1JKI4ErgG91eXFdpwo4DPhBSulQoBFPdys1+9RO5K1Pgb3KPmWf6qBLe1V3DEBLyM4d3WYk3XzY8OVExAXAm8jOJe32TTYiqskaylUppd+TfZo0DpgVEQvJ/k3MjIihpauy6xSGkP9Cdt79ErJjA3AdcEiJyuoqxwFnFv7erwZOjog/ApMLnzQC/AY4tkT1dYUlwJIOf95ryZrM8ogYBlB47PanmZQR+9QO8tanwF7VkX0q930KurhXdccANA2YGBHjIqKG7K7dN5S4ppKJiDOAzwFnppSaSl1PsUVEkJ0/Ojel9C2AlNJjKaXBKaWxKaWxZP/JDkspPV/CUosqIgZtm0kpInoApwJPANcDJxd2O5HsguNuK6X0LymlkYW/93OBO8nOJ+4TEZMKu53G31942q0U/p0vjoj9C5tOAR4n+7l4QWHbBcAfSlBeXtmnOshbnwJ7FdintrFPZbq6V1V1xouUk5RSa0R8FLgVqAQuTynNKXFZXSIifg28FhhYOGf0P8hm06kFbs9+3vJgSumikhVZfMeRnSv7WOG8YoAvpJRuLmFNpTAM+HlhtqkK4JqU0k0RcR9wVUR8CthEdo5xrhR+RnwI+F1EtANrya476M4+Rvb3XgMsAN5H4d9FRHyAbLapd5SwvlyxT+W+T4G9CuxTu5TTPgVd2KsiJyPNkiRJktQtT4GTJEmSpJ0yAEmSJEnKDQOQJEmSpNwwAEmSJEnKDQOQJEmSpNwwAEmSJHUjEdEWEY90WD7fia89NiJmd9brSaXQ7e4DJEmSlHObU0pTSl2EVK4cAZIkScqBiFgYEV+LiL8Vlv0K28dExB0R8WjhcXRh+5CIuC4iZhWWYwsvVRkRP46IORFxW0T0KOz/8Yh4vPA6V5fojym9LAOQJElS99Jjh1PgzunwtQ0ppSOB7wPfKWz7PvCLlNIhwFXAJYXtlwB3p5QmA4cBcwrbJwKXppQOAtYBby9s/zxwaOF1LirWH07aW5FSKnUNkiRJ6iQRsSml1LCT7QuBk1NKCyKiGng+pTQgIlYBw1JKWwvbl6WUBkbESmBkSqm5w2uMBW5PKU0srH8OqE4p/VdE/AnYBFwPXJ9S2lTkP6q0RxwBkiRJyo+0i+e72mdnmjs8b2P7NeVvBC4FDgdmRITXmqssGYAkSZLy45wOjw8Unv8VOLfw/N3AfYXndwAfBoiIyojovasXjYgKYFRK6S7gs0Bf4EWjUFI5MJlLkiR1Lz0i4pEO639KKW2bCrs2Ih4i+xD8vMK2jwOXR8TFwErgfYXtnwB+FBEfIBvp+TCwbBfvWQlcGRF9gAC+nVJa12l/IqkTeQ2QJElSDhSuAZqaUlpV6lqkUvIUOEmSJEm54QiQJEmSpNxwBEiSJElSbhiAJEmSJOWGAUiSJElSbhiAJEmSJOWGAUiSJElSbvx/OfPMqNlOeIcAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 1008x432 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jtPNCR4yYhc3",
        "colab_type": "text"
      },
      "source": [
        "En base a las predicciones ploteamos el accuracy obtenido:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lIuukNFqYhc4",
        "colab_type": "code",
        "colab": {},
        "outputId": "5c66941e-9fec-489f-d602-5f6fc1bb2d5a"
      },
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "y_pred =model.predict_classes(X_test_vec)\n",
        "\n",
        "print(\"Accuracy sobre Red: {}\".format(accuracy_score(y_test, y_pred.round())))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy sobre Red: 0.7780994803266518\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QKTYK_yaYhc-",
        "colab_type": "code",
        "colab": {},
        "outputId": "5dfe1342-dd92-46d6-a989-8212cec7491a"
      },
      "source": [
        "from sklearn.utils.multiclass import unique_labels\n",
        "from sklearn.metrics import confusion_matrix\n",
        "plt.figure(figsize=(10, 10))\n",
        "classes = unique_labels(y_test, y_pred)\n",
        "plt.title(\"confusion matrix Red\")\n",
        "\n",
        "mat = confusion_matrix(y_test, y_pred)\n",
        "ax=sns.heatmap(mat, square=True, annot=True, fmt='d', cbar=False)\n",
        "sns.set(font_scale=2) \n",
        "\n",
        "plt.xlabel('Etiquetas predichas')\n",
        "plt.ylabel('Etiquetas verdaderas')\n",
        "ax.set_xticklabels(classes,rotation=90)\n",
        "ax.set_yticklabels(classes,rotation=0);\n",
        "\n",
        "plt.savefig('Matriz Confusion Red.jpg', dpi=300)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x720 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}